#!/bin/bash
# ############################################################################################
# File: ........: functions 
# Language .....: bash 
# Author .......: Sacha Dubois, VMware
# Description ..: Tanzu Demo Hub - Cluster Installation
# ############################################################################################

ROOT_SHELL=0
  
[ -f usr/local/bin/bat ] && alias cat='/usr/local/bin/bat -p'

function ctrl_c() {
  echo "** Trapped CTRL-C"
}

waitCmd() {
  read
}

prtHead() {
  if [ "${tocindex}" == "" ]; then tocindex=1; else let tocindex=tocindex+1; fi
  printf "%2d.) %s\n" $tocindex "$1"
}

prtRead() {
  if [ "${tocindex}" == "" ]; then tocindex=1; else let tocindex=tocindex+1; fi
  printf "%2s   %s" "" "$1"; read x
}

prtText() {
  printf "%2s   %s\n" "" "$1"
}

lineFed() {
  echo
}

slntCmd() {
  bold=$(tput bold)
  normal=$(tput sgr0)

  echo -e "     => ${bold}$1${normal}\c"; read x
  eval "$1" 2>&1 | sed -e '/^$/d' -e 's/^/     /g'
  if [ $? -ne 0 ]; then exit 1; fi
}

fakeCmd() {
  bold=$(tput bold)
  normal=$(tput sgr0)

  echo -e "     => ${bold}$1${normal}\c"; read x
  echo "     ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
  cat /tmp/log | sed -e 's/^/     /g'
  echo "     ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
  echo ""
}

execCat() {
  echo -e "     => cat $1\c"; read x
  echo "     ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"

  if [ -f /usr/local/bin/bat -o /usr/bin/bat ]; then
    bat -p --color always $1 | sed 's/^/     /g'
  else
    cat $1
  fi

  echo "     ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
  echo ""
}

execCmd() {
  bold=$(tput bold)
  normal=$(tput sgr0)

  if [ "$(uname)" == "Darwin" ]; then SED=gsed; else SED=sed; fi

  echo -e "     => ${bold}$1${normal}\c"; read x
  echo "     ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
  ret=1; cnt=0
  while [ $ret -ne 0 -a $cnt -lt 5 ]; do
    eval "$1" > /tmp/execCmd.log 2>&1; ret=$?
    [ $ret -eq 0 ] && break

    let cnt=cnt+1
    sleep 3
  done

  $SED '${/^$/d;}' /tmp/execCmd.log | $SED 's/^/     /g'

  #DELETEME
  #eval "$1" 2>&1 | $SED '${/^$/d;}' | $SED 's/^/     /g' 
  echo "     ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
  echo ""
}

loopCmd() {
  bold=$(tput bold)
  normal=$(tput sgr0)

  if [ "$(uname)" == "Darwin" ]; then SED=gsed; else SED=sed; fi

  echo -e "     => ${bold}$1${normal}\c"; read x
  echo "     ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
  
  ret=1
  while [ $ret -ne 0 ]; do
    eval "$1" 2>&1; ret=$? | $SED -e 's/^/     /g'; ret=$?
  done

  echo "     ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
  echo ""
}

oktaVerifyAccount() {
  # [ "${TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE}" == "none" ] && return
  
  if [ "${TDH_OKTA_DOMAIN}" == "" -o "${TDH_OKTA_API_TOKEN}" == "" -o "${TDH_OKTA_USER}" == "" ]; then
    missing_variables=1
    echo ""
    echo "  MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

    if [ "${TDH_OKTA_DOMAIN}" == "" ]; then
      echo "  TDH_OKTA_DOMAIN                  (required) OKTA Domain URL"
    fi

    if [ "${TDH_OKTA_API_TOKEN}" == "" ]; then
      echo "  TDH_OKTA_API_TOKEN               (required) OKTA API Token"
      echo "                                   Okta Dahshboard -> Security -> API -> Create Token"
    fi

    if [ "${TDH_OKTA_USER}" == "" ]; then 
      echo "  TDH_OKTA_USER                    (required) OKTA Admin User"
    fi
  else
    if [ "$TDH_INFO" == "true" ]; then
      echo "LDAP Identitiy Management (IdM)"
      messagePrint " ▪ OKTA Domain "                       "$TDH_OKTA_DOMAIN"
      messagePrint " ▪ OKTA URL"                           "https://${TDH_OKTA_DOMAIN}"
      messagePrint " ▪ OKTA Admin URL"                     "https://${TDH_OKTA_DOMAIN}-admin"
      messagePrint " ▪ OKTA API Token"                     "$TDH_OKTA_API_TOKEN"
    fi
  fi

  # --- VERIFY OKTA DOMAIN ---
  issuer=$(curl https://${TDH_OKTA_DOMAIN}.okta.com/.well-known/openid-configuration 2>/dev/null | jq -r '.issuer' 2>/dev/null)
  if [ "$issuer" != "https://${TDH_OKTA_DOMAIN}.okta.com" ]; then 
    echo "ERROR: $TDH_OKTA_ADMIN_URL is not a valiad OKta Domain. Please verify or create a new one"
    exit 1
  fi

  oktaAPI verify
}

oktaVerifyApp() {
  # [ "${TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE}" == "none" ] && return

  if [ "${TDH_OKTA_SECRET_ID}" == "" -o "${TDH_OKTA_USERNAME_CLAIM}" == "" -o "${TDH_OKTA_CLIENT_SECRET}" == "" -o \
       "${TDH_OKTA_URL}" == "" -o "${TDH_OKTA_SCOPES}" == "" -o "${TDH_OKTA_GROUP_CLAIM}" == "" ]; then

    missing_variables=1
    echo ""
    echo "  MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

    if [ "${TDH_OKTA_SECRET_ID}" == "" ]; then
      echo "  TDH_OKTA_SECRET_ID               (required) OKTA SecretId"
    fi

    if [ "${TDH_OKTA_CLIENT_SECRET}" == "" ]; then
      echo "  TDH_OKTA_CLIENT_SECRET           (required) OKTA Client Secret"
    fi
  else
    echo "LDAP Identitiy Management (IdM)"
    messagePrint " ▪ LDAP Domain"                        "$LDAP_DOMAIN"
    messagePrint " ▪ LDAP User Serach Name"              "userPrincipalName"
    messagePrint " ▪ LDAP Host"                          "jump.${TDH_TKGMC_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"
    messagePrint " ▪ LDAP Group Search Name Attribute"   "cn"
    messagePrint " ▪ LDAP Group Search User Attribute"   "DN"
    messagePrint " ▪ LDAP Search Username"               "userPrincipalName"
  fi

  curl https://dev-15903420.okta.com/.well-known/openid-configuration | jq
}



checkIdentityProvider() {
  [ "${TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE}" == "none" ] && return
  missing_variables=0
  if [ "${TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE}" == "ldap" ]; then
    if [ "${LDAP_DOMAIN}" == "" ]; then

      missing_variables=1
      echo ""
      echo "  MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
      echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
 
      if [ "${LDAP_DOMAIN}" == "" ]; then
        echo "  LDAP_DOMAIN                       (required) LDAP Domain"
      fi
    else
      echo "LDAP Identitiy Management (IdM)"
      messagePrint " ▪ LDAP Domain"                        "$LDAP_DOMAIN"
      messagePrint " ▪ LDAP User Serach Name"              "userPrincipalName"
      messagePrint " ▪ LDAP Host"                          "jump.${TDH_TKGMC_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"
      messagePrint " ▪ LDAP Group Search Name Attribute"   "cn"
      messagePrint " ▪ LDAP Group Search User Attribute"   "DN"
      messagePrint " ▪ LDAP Search Username"               "userPrincipalName"
    fi
  fi

  if [ "${TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE}" == "oidc" ]; then
    if [ "${TDH_OKTA_SECRET_ID}" == "" -o "${TDH_OKTA_USERNAME_CLAIM}" == "" -o "${TDH_OKTA_CLIENT_SECRET}" == "" -o \
         "${TDH_OKTA_URL}" == "" -o "${TDH_OKTA_SCOPES}" == "" -o "${TDH_OKTA_GROUP_CLAIM}" == "" ]; then

      missing_variables=1
      echo ""
      echo "  MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
      echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
  
      if [ "${TDH_OKTA_SECRET_ID}" == "" ]; then
        echo "  TDH_OKTA_SECRET_ID               (required) OKTA SecretId"
      fi
  
      if [ "${TDH_OKTA_CLIENT_SECRET}" == "" ]; then
        echo "  TDH_OKTA_CLIENT_SECRET           (required) OKTA Client Secret"
      fi
  
      if [ "${TDH_OKTA_URL}" == "" ]; then
        echo "  TDH_OKTA_URL                     (required) OKTA URL"
      fi
  
      if [ "${TDH_OKTA_SCOPES}" == "" ]; then
        echo "  TDH_OKTA_SCOPES                  (required) OKTA Scopes"
      fi
  
      if [ "${TDH_OKTA_GROUP_CLAIM}" == "" ]; then
        echo "  TDH_OKTA_GROUP_CLAIM             (required) OKTA Group Claim"
      fi
  
      if [ "${TDH_OKTA_USERNAME_CLAIM}" == "" ]; then
        echo "  TDH_OKTA_USERNAME_CLAIM          (required) OKTA Username Claim"
      fi
    else
      echo "OIDC Identitiy Management (IdM)"
      messagePrint " ▪ OKTA SecretId"                $(maskPassword "$TDH_OKTA_SECRET_ID")
      messagePrint " ▪ OKTA Client Secret"           $(maskPassword "$TDH_OKTA_CLIENT_SECRET")
      messagePrint " ▪ OKTA URL"                     "$TDH_OKTA_URL"
      messagePrint " ▪ OKTA Scopes"                  "$TDH_OKTA_SCOPES"
      messagePrint " ▪ OKTA Group Claim"             "$TDH_OKTA_GROUP_CLAIM"
      messagePrint " ▪ OKTA Username Claim"          "$TDH_OKTA_USERNAME_CLAIM"
    fi
  fi

  if [ ${missing_variables} -eq 1 ]; then
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    echo "  IMPORTANT: Please set the missing environment variables either in your shell or in the tanzu-demo-hub"
    echo "             configuration file ~/.tanzu-demo-hub.cfg and set all variables with the 'export' notation"
    echo "             export TMC_SERVICE_TOKEN='xxxxxxxxxxxx'"
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    exit 1
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: vSphereLogin
# Function Purpose ...: Login to vSphere Supervisor Cluster
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: TMC Supervisor Cluster Display Name
# Dependancies: ......: $HOME/.tanzu-demo-hub.cfg
# ------------------------------------------------------------------------------------------
# Variables ..........: VSPHERE_TKGS_VCENTER_SERVER ......... (required) vSphere Server Name
#                       VSPHERE_TKGS_VCENTER_SERVER ......... (required) vSphere Server Name
#                       VSPHERE_TKGS_VCENTER_ADMIN ........ (required) vCenter Amdinistrator
#                       VSPHERE_TKGS_VCENTER_PASSWORD .... (required) vCenter Admin Password
#                       VSPHERE_TKGS_SUPERVISOR_CLUSTER ...... (required) Supervisor Cluster
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 on Success, 1 on failure
# ------------------------------------------------------------------------------------------
vSphereLogin() {
  missing_variables=0

  if [ "${VSPHERE_TKGS_VCENTER_ADMIN}" == "" -o "${VSPHERE_TKGS_VCENTER_PASSWORD}" == "" -o "${VSPHERE_TKGS_VCENTER_SERVER}" == "" -o \
       "${VSPHERE_TKGS_SUPERVISOR_CLUSTER}" == "" -o "${TDH_USER}" == "" ]; then
 
    missing_variables=1
    echo ""
    echo "  4MISSING ENVIRONMENT-VARIABES         DESCRIPTION        "
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
 
    if [ "${VSPHERE_TKGS_VCENTER_SERVER}" == "" ]; then
      echo "  VSPHERE_TKGS_VCENTER_SERVER           (required) vSphere Server Name"
      echo "                                        ie. https://vcsa-01.haas-464.pez.vmware.com"
    fi
 
    if [ "${VSPHERE_TKGS_VCENTER_ADMIN}" == "" ]; then
      echo "  VSPHERE_TKGS_VCENTER_ADMIN            (required) vCenter Amdinistrator"
    fi
 
    if [ "${VSPHERE_TKGS_VCENTER_PASSWORD}" == "" ]; then
      echo "  VSPHERE_TKGS_VCENTER_PASSWORD         (required) vCenter Admin Password"
    fi
 
    if [ "${VSPHERE_TKGS_SUPERVISOR_CLUSTER}" == "" ]; then
      echo "  VSPHERE_TKGS_SUPERVISOR_CLUSTER       (required) vSphere Supervisor Cluster"
    fi

    if [ "${TDH_USER}" == "" ]; then
      echo "  TDH_USER                             (required) Tanzu Demo Hub User to tag TKG Clusters with your Name"
      echo "                                        => (usually you VMware User Name without @vmware.com)"
    fi
  else
    messageTitle "vSphere Access Credentials (TKGs)"
    messagePrint " ▪ vCenter Server Name"                "$VSPHERE_TKGS_VCENTER_SERVER"
    messagePrint " ▪ vCenter Admin User"                 "$VSPHERE_TKGS_VCENTER_ADMIN"
    messagePrint " ▪ vCenter Admin Password"             $(maskPassword "$VSPHERE_TKGS_VCENTER_PASSWORD")
    messagePrint " ▪ vCenter Supervisor Cluster"         "$VSPHERE_TKGS_SUPERVISOR_CLUSTER"
  fi

  if [ ${missing_variables} -eq 1 ]; then
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    echo "  IMPORTANT: Please set the missing environment variables either in your shell or in the tanzu-demo-hub"
    echo "             configuration file ~/.tanzu-demo-hub.cfg and set all variables with the 'export' notation"
    echo "             ie. => export <environment_variable>=\"<value>\""
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    exit 1
  fi

  # --- SET LOGIN PASSWORD AS ENVIRONMENT VARIABLE ---
  export KUBECTL_VSPHERE_PASSWORD=$VSPHERE_TKGS_VCENTER_PASSWORD

  ret=1; cnt=0
  while [ $ret -ne 0 -a $cnt -lt 5 ]; do
    kubectl vsphere login --insecure-skip-tls-verify --server $VSPHERE_TKGS_SUPERVISOR_CLUSTER -u $VSPHERE_TKGS_VCENTER_ADMIN > /tmp/error.log 2>&1; ret=$?
    [ $ret -eq 0 ] && break
    let cnt=cnt+1
    sleep 30
  done

  if [ $ret -ne 0 ]; then
    logMessages /tmp/error.log
    echo "ERROR: failed to login into TMC"
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ export KUBECTL_VSPHERE_PASSWORD=$VSPHERE_TKGS_VCENTER_PASSWORD"
      echo "          tdh-tools:/$ kubectl vsphere login --insecure-skip-tls-verify --server $VSPHERE_TKGS_SUPERVISOR_CLUSTER -u $VSPHERE_VCENTER_ADMIN"
      echo "          tdh-tools:/$ exit"
    else
      echo "       => export KUBECTL_VSPHERE_PASSWORD=$VSPHERE_TKGS_VCENTER_PASSWORD"
      echo "       => kubectl vsphere login --insecure-skip-tls-verify --server $VSPHERE_TKGS_SUPERVISOR_CLUSTER -u $VSPHERE_VCENTER_ADMIN"
    fi

    echo "       => Make sure that the following variables in ~/.tanzu-demo-hub.cfg are correct"
    echo "          VSPHERE_TKGS_VCENTER_SERVER=$VSPHERE_TKGS_VCENTER_SERVER"
    echo "          VSPHERE_TKGS_VCENTER_ADMIN=$VSPHERE_TKGS_VCENTER_ADMIN"
    echo "          VSPHERE_TKGS_VCENTER_PASSWORD=$VSPHERE_TKGS_VCENTER_PASSWORD"
    echo "          VSPHERE_TKGS_SUPERVISOR_CLUSTER=$VSPHERE_TKGS_SUPERVISOR_CLUSTER"
    echo ""
    exit 1
  fi

  kubectl config use-context $VSPHERE_TKGS_SUPERVISOR_CLUSTER > /dev/null 2>&1
  kubectl get ns > /tmp/error.log 2>&1; ret=$?
  if [ $ret -ne 0 ]; then
    logMessages /tmp/error.log
    echo "ERROR: failed to switch kubernetes context"
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ kubectl config use-context $VSPHERE_TKGS_SUPERVISOR_CLUSTER"
      echo "          tdh-tools:/$ kubectl get ns"
      echo "          tdh-tools:/$ exit"
    else
      echo "       => kubectl config use-context $VSPHERE_TKGS_SUPERVISOR_CLUSTER"
      echo "       => kubectl get ns"
    fi
    exit
  fi

  # --- RECONFIGURE WORKLOAD CLUSTERS ---
  cmdLoop kubectl get clusters -A -o json > /tmp/output.json
  cmdLoop kubectl config view -o json > /tmp/kubeconfig.json

  first=0
  for nam in $(jq -r '.items[] | select(.status.phase == "Provisioned").metadata.name' /tmp/output.json); do
    nsp=$(jq -r --arg key "$nam" '.items[] | select(.metadata.name == $key).metadata.namespace' /tmp/output.json) 
    clu=$(jq -r --arg key "$nam" '.contexts[] | select(.name == $key).cluster' /tmp/kubeconfig.json) 
    if [ "$clu" == "" ]; then 
      [ $first -eq 0 ] && messageTitle "vSphere Workload Clusters" && first=1
      messagePrint " ▪ Configure context for Cluster"                "$nam"
 
      export KUBECTL_VSPHERE_PASSWORD=$VSPHERE_TKGS_VCENTER_PASSWORD
      kubectl vsphere login --insecure-skip-tls-verify --server $VSPHERE_TKGS_SUPERVISOR_CLUSTER \
          -u $VSPHERE_TKGS_VCENTER_ADMIN --tanzu-kubernetes-cluster-name $nam \
          --tanzu-kubernetes-cluster-namespace $nsp > /dev/null 2>&1
    fi
  done

  # --- SWITCH CONTEXT BACK TO WORKLOAD CLUSTERS ---
  kubectl config use-context $VSPHERE_TKGS_SUPERVISOR_CLUSTER > /dev/null 2>&1
}

vSphereKubeconfig() {
  # --- SET LOGIN PASSWORD AS ENVIRONMENT VARIABLE ---
  export KUBECTL_VSPHERE_PASSWORD=$VSPHERE_TKGS_VCENTER_PASSWORD

#hallo
  mv $HOME/.kube/config $HOME/.kube/config.bak

  ret=1; cnt=0
  while [ $ret -ne 0 -a $cnt -lt 5 ]; do
    kubectl vsphere login --insecure-skip-tls-verify --server $VSPHERE_TKGS_SUPERVISOR_CLUSTER -u $VSPHERE_TKGS_VCENTER_ADMIN > /tmp/error.log 2>&1; ret=$?
    [ $ret -eq 0 ] && break
    let cnt=cnt+1
    sleep 30
  done

  if [ $ret -ne 0 ]; then
    logMessages /tmp/error.log
    echo "ERROR: failed to login into TMC"
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ export KUBECTL_VSPHERE_PASSWORD=$VSPHERE_TKGS_VCENTER_PASSWORD"
      echo "          tdh-tools:/$ kubectl vsphere login --insecure-skip-tls-verify --server $VSPHERE_TKGS_SUPERVISOR_CLUSTER -u $VSPHERE_VCENTER_ADMIN"
      echo "          tdh-tools:/$ exit"
    else
      echo "       => export KUBECTL_VSPHERE_PASSWORD=$VSPHERE_TKGS_VCENTER_PASSWORD"
      echo "       => kubectl vsphere login --insecure-skip-tls-verify --server $VSPHERE_TKGS_SUPERVISOR_CLUSTER -u $VSPHERE_VCENTER_ADMIN"
    fi

    echo "       => Make sure that the following variables in ~/.tanzu-demo-hub.cfg are correct"
    echo "          VSPHERE_TKGS_VCENTER_SERVER=$VSPHERE_TKGS_VCENTER_SERVER"
    echo "          VSPHERE_TKGS_VCENTER_ADMIN=$VSPHERE_TKGS_VCENTER_ADMIN"
    echo "          VSPHERE_TKGS_VCENTER_PASSWORD=$VSPHERE_TKGS_VCENTER_PASSWORD"
    echo "          VSPHERE_TKGS_SUPERVISOR_CLUSTER=$VSPHERE_TKGS_SUPERVISOR_CLUSTER"
    echo ""
    exit 1
  fi

  mv $HOME/.kube/config $HOME/.kube/config_$VSPHERE_TKGS_SUPERVISOR_CLUSTER
  mv $HOME/.kube/config.bak $HOME/.kube/config
}

tmcCheckLogin() {
  FLAG=$1
  [ "${TDH_TANZU_MISSION_CONTROL_REGISTRATION}" == "false" ] && return
  missing_variables=0

  if [ "${TMC_SERVICE_TOKEN}" == "" -o "${TMC_CONTEXT_NAME}" == "" -o "${TMC_SERVER_URL}" == "" ]; then
    missing_variables=1
    echo ""
    echo "  ?MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

    if [ "${TMC_SERVICE_TOKEN}" == "" ]; then
      echo "  TMC_SERVICE_TOKEN                (required) is the TMC_API_TOKEN which can be optained in the TMC console"
    fi

    if [ "${TMC_SERVER_URL}" == "" ]; then
      echo "  TMC_SERVER_URL                   (required) The TMC Server Url"
      echo "                                   ie. TMC_SERVER_URL=\"https://tanzuemea.tmc.cloud.vmware.com\""
    fi

    if [ "${TMC_CONTEXT_NAME}" == "" ]; then
      echo "  TMC_CONTEXT_NAME                 (required) The TMC Context Name"
    fi
  fi

  if [ ${missing_variables} -eq 1 ]; then
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    echo "  IMPORTANT: Please set the missing environment variables either in your shell or in the tanzu-demo-hub"
    echo "             configuration file ~/.tanzu-demo-hub.cfg and set all variables with the 'export' notation"
    echo "             export TMC_SERVICE_TOKEN='xxxxxxxxxxxx'"
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    exit 1
  fi

  if [ ! -x /usr/local/bin/tmc ]; then 
    _arch=$(uname) 
    _mach=$(uname -m) 

#hallo
    [ "${_arch}" == "Darwin" -a "${_mach}" == "x86_64" ] && _type="darwinX64"
    [ "${_arch}" == "Darwin" -a "${_mach}" == "arm64" ] && _type="darwinX64"
    [ "${_arch}" == "Linux" -a "${_mach}" == "x86_64" ] && _type="linuxX64"
    if [ "$_type" == "" ]; then 
      echo "ERROR: unknown Architecure $_arch/$_mach"
      exit 1
    fi

    API_TOKEN=$(tmcAPI_getToken $TMC_SERVICE_TOKEN) 
    url=$(tmcAPI_getCliBinary $API_TOKEN $TMC_SERVER_URL darwinX64)
    if [ "$url" == "" ]; then 
      echo "ERROR: failed to download tmc binary from $TMC_SERVER_URL"
      exit 1
    fi

    wget -q $url -O /tmp/tmc 
    echo "ERROR: TMC Binary has not yet installed. We downloaded it for you proactively"
    echo "       as  type (darwinX64). Please do the following to complete the installatio"
    echo "       => sudo mv /tmp/tmc /usr/local/bin"
    echo "       => sudo chmod a+x /usr/local/bin/tmc"
    exit 1
  fi

  # --- IF STARTED BY (tdh-tools) CONTAINER, NO TMC CLI IST INSTALLED ---
  [ "$FLAG" == "--noverify" ] && return

  export TMC_API_TOKEN=$TMC_SERVICE_TOKEN
  ret=1; cnt=0
  while [ $ret -ne 0 -a $cnt -lt 5 ]; do
    tmc login --name $TMC_CONTEXT_NAME --no-configure > /dev/null 2>&1; ret=$?
    [ $ret -eq 0 ] && break
    let cnt=cnt+1
    sleep 30
  done

  if [ $ret -ne 0 ]; then
    echo "ERROR: failed to login into TMC"
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ export TMC_API_TOKEN=$TMC_SERVICE_TOKEN"
      echo "          tdh-tools:/$ tmc login --name $TMC_CONTEXT_NAME --no-configure"
      echo "          tdh-tools:/$ exit"
      echo "       => Verify you API token, mabe a new token is required"
      echo "          TMC => My Account => API Token"
    else
      echo "       => export TMC_API_TOKEN=$TMC_SERVICE_TOKEN"
      echo "       => tmc login --name $TMC_CONTEXT_NAME --no-configure"
      echo "       => Verify you API token, mabe a new token is required"
      echo "          TMC => My Account => API Token"
    fi
  fi

  tmc cluster list >/dev/null 2>&1

  if [ $? -ne 0 ]; then 
    echo "ERROR: Currently not logged in TMC, please login: tmc login"
    exit 1
  fi

}

messageTitle() {
  _msg="$*"
  _dat=$(date "+%Y%m%d-%H:%M:%S")

  [ $(uname) == "Linux" ] && HST="jump-host" || HST="localhost"

  if [ "${PCF_DEPLOYMENT_DEBUG}" == "true" -o "${DEBUG}" == "1" ]; then
    if [ "$HST" == "localhost" ]; then
      #printf "\e[37m[${_dat}-$HST] ${_msg}\e[0m\n"
      printf "\e[1m[${_dat}-$HST] ${_msg}\e[0m\n"
    else
      echo -e "[${_dat}-$HST] ${_msg}"
    fi
  else
    #printf "\e[37m[${_msg}\e[0m\n"
    printf "\e[1m${_msg}\e[0m\n"
    #echo -e "\e[1mBold${_msg}"
  fi
}

pivnetAPIdownload() {
  REFRESH_TOKEN="$1"
  PRODUCT_SLUG="$2"
  PRODUCT_ID=$3
  PRODUCT_FILE=$4
  FILE_NAME="$5"

  token=$(curl -X POST https://network.pivotal.io/api/v2/authentication/access_tokens \
               -d "{\"refresh_token\":\"$REFRESH_TOKEN\"}" 2>/dev/null | jq -r '.access_token')
  if [ "${token}" == "" -o "${token}" == "null" ]; then return 1; fi

  # --- LOGIN INTO PIVNET ---
  wget -O /tmp/$FILE_NAME --header="Authorization: Bearer $token" \
  https://network.tanzu.vmware.com/api/v2/products/$PRODUCT_SLUG/releases/$PRODUCT_ID/product_files/$PRODUCT_FILE/download > /dev/null 2>&1

  #https://network.pivotal.io/api/v2/products/$PRODUCT_SLUG/releases/$PRODUCT_ID/product_files/$PRODUCT_FILE/download > /dev/null 2>&1
  #wget -O /tmp/$FILE_NAME --header="Authorization: Bearer $token" \
  #https://network.pivotal.io/api/v2/products/$PRODUCT_SLUG/releases/$PRODUCT_ID/product_files/$PRODUCT_FILE/download 

  if [ ! -f /tmp/$FILE_NAME ]; then
    echo "1ERROR: Error downloading file from PIVNET /tmp/$FILE_NAME"
    echo "       SLUG:$PRODUCT_SLUG, PRODUCT_ID:$PRODUCT_ID PRODUCT_FILE:$PRODUCT_FILE"
    exit 1
  fi


#pivnetAPIdownload "1edb78016b054bbaab960b3007a77b12-r" build-service 843689 build-service-1.1.2.tar

  return 0
}


messageLine() {
  echo "------------------------------------------------------------------------------------------------------------------------------------------------------------------"
} 

messagePrint() {
  _msg="$1"
  _stt="$2"
  _cnt=$(echo "${_msg}" | wc -c | sed 's/ //g')
  _dat=$(date "+%Y%m%d-%H:%M:%S")

  ttt=$(echo "$_msg" | egrep -c "^   ") 
  if [ $ttt -eq 1 ]; then MAX=62; else MAX=64; fi

  _str=""
  while [ $_cnt -lt $MAX ]; do
    _str="${_str}."
    let _cnt=_cnt+1
  done

  # --- INIZIALIZE VALUES IF EMPTY --- ---
  [ "$DEBUG" == "" ] && DEBUG=0

  [ $(uname) == "Linux" ] && HST="jump-host" || HST="localhost"

  if [ "${PCF_DEPLOYMENT_DEBUG}" == "true" -o "${DEBUG}" == "1" ]; then
    if [ "$HST" == "localhost" ]; then 
      printf "\e[37m[${_dat}-$HST] ${_msg} ${_str}: ${_stt}\e[0m\n"
      #echo -e "[${_dat}-$HST] ${_msg} ${_str}: ${_stt}"
    else
      echo -e "\e[33m[${_dat}-$HST]\e[0m \e[37m${_msg} ${_str}: ${_stt}\e[0m"
    fi
    #echo "[${_dat}-$HST] ${_msg} ${_str}: ${_stt}"
  else
    printf "\e[37m${_msg} ${_str}: ${_stt}\e[0m\n"
    #echo "${_msg} ${_str}: ${_stt}"
  fi
}

sshEnvironment() {
  if [ "${TDH_DEPLOYMENT_ENV_NAME}" == "vSphere" ]; then
    JUMP_HOST="${VSPHERE_JUMPHOST_NAME}"

    SSH_USER=$VSPHERE_JUMPHOST_USER
    SSH_HOME=/home/$SSH_USER
    SSH_PRIVATE_KEY=~/.tanzu-demo-hub/KeyPair-Azure.pem
    SSH_PRIVATE_KEY=$VSPHERE_SSH_PRIVATE_KEY_FILE
    SSH_HOST=$JUMP_HOST
    SSH_OPTIONS="-o StrictHostKeyChecking=no"
    SSH_OPTIONS="-o StrictHostKeyChecking=no -o RequestTTY=yes -o ServerAliveInterval=240 -o ClientAliveInterval=240"
    SSH_OPTIONS="-o StrictHostKeyChecking=no -o RequestTTY=yes -o ServerAliveInterval=240"
    SCP_OPTIONS="-o StrictHostKeyChecking=no"
    SSH_TTY="ssh -tt -q $SSH_OPTIONS -i ${SSH_PRIVATE_KEY} ${SSH_USER}@${SSH_HOST}"
    SSH_COMMAND="ssh -q $SSH_OPTIONS -i ${SSH_PRIVATE_KEY} ${SSH_USER}@${SSH_HOST}"
    SCP_COMMAND="scp -i ${SSH_PRIVATE_KEY} -r $SCP_OPTIONS"
    SSH_DISPLAY="ssh -i ${SSH_PRIVATE_KEY} ${SSH_USER}@${SSH_HOST}"
  fi

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "Azure" -o "${TDH_TKGMC_INFRASTRUCTURE}" == "Azure" ]; then
    JUMP_HOST="jump-${TDH_TKGMC_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"

    pip=""
    while [ "$pip" == "" -o "$pip" == "null" ]; do
      pip=$(az network public-ip list -g Admin --query "[?contains(name, 'AdminPublicIP_$TDH_TKGMC_ENVNAME')]" | \
            jq -r '.[].ipAddress')
      sleep 10
    done

    SSH_USER=ubuntu
    SSH_HOME=/home/ubuntu
    SSH_PRIVATE_KEY=~/.tanzu-demo-hub/KeyPair-Azure.pem
    SSH_HOST=$pip
    SSH_OPTIONS="-o StrictHostKeyChecking=no"
    SSH_OPTIONS="-o StrictHostKeyChecking=no -o RequestTTY=yes -o ServerAliveInterval=240 -o ClientAliveInterval=240"
    SSH_OPTIONS="-o StrictHostKeyChecking=no -o RequestTTY=yes -o ServerAliveInterval=240"
    SCP_OPTIONS="-o StrictHostKeyChecking=no"
    SSH_TTY="ssh -tt -q $SSH_OPTIONS -i ${SSH_PRIVATE_KEY} ${SSH_USER}@${SSH_HOST}"
    SSH_COMMAND="ssh -q $SSH_OPTIONS -i ${SSH_PRIVATE_KEY} ${SSH_USER}@${SSH_HOST}"
    SCP_COMMAND="scp -i ${SSH_PRIVATE_KEY} -r $SCP_OPTIONS"
    SSH_DISPLAY="ssh $SSH_OPTIONS -i ${SSH_PRIVATE_KEY} ${SSH_USER}@${SSH_HOST}"
  fi

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "AWS" -o "${TDH_TKGMC_INFRASTRUCTURE}" == "AWS" ]; then
    SSH_HOST=$JUMP_HOST
    SSH_USER=ubuntu
    SSH_HOME=/home/ubuntu
    SSH_KEY_NAME=tanzu-demo-hub
    SSH_KEY_FILE=~/.tanzu-demo-hub/KeyPair-${SSH_KEY_NAME}-${AWS_REGION}.pem
    SSH_OPTIONS="-o StrictHostKeyChecking=no -o RequestTTY=yes -o ServerAliveInterval=30"
    SSH_DISPLAY="-o StrictHostKeyChecking=no"
    SCP_OPTIONS="-o StrictHostKeyChecking=no"
    SSH_TTY="ssh -tt -q $SSH_OPTIONS -i $SSH_KEY_FILE ${SSH_USER}@${SSH_HOST}"
    SSH_COMMAND="ssh -q $SSH_OPTIONS -i $SSH_KEY_FILE ${SSH_USER}@${SSH_HOST}"
    SCP_COMMAND="scp -r $SCP_OPTIONS -i $SSH_KEY_FILE"
    SSH_DISPLAY="ssh -i $SSH_KEY_FILE ${SSH_USER}@${SSH_HOST}"
  fi

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "GCP" ]; then
    INSTANCE_NAME="$(echo $JUMP_HOST | awk -F'.' '{ print $1 }')"
    #ZONE=$(gcloud compute zones list | grep " $GCP_REGION " | head -1 | awk '{ print $1 }')
    ZONE=$(gcloud compute instances list --filter="name=( '$INSTANCE_NAME')" | egrep "^$INSTANCE_NAME" | awk '{ print $2 }')

    prj=$(gcloud projects list | sed '1d' | awk '{ print $1 }')
    prj=${GCP_PROJECT}

    SSH_USER=$(whoami)
    SSH_HOME=/home/$(whoami)
    SSH_HOST="${INSTANCE_NAME}.${ZONE}.${prj}"
    SSH_OPTIONS="-o StrictHostKeyChecking=no -o RequestTTY=yes -o ServerAliveInterval=30"
    SCP_OPTIONS="-o StrictHostKeyChecking=no"
    SSH_TTY="ssh -tt -q $SSH_OPTIONS ${SSH_USER}@${SSH_HOST}"
    SSH_COMMAND="ssh -q $SSH_OPTIONS ${SSH_USER}@${SSH_HOST}"
    SCP_COMMAND="scp -r $SCP_OPTIONS"
    SSH_DISPLAY="ssh ${SSH_USER}@${SSH_HOST}"
  fi

  messagePrint " ▪ SSH Command" "$JUMP_HOST"
  messageLine
  echo "$SSH_DISPLAY"
  messageLine
}

checkCloudCLI() {
  if [ "${TDH_DEPLOYMENT_CLOUD}" == "GCP" ]; then
    # --- CHECK FOR AWS CLI ---
    GCLOUD=$(which gcloud)
    if [ "${GCLOUD}" != "" ]; then
      $GCLOUD --version >/dev/null 2>&1; ret=$?
      if [ ${ret} -eq 0 ]; then
        GCLOUD_VERSION=$($GCLOUD --version 2>/dev/null | grep "Google Cloud SDK" | awk '{ print $NF}')
        GCP_CLI_ENABLED=1
        messagePrint " ▪ checking for gcloud CLI utility" "installed - ${GCLOUD_VERSION}"
      else
        echo ""
        echo "ERROR: The gcloud utility does not seam to be correct"
        echo "       please install gcloud from https://cloud.google.com/sdk/install"; exit 1
      fi
    else
      messagePrint " ▪ checking for gcloud CLI utility" "not installed"
      echo ""
      echo "ERROR: gcloud utility is not installed. please install terraform from"
      echo "       https://cloud.google.com/sdk/install"; exit 1
    fi

    # --- CHECK FOR AWS CLI ---
    AWSCLI=$(which aws)
    if [ "${AWSCLI}" != "" ]; then
      $AWSCLI --version >/dev/null 2>&1; ret=$?
      if [ ${ret} -eq 0 ]; then
        AWS_VERSION=$($AWSCLI --version 2>/dev/null | awk -F'/' '{ print $2 }' | awk '{ print $1}')
        AWS_CLI_ENABLED=1
        messagePrint " ▪ checking for AWS CLI utility (needed for AWS Route53)" "installed - ${AWS_VERSION}"
      else
        echo ""
        echo "ERROR: the AWS cli utility does not seam to be correct, please reinstall it from:"
        echo "       https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html"; exit 1
      fi
    else
      messagePrint " ▪ checking for AWS CLI utility" "not installed"
      echo "ERROR: the AWS cli utility is not installed, please install it from:"
      echo "       https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html"; exit 1
    fi
  fi

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "AWS" -o "$TDH_DEPLOYMENT_ENV_NAME" == "AWS" ]; then
    # --- CHECK FOR AWS CLI ---
    AWSCLI=$(which aws)
    if [ "${AWSCLI}" != "" ]; then
      $AWSCLI --version >/dev/null 2>&1; ret=$?
      if [ ${ret} -eq 0 ]; then
        AWS_VERSION=$($AWSCLI --version 2>/dev/null | awk -F'/' '{ print $2 }' | awk '{ print $1}')
        AWS_CLI_ENABLED=1
        messagePrint " ▪ checking for AWS CLI utility" "installed - ${AWS_VERSION}"
      else
        echo ""
        echo "ERROR: the AWS cli utility does not seam to be correct, please reinstall it from:"
        echo "       https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html"; exit 1
      fi
    else
      messagePrint " ▪ checking for AWS CLI utility" "not installed"
      echo "ERROR: the AWS cli utility is not installed, please install it from:"
      echo "       https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html"; exit 1
    fi
  fi

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "Azure" -o "$TDH_DEPLOYMENT_ENV_NAME" == "Azure" ]; then
    # --- CHECK FOR AWS CLI ---
    AZCLI=$(which az)
    if [ "${AZCLI}" != "" ]; then
      $AZCLI -v >/dev/null 2>&1; ret=$?
      if [ ${ret} -eq 0 ]; then
        AZ_VERSION=$($AZCLI -v 2>/dev/null | egrep "^azure-cli" | awk '{ print $2 }'); ret=$?
        AZURE_CLI_ENABLED=1
        messagePrint " ▪ checking for Azure CLI utility (az)" "installed - ${AZ_VERSION}"
      else
        echo ""
        echo "ERROR: the $cloud CLI $(which az) does not seam to be correct"
        echo "       please install it from https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest"; exit 1
      fi

      # --- CHECK FOR THE CLI VERSION ---
      az vm image terms -h > /dev/null 2>&1; ret=$?

      if [ ${ret} -ne 0 ]; then
        ver=$(az -v 2>/dev/null | egrep "^azure-cli" | awk '{ print $2 }')
        echo "ERROR: Your azure CLI utility is to old ($ver), please update to a more recent one"
        echo "       https://docs.microsoft.com/en-us/cli/azure/update-azure-cli"
        echo "       => brew upgrade az (if version <= 2.11.0)"
        echo "       => az upgrade (if version => 2.11.0)"
        exit
      fi

    else
      messagePrint " ▪ checking for $cloud CLI utility" "no installed"
    fi

    # --- CHECK FOR AWS CLI ---
    AWSCLI=$(which aws)
    if [ "${AWSCLI}" != "" ]; then
      $AWSCLI --version >/dev/null 2>&1; ret=$?
      if [ ${ret} -eq 0 ]; then
        AWS_VERSION=$($AWSCLI --version 2>/dev/null | awk -F'/' '{ print $2 }' | awk '{ print $1}')
        AWS_CLI_ENABLED=1
        messagePrint " ▪ checking for AWS CLI utility (needed for AWS Route53)" "installed - ${AWS_VERSION}"
      else
        echo ""
        echo "ERROR: the AWS cli utility does not seam to be correct, please reinstall it from:"
        echo "       https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html"; exit 1
      fi
    else
      messagePrint " ▪ checking for AWS CLI utility" "not installed"
      echo "ERROR: the AWS cli utility is not installed, please install it from:"
      echo "       https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html"; exit 1
    fi
  fi

  if [ ! -x /usr/local/bin/kp ]; then 
    echo "ERROR: Please install the kp utility from https://network.pivotal.io/products/build-service"
    echo "       to /usr/local/bin/kp"
    echo "       => sudo mv kp-darwin-0.2.0 /usr/local/bin/kp"
    echo "       => sudo chmod a+x /usr/local/bin/kp"
    exit 1
  fi

  if [ ! -x /usr/local/bin/kbld ]; then 
    echo "ERROR: Please install the kp utility from https://network.pivotal.io/products/kbld" 
    echo "       to /usr/local/bin/kbld"
    echo "       => sudo mv kbld-darwin-amd64-0.29.0 /usr/local/bin/kbld"
    echo "       => sudo chmod a+x /usr/local/bin/kbld"
    exit 1
  fi
}

checkKubernetesServices() {
  service=$1
  missing_variables=0

  if [ 1 -eq 1 ]; then
    if [ "${service}" == "cert-manager" ]; then
      if [ "${TDH_CERTMANAGER_EMAIL}" == "" ]; then
        missing_variables=1
        echo ""
        echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
        echo "  -----------------------------------------------------------------------------------------------------------------------------------------------------"

        if [ "${TDH_CERTMANAGER_EMAIL}" == "" ]; then
          echo "  TDH_CERTMANAGER_EMAIL              (required) Personal Email address to receive update from Let's Enscript"
        fi
      fi
    fi

    if [ "${service}" == "tanzu_data_protection" ]; then
      if [ "${TDH_TANZU_DATA_PROTECTION_ARN}" == "" -o "TDH_TANZU_DATA_PROTECTION_BACKUP_LOCATION" == "" ]; then
        missing_variables=1
        echo ""
        echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
        echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

        if [ "${TDH_TANZU_DATA_PROTECTION_ARN}" == "" -o "TDH_TANZU_DATA_PROTECTION_BACKUP_LOCATION" == "" ]; then
          echo "  TDH_TANZU_DATA_PROTECTION_ARN                (required) AWS CloudFormation ARN for Tanzu Data Protection"
          echo "  TDH_TANZU_DATA_PROTECTION_BACKUP_LOCATION    (required) AWS Backup location"
          echo "                                               Create AWS data protection provider credentials"
          echo ""
          echo "                                               1.) TMC -> Administrations -> Accounts"
          echo "                                               2.) Create Account Credentials -> AWS data protection credentils"
          echo "                                                   the Credential Name ie. 'sadubois-aws-dp' should be used as backup location"
          echo "                                                   export TDH_TANZU_DATA_PROTECTION_BACKUP_LOCATION=<credential-name>"
          echo "                                               3.) Follow the instructions to make an AWS ClouFormation deployment and note the"
          echo "                                                   ARN generated. Set the ARN with export TDH_TANZU_DATA_PROTECTION_ARN=<arn>"
          echo ""
        fi
      fi
    fi

    if [ "${service}" == "tanzu_observability" ]; then
      if [ "${TDH_TANZU_OBSERVABILITY_API_TOKEN}" == "" -o "${TDH_TANZU_OBSERVABILITY_URL}" == "" ]; then
        missing_variables=1
        echo ""
        echo "  4MISSING ENVIRONMENT-VARIABES      DESCRIPTION        "
        echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

        if [ "${TDH_TANZU_OBSERVABILITY_API_TOKEN}" == "" ]; then
          echo "  TDH_TANZU_OBSERVABILITY_API_TOKEN  (required) Tanzu Observability API Token"
          echo "                                      => Tanzu Mission Control -> MyAccount -> API Token -> Generate"
        fi

        if [ "${TDH_TANZU_DATA_PROTECTION_ARN}" == "" ]; then
          echo "  TDH_TANZU_OBSERVABILITY_URL        (required) Tanzu Observability Portal URL"
          echo "                                      => ie. https://vmware.wavefront.com"
        fi
      fi
    fi

    if [ "${service}" == "registry_docker" ]; then
      export TDH_REGISTRY_DOCKER_NAME=docker.io

      if [ "${TDH_REGISTRY_DOCKER_NAME}" == "" -o "${TDH_REGISTRY_DOCKER_USER}" == "" -o "${TDH_REGISTRY_DOCKER_PASS}" == "" ]; then
        missing_variables=1
        echo ""
        echo "  MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
        echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

        if [ "${TDH_REGISTRY_DOCKER_NAME}" == "" ]; then
          echo "  TDH_REGISTRY_DOCKER_NAME         (required) Docker Registry (docker.io)"
        fi

        if [ "${TDH_REGISTRY_DOCKER_USER}" == "" ]; then
          echo "  TDH_REGISTRY_DOCKER_USER         (required) Docker User"
        fi

        if [ "${TDH_REGISTRY_DOCKER_PASS}" == "" ]; then
          echo "  TDH_REGISTRY_DOCKER_PASS         (required) Docker Password"
        fi
      fi
    fi

    if [ "${service}" == "github" ]; then
      if [ "${TDH_GITHUB_USER}" == "" -o "${TDH_GITHUB_PASS}" == "" -o "${TDH_GITHUB_TOKEN}" == "" ]; then
        missing_variables=1
        echo ""
        echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
        echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

        if [ "${TDH_GITHUB_USER}" == "" ]; then
          echo "  TDH_GITHUB_USER                  (required) Github User"
        fi

        if [ "${TDH_GITHUB_PASS}" == "" ]; then
          echo "  TDH_GITHUB_PASS                  (required) Github Password"
        fi

        if [ "${TDH_GITHUB_TOKEN}" == "" ]; then
          echo "  TDH_GITHUB_TOKEN                 (required) Github Access Token (https://github.com/settings/tokens)"
        fi
      fi
    fi

    if [ "${service}" == "registry_vmware" ]; then
      if [ "${TDH_REGISTRY_VMWARE_NAME}" == "" -o "${TDH_REGISTRY_VMWARE_USER}" == "" -o "${TDH_REGISTRY_VMWARE_PASS}" == "" ]; then
        missing_variables=1
        echo ""
        echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
        echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

        if [ "${TDH_REGISTRY_VMWARE_NAME}" == "" ]; then
          echo "  TDH_REGISTRY_VMWARE_NAME         (required) VMware Registry (registry.pivotal.io)"
        fi

        if [ "${TDH_REGISTRY_VMWARE_USER}" == "" ]; then
          echo "  TDH_REGISTRY_VMWARE_USER         (required) VMware User"
        fi

        if [ "${TDH_REGISTRY_VMWARE_PASS}" == "" ]; then
          echo "  TDH_REGISTRY_VMWARE_PASS         (required) VMware Password"
        fi
      fi
    fi

    if [ "${service}" == "harbor" ]; then
      if [ "${TDH_SERVICE_REGISTRY_HARBOR}" == "true" ]; then
        if [ "${TDH_HARBOR_ADMIN_PASSWORD}" == "" -o "${TDH_HARBOR_STAGING_TLS_CERT}" == "" ]; then
          missing_variables=1
          echo ""
          echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
          echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

          if [ "${TDH_HARBOR_ADMIN_PASSWORD}" == "" ]; then
            echo "  TDH_HARBOR_ADMIN_PASSWORD        (required) Harbor Admin Password"
          fi

          if [ "${TDH_HARBOR_STAGING_TLS_CERT}" == "" ]; then
            echo "  TDH_HARBOR_STAGING_TLS_CERT      (required) TLS Staging (true) or Productive (false)"
          fi
        fi
      fi
    fi
  fi

  if [ ${missing_variables} -eq 1 ]; then
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    echo "  IMPORTANT: Please set the missing environment variables either in your shell or in the tanzu-demo-hub"
    echo "             configuration file ~/.tanzu-demo-hub.cfg and set all variables with the 'export' notation"
    echo "             ie. => export <environment_variable>=\"<value>\""
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    exit 1
  fi
}

checkCloudAccess() {
  service=$1
  missing_variables=0

  if [ "${TDH_DEPLOYMENT_ENV_NAME}" == "minikube" ]; then
    if [ "${TDH_USER}" == "" -o "${AWS_ACCESS_KEY}" == "" -o "${AWS_SECRET_KEY}" == "" -o "${AWS_REGION}" == "" -o "${AWS_PRIMARY_AZ}" == "" ]; then
      missing_variables=1
      echo ""
      echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
      echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

      if [ "${TDH_USER}" == "" ]; then
        echo "  TDH_USER                        (required) Tanzu Demo Hub User to tag TKG Clusters with your Name"
        echo "                                   => (usually you VMware User Name without @vmware.com)"
      fi

      if [ "${AWS_ACCESS_KEY}" == "" ]; then
        echo "  AWS_ACCESS_KEY                  (required) AWS Acess Key for Route53 Access"
      fi

      if [ "${AWS_SECRET_KEY}" == "" ]; then
        echo "  AWS_SECRET_KEY                  (required) AWS Secret Key for Route53 Access"
      fi

      if [ "${AWS_REGION}" == "" ]; then
        echo "  AWS_REGION                    (required) Choose the AWS Region where your installation should run"
        echo "                                  => aws ec2 --region=eu-west-3 describe-regions --output text | awk '{ print $NF }'"
        echo "                                     eu-north-1, eu-west-3, eu-central-1 etc."
      fi

      if [ "${AWS_PRIMARY_AZ}" == "" ]; then
        echo "  AWS_PRIMARY_AZ                (required) Choose the Primary Availability Zone of a AWS Region"
        echo "                                  => aws ec2 --region=eu-west-3 describe-availability-zones --output text"
        echo "                                     eu-north-1a, eu-north-1b, eu-north-1c"
      fi
    else
      if [ -f ~/.minikube/profiles/$TDH_MINIKUBE_PROFILE/config.json ]; then 
        cpu=$(cat ~/.minikube/profiles/$TDH_MINIKUBE_PROFILE/config.json | jq -r '.CPUs')
        dsk=$(cat ~/.minikube/profiles/$TDH_MINIKUBE_PROFILE/config.json | jq -r '.DiskSize')
        mem=$(cat ~/.minikube/profiles/$TDH_MINIKUBE_PROFILE/config.json | jq -r '.Memory')
        drv=$(cat ~/.minikube/profiles/$TDH_MINIKUBE_PROFILE/config.json | jq -r '.Driver')
      else
        echo "ERROR: Minukube is not running, start it first"
        #echo "       => minikube -p $TDH_MINIKUBE_PROFILE start --driver=hyperkit --cpus $TDH_MINIKUBE_CONFIG_CPU --memory $TDH_MINIKUBE_CONFIG_MEMORY --disk-size $TDH_MINIKUBE_CONFIG_DISK --nodes=$TDH_MINIKUBE_NODES"; exit 1
        echo "       => minikube -p $TDH_MINIKUBE_PROFILE start --driver=hyperkit --cpus $TDH_MINIKUBE_CONFIG_CPU --memory $TDH_MINIKUBE_CONFIG_MEMORY --disk-size $TDH_MINIKUBE_CONFIG_DISK"; exit 1
      fi

      stt=$(minikube status -p $TDH_MINIKUBE_PROFILE| grep "^host:" | awk '{ print $NF }' | head -1)
      messageTitle "MiniKube Status and Configuration"
      messagePrint " ▪ MiniKube Profile"                "$TDH_MINIKUBE_PROFILE"
      messagePrint " ▪ MiniKube Version"                "$(minikube version | head -1 | awk '{ print $NF }')"
      messagePrint " ▪ MiniKube Status"                 "$stt"
      messagePrint " ▪ Minikube Disk"                  "$dsk"
      messagePrint " ▪ Minikube Memory"                "$mem"
      messagePrint " ▪ Minikube CPU's"                 "$cpu"
      messagePrint " ▪ Minikube Driver"                "$drv"
      messageTitle " ▪ Minikube Nodes"              
      echo "   ------------------------------------------------------------------------"
      minikube -p $TDH_MINIKUBE_PROFILE node list | sed 's/^/   /g'
      echo "   ------------------------------------------------------------------------"

      if [ "$stt" == "Stopped" -o "$stt" == "" ]; then 
        echo "ERROR: Minukube is not running, start it first"
        echo "       => minikube -p $TDH_MINIKUBE_PROFILE start --driver=hyperkit --cpus $TDH_MINIKUBE_CONFIG_CPU --memory $TDH_MINIKUBE_CONFIG_MEMORY --disk-size $TDH_MINIKUBE_CONFIG_DISK"; exit 1
        echo "       => cat ~/.minikube/profiles/tanzu-demo-hub/config.json"
      fi

      messagePrint " ▪ MiniKube ServerIP"               "$(minikube ip -p $TDH_MINIKUBE_PROFILE)"
    fi
  fi

  if [ "${TDH_DEPLOYMENT_ENV_NAME}" == "vSphere" -a "$TDH_TKGMC_CREATE_JUMPHOST" == "false" ]; then
    if [ "${TDH_TKGMC_TKG_TYPE}" == "tkgs" ]; then
      if [ "${VSPHERE_TKGS_VCENTER_ADMIN}" == "" -o "${VSPHERE_TKGS_VCENTER_PASSWORD}" == "" -o "${VSPHERE_TKGS_VCENTER_SERVER}" == "" -o \
           "${VSPHERE_TKGS_JUMPHOST_NAME}" == "" -o "${VSPHERE_TKGS_JUMPHOST_USER}" == "" -o \
           "${VSPHERE_TKGS_SSH_PRIVATE_KEY_FILE}" == "" -o "${VSPHERE_TKGS_SSH_PUBLIC_KEY_FILE}" == "" ]; then
  
        missing_variables=1
        echo ""
        echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
        echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
  
        if [ "${VSPHERE_TKGS_VCENTER_SERVER}" == "" ]; then
          echo "  VSPHERE_TKGS_VCENTER_SERVER           (required) vSphere Server Name"
        fi
  
        if [ "${VSPHERE_TKGS_VCENTER_ADMIN}" == "" ]; then
          echo "  VSPHERE_TKGS_VCENTER_ADMIN            (required) vCenter Amdinistrator"
        fi
  
        if [ "${VSPHERE_TKGS_VCENTER_PASSWORD}" == "" ]; then
          echo "  VSPHERE_TKGS_VCENTER_PASSWORD         (required) vCenter Admin Password"
        fi
  
        if [ "${VSPHERE_TKGS_SSH_PRIVATE_KEY_FILE}" == "" ]; then
          echo "  VSPHERE_TKGS_SSH_PRIVATE_KEY_FILE     (required) SSH Private Key File"
        fi

        if [ "${VSPHERE_TKGS_SSH_PUBLIC_KEY_FILE}" == "" ]; then
          echo "  VSPHERE_TKGS_SSH_PUBLIC_KEY_FILE      (required) SSH Public Key File"
        fi
  
        if [ "${VSPHERE_TKGS_JUMPHOST_NAME}" == "" ]; then
          echo "  VSPHERE_TKGS_JUMPHOST_NAME            (required) vSphere Jump Host Name"
        fi
  
        if [ "${VSPHERE_TKGS_JUMPHOST_USER}" == "" ]; then
          echo "  VSPHERE_TKGS_JUMPHOST_USER            (required) vSphere Jump Host User"
        fi
      else
        echo "vSphere Access Credentials (TKGs)"
        messagePrint " ▪ vCenter Server Name"                "$VSPHERE_TKGS_VCENTER_SERVER"
        messagePrint " ▪ vCenter Admin User"                 "$VSPHERE_TKGS_VCENTER_ADMIN"
        messagePrint " ▪ vCenter Admin Password"             $(maskPassword "$VSPHERE_TKGS_VCENTER_PASSWORD")
        messagePrint " ▪ Jump Host Name"                     "$VSPHERE_TKGS_JUMPHOST_NAME"
        messagePrint " ▪ Jump Host User"                     "$VSPHERE_TKGS_JUMPHOST_USER"
        messagePrint " ▪ SSH Private Key"                    "$VSPHERE_TKGS_SSH_PRIVATE_KEY_FILE"
        messagePrint " ▪ SSH Public Key"                     "$VSPHERE_TKGS_SSH_PUBLIC_KEY_FILE"
      fi

      export VSPHERE_DNS_DOMAIN="$VSPHERE_TKGS_DNS_DOMAIN"
      export VSPHERE_API_SERVER="$VSPHERE_TKGS_API_SERVER"
      export VSPHERE_VCENTER_SERVER="$VSPHERE_TKGS_VCENTER_SERVER"
      export VSPHERE_VCENTER_ADMIN="$VSPHERE_TKGS_VCENTER_ADMIN"
      export VSPHERE_VCENTER_PASSWORD="$VSPHERE_TKGS_VCENTER_PASSWORD"
      export VSPHERE_JUMPHOST_NAME="$VSPHERE_TKGS_JUMPHOST_NAME"
      export VSPHERE_JUMPHOST_USER="$VSPHERE_TKGS_JUMPHOST_USER"
      export VSPHERE_JUMPHOST_PASSWORD="$VSPHERE_TKGS_JUMPHOST_PASSWORD"
      export VSPHERE_SSH_PRIVATE_KEY_FILE="$VSPHERE_TKGS_SSH_PRIVATE_KEY_FILE"
      export VSPHERE_SSH_PUBLIC_KEY_FILE="$VSPHERE_TKGS_SSH_PUBLIC_KEY_FILE"
      export VSPHERE_DATASTORE="$VSPHERE_TKGS_DATASTORE"
      export VSPHERE_DATACENTER="$VSPHERE_TKGS_DATACENTER"
      export VSPHERE_CLUSTER="$VSPHERE_TKGS_CLUSTER"
      export VSPHERE_NETWORK="$VSPHERE_TKGS_NETWORK"
    else
      if [ "${VSPHERE_TKGM_VCENTER_ADMIN}" == "" -o "${VSPHERE_TKGM_VCENTER_PASSWORD}" == "" -o "${VSPHERE_TKGM_VCENTER_SERVER}" == "" -o \
           "${VSPHERE_TKGM_JUMPHOST_NAME}" == "" -o "${VSPHERE_TKGM_JUMPHOST_USER}" == "" -o "${VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT}" == "" -o \
           "${VSPHERE_TKGM_SUBNET}" == "" -o \
           "${VSPHERE_TKGM_WKLD_CLUSTER01_CONTROL_PLANE}" == "" -o "${VSPHERE_TKGM_WKLD_CLUSTER01_LOADBALANCER_POOL}" == "" -o \
           "${VSPHERE_TKGM_WKLD_CLUSTER02_CONTROL_PLANE}" == "" -o "${VSPHERE_TKGM_WKLD_CLUSTER02_LOADBALANCER_POOL}" == "" -o \
           "${VSPHERE_TKGM_WKLD_CLUSTER03_CONTROL_PLANE}" == "" -o "${VSPHERE_TKGM_WKLD_CLUSTER03_LOADBALANCER_POOL}" == "" -o \
           "${VSPHERE_TKGM_SSH_PRIVATE_KEY_FILE}" == "" -o "${VSPHERE_TKGM_SSH_PUBLIC_KEY_FILE}" == "" ]; then

        missing_variables=1
        echo ""
        echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
        echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

        if [ "${VSPHERE_TKGM_VCENTER_SERVER}" == "" ]; then
          echo "  VSPHERE_TKGM_VCENTER_SERVER                      (required) vSphere Server Name"
        fi

        if [ "${VSPHERE_TKGM_VCENTER_ADMIN}" == "" ]; then
          echo "  VSPHERE_TKGM_VCENTER_ADMIN                       (required) vCenter Amdinistrator"
        fi

        if [ "${VSPHERE_TKGM_VCENTER_PASSWORD}" == "" ]; then
          echo "  VSPHERE_TKGM_VCENTER_PASSWORD                    (required) vCenter Admin Password"
        fi

        if [ "${VSPHERE_TKGM_SSH_PRIVATE_KEY_FILE}" == "" ]; then
          echo "  VSPHERE_TKGM_SSH_PRIVATE_KEY_FILE                (required) SSH Private Key File"
        fi

        if [ "${VSPHERE_TKGM_SSH_PUBLIC_KEY_FILE}" == "" ]; then
          echo "  VSPHERE_TKGM_SSH_PUBLIC_KEY_FILE                 (required) SSH Public Key File"
        fi

        if [ "${VSPHERE_TKGM_JUMPHOST_NAME}" == "" ]; then
          echo "  VSPHERE_TKGM_JUMPHOST_NAME                       (required) vSphere Jump Host Name"
        fi

        if [ "${VSPHERE_TKGM_JUMPHOST_USER}" == "" ]; then
          echo "  VSPHERE_TKGM_JUMPHOST_USER                       (required) vSphere Jump Host User"
        fi

        if [ "${VSPHERE_TKGM_SUBNET}" == "" ]; then
          echo "  VSPHERE_TKGM_SUBNET                              (required) vSphere VM Subnet (ie: 10.213.108)"
        fi

        if [ "${VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT}" == "" ]; then
          echo "  VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT              (required) TKG Management Cluster Control Plance IP Adddress"
        fi

        if [ "${VSPHERE_TKGM_WKLD_CLUSTER01_CONTROL_PLANE}" == "" -o "${VSPHERE_TKGM_WKLD_CLUSTER01_LOADBALANCER_POOL}" == "" ]; then
          echo "  TKG Workload Cluster 01"                        
          echo "  - VSPHERE_TKGM_WKLD_CLUSTER01_CONTROL_PLANE      (required) Control Plane IP"
          echo "  - VSPHERE_TKGM_WKLD_CLUSTER01_LOADBALANCER_POOL  (required) LoadBalancer Pool"
          echo "  => ie. set following variables in ~/.tanzu-demo-hub.cfg and adjust IP if needed"
          echo "    export VSPHERE_TKGM_WKLD_CLUSTER01_CONTROL_PLANE=\${VSPHERE_TKGM_SUBNET}.111"
          echo "    export VSPHERE_TKGM_WKLD_CLUSTER01_LOADBALANCER_POOL=\${VSPHERE_TKGM_SUBNET}.115-\${VSPHERE_TKGM_SUBNET}.119"
          echo ""
        fi

        if [ "${VSPHERE_TKGM_WKLD_CLUSTER02_CONTROL_PLANE}" == "" -o "${VSPHERE_TKGM_WKLD_CLUSTER02_LOADBALANCER_POOL}" == "" ]; then
          echo "  TKG Workload Cluster 02"                         
          echo "  - VSPHERE_TKGM_WKLD_CLUSTER02_CONTROL_PLANE      (required) Control Plane IP"
          echo "  - VSPHERE_TKGM_WKLD_CLUSTER02_LOADBALANCER_POOL  (required) LoadBalancer Pool"
          echo "  => ie. set following variables in ~/.tanzu-demo-hub.cfg and adjust IP if needed"
          echo "     export VSPHERE_TKGM_WKLD_CLUSTER02_CONTROL_PLANE=\${VSPHERE_TKGM_SUBNET}.121"
          echo "     export VSPHERE_TKGM_WKLD_CLUSTER02_LOADBALANCER_POOL=\${VSPHERE_TKGM_SUBNET}.125-\${VSPHERE_TKGM_SUBNET}.129"
          echo ""
        fi

        if [ "${VSPHERE_TKGM_WKLD_CLUSTER03_CONTROL_PLANE}" == "" -o "${VSPHERE_TKGM_WKLD_CLUSTER03_LOADBALANCER_POOL}" == "" ]; then
          echo "  TKG Workload Cluster 03"                         
          echo "  - VSPHERE_TKGM_WKLD_CLUSTER03_CONTROL_PLANE      (required) Control Plane IP"
          echo "  - VSPHERE_TKGM_WKLD_CLUSTER03_LOADBALANCER_POOL  (required) LoadBalancer Pool"
          echo "  => ie. set following variables in ~/.tanzu-demo-hub.cfg and adjust IP if needed"
          echo "    export VSPHERE_TKGM_WKLD_CLUSTER03_CONTROL_PLANE=\${VSPHERE_TKGM_SUBNET}.131"
          echo "    export VSPHERE_TKGM_WKLD_CLUSTER03_LOADBALANCER_POOL=\${VSPHERE_TKGM_SUBNET}.135-\${VSPHERE_TKGM_SUBNET}.139"
          echo ""
        fi

      else
        echo "vSphere Access Credentials"
        messagePrint " ▪ vCenter Server Name"                   "$VSPHERE_TKGM_VCENTER_SERVER"
        messagePrint " ▪ vCenter Admin User"                    "$VSPHERE_TKGM_VCENTER_ADMIN"
        messagePrint " ▪ vCenter Admin Password"                $(maskPassword "$VSPHERE_TKGM_VCENTER_PASSWORD")
        messagePrint " ▪ TKG Management Cluster IP"             "$VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT"
        messagePrint " ▪ Jump Host Name"                        "$VSPHERE_TKGM_JUMPHOST_NAME"
        messagePrint " ▪ Jump Host User"                        "$VSPHERE_TKGM_JUMPHOST_USER"
        messagePrint " ▪ SSH Private Key"                       "$VSPHERE_TKGM_SSH_PRIVATE_KEY_FILE"
        messagePrint " ▪ SSH Public Key"                        "$VSPHERE_TKGM_SSH_PUBLIC_KEY_FILE"
        messagePrint " ▪ TKG Management Custer Control Plane"   "$VSPHERE_TKGM_CLUSTER_CONTROL_PLANE"
        messagePrint " ▪ TKG Workload Cluster 01"               "NAME_TAG: TKG_CLUSTER_01"
        messagePrint " ▪     Cluster Control Plane"             "$VSPHERE_TKGM_WKLD_CLUSTER01_CONTROL_PLANE"
        messagePrint " ▪     LoadBalancer IP Pool"              "$VSPHERE_TKGM_WKLD_CLUSTER01_LOADBALANCER_POOL"
        messagePrint " ▪ TKG Workload Cluster 02"               "NAME_TAG: TKG_CLUSTER_02"
        messagePrint " ▪     Cluster Control Plane"             "$VSPHERE_TKGM_WKLD_CLUSTER02_CONTROL_PLANE"
        messagePrint " ▪     LoadBalancer IP Pool"              "$VSPHERE_TKGM_WKLD_CLUSTER02_LOADBALANCER_POOL"
        messagePrint " ▪ TKG Workload Cluster 03"               "NAME_TAG: TKG_CLUSTER_03"
        messagePrint " ▪     Cluster Control Plane"             "$VSPHERE_TKGM_WKLD_CLUSTER03_CONTROL_PLANE"
        messagePrint " ▪     LoadBalancer IP Pool"              "$VSPHERE_TKGM_WKLD_CLUSTER03_LOADBALANCER_POOL"
      fi

      # --- VERIFY VSPHERE_TKGM_VCENTER_SERVER ADRESS ---
#      vsp=$(nslookup $VSPHERE_TKGM_VCENTER_SERVER | grep "Address:" | tail -1 | awk '{ print $NF }')
#
#      vsp_a=$(echo $vsp | awk -F'.' '{ print $1 }')
#      vsp_b=$(echo $vsp | awk -F'.' '{ print $2 }')
#      vsp_c=$(echo $vsp | awk -F'.' '{ print $3 }')
#
#      cpe_a=$(echo $VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT | awk -F'.' '{ print $1 }')
#      cpe_b=$(echo $VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT | awk -F'.' '{ print $2 }')
#      cpe_c=$(echo $VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT | awk -F'.' '{ print $3 }')
#
#     if [ "$vsp_a" == "$cpe_a" ]; then 
#       if [ "$vsp_b" == "$cpe_b" ]; then 
#         if [ "$vsp_c" != "$cpe_c" ]; then 
#           echo "ERROR: Control Pane IP Addres (VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT) defined in (~/.tanzu-demo-hub.cfg)"
#           echo "       is not in the same ip range as the vCenter Server $VSPHERE_TKGM_VCENTER_SERVER"
#           printf "       => %-16s %s\n" $vsp "VSPHERE_TKGM_VCENTER_SERVER=$VSPHERE_TKGM_VCENTER_SERVER)"
#           printf "       => %-16s %s\n" $VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT "VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT (~/.tanzu-demo-hub.cfg)"
#         fi
#       else
#         echo "ERROR: Control Pane IP Addres (VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT) defined in (~/.tanzu-demo-hub.cfg)"
#         echo "       is not in the same ip range as the vCenter Server $VSPHERE_TKGM_VCENTER_SERVER"
#         printf "       => %-16s %s\n" $vsp "VSPHERE_TKGM_VCENTER_SERVER=$VSPHERE_TKGM_VCENTER_SERVER)"
#         printf "       => %-16s %s\n" $VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT "VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT (~/.tanzu-demo-hub.cfg)"
#       fi
#     else
#       echo "ERROR: Control Pane IP Addres (VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT) defined in (~/.tanzu-demo-hub.cfg)"
#       echo "       is not in the same ip range as the vCenter Server $VSPHERE_TKGM_VCENTER_SERVER"
#       printf "       => %-16s %s\n" $vsp "VSPHERE_TKGM_VCENTER_SERVER=$VSPHERE_TKGM_VCENTER_SERVER)"
#       printf "       => %-16s %s\n" $VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT "VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT (~/.tanzu-demo-hub.cfg)"
#     fi

      export VSPHERE_DNS_DOMAIN="$VSPHERE_TKGM_DNS_DOMAIN"
      export VSPHERE_API_SERVER="$VSPHERE_TKGM_API_SERVER"
      export VSPHERE_VCENTER_SERVER="$VSPHERE_TKGM_VCENTER_SERVER"
      export VSPHERE_VCENTER_ADMIN="$VSPHERE_TKGM_VCENTER_ADMIN"
      export VSPHERE_VCENTER_PASSWORD="$VSPHERE_TKGM_VCENTER_PASSWORD"
      export VSPHERE_JUMPHOST_NAME="$VSPHERE_TKGM_JUMPHOST_NAME"
      export VSPHERE_JUMPHOST_USER="$VSPHERE_TKGM_JUMPHOST_USER"
      export VSPHERE_JUMPHOST_PASSWORD="$VSPHERE_TKGM_JUMPHOST_PASSWORD"
      export VSPHERE_SSH_PRIVATE_KEY_FILE="$VSPHERE_TKGM_SSH_PRIVATE_KEY_FILE"
      export VSPHERE_SSH_PUBLIC_KEY_FILE="$VSPHERE_TKGM_SSH_PUBLIC_KEY_FILE"
      export VSPHERE_DATASTORE="$VSPHERE_TKGM_DATASTORE"
      export VSPHERE_DATACENTER="$VSPHERE_TKGM_DATACENTER"
      export VSPHERE_CLUSTER="$VSPHERE_TKGM_CLUSTER"
      export VSPHERE_NETWORK="$VSPHERE_TKGM_NETWORK"
    fi
  fi

  if [ "${TDH_DEPLOYMENT_ENV_NAME}" == "vSphere" -a "$TDH_TKGMC_CREATE_JUMPHOST" == "true1" ]; then
    if [ "${VSPHERE_ADMIN}" == "" -o "${VSPHERE_PASSWORD}" == "" -o "${VSPHERE_SERVER}" == "" -o \
         "${VSPHERE_DATACENTER}" == "" -o "${VSPHERE_DATASTORE}" == "" -o "${VSPHERE_CLUSTER}" == "" -o \
         "${VSPHERE_SSH_PRIVATE_KEY_FILE}" == "" -o "${VSPHERE_SSH_PUBLIC_KEY_FILE}" == "" -o \
         "${VSPHERE_CONTROLPLANE_IP}" == "" -o \
         "${VSPHERE_MANAGEMENT_NETWORK}" == "" -o "${VSPHERE_WAN_NETWORK}" == "" -o "${VSPHERE_CLUSTER}" == "" ]; then
      missing_variables=1
      echo ""
      echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
      echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

      if [ "${VSPHERE_SERVER}" == "" ]; then
        echo "  VSPHERE_SERVER                   (required) vSphere Server Name"
      fi

      if [ "${VSPHERE_CONTROLPLANE_IP}" == "" ]; then
        echo "  VSPHERE_CONTROLPLANE_IP          (required) TKG Controlplance IP"
      fi

      if [ "${VSPHERE_ADMIN}" == "" ]; then
        echo "  VSPHERE_ADMIN                    (required) vSphere Amdinistrator"
      fi

      if [ "${VSPHERE_PASSWORD}" == "" ]; then
        echo "  VSPHERE_PASSWORD                 (required) vSphere Admin Password"
      fi

      if [ "${VSPHERE_DATACENTER}" == "" ]; then
        echo "  VSPHERE_DATACENTER               (required) vSphere Datacenter"
      fi

      if [ "${VSPHERE_DATASTORE}" == "" ]; then
        echo "  VSPHERE_DATASTORE                (required) vSphere Datastore"
      fi

      if [ "${VSPHERE_CLUSTER}" == "" ]; then
        echo "  VSPHERE_CLUSTER                  (required) vSphere Cluster"
      fi

      if [ "${VSPHERE_MANAGEMENT_NETWORK}" == "" ]; then
        echo "  VSPHERE_MANAGEMENT_NETWORK       (required) vSphere Management Network"
      fi

      if [ "${VSPHERE_WAN_NETWORK}" == "" ]; then
        echo "  VSPHERE_WAN_NETWORK              (required) vSphere WAN Network"
      fi

      if [ "${VSPHERE_SSH_PRIVATE_KEY_FILE}" == "" ]; then
        echo "  VSPHERE_SSH_PRIVATE_KEY_FILE     (required) SSH Private Key File"
      fi
    
      if [ "${VSPHERE_SSH_PUBLIC_KEY_FILE}" == "" ]; then
        echo "  VSPHERE_SSH_PUBLIC_KEY_FILE      (required) SSH Public Key File"
      fi
    else
      echo "vSphere Access Credentials"
      messagePrint " ▪ vSphere Server Name"                "$VSPHERE_SERVER"
      messagePrint " ▪ vSphere Admin User"                 "$VSPHERE_ADMIN"
      messagePrint " ▪ vSphere Admin Password"             $(maskPassword "$VSPHERE_PASSWORD")
      messagePrint " ▪ vSphere Datacenter"                 "$VSPHERE_DATACENTER"
      messagePrint " ▪ vSphere Datastore"                  "$VSPHERE_DATASTORE"
      messagePrint " ▪ vSphere Cluster NAme"               "$VSPHERE_CLUSTER"
      messagePrint " ▪ vSphere Management Network"         "$VSPHERE_MANAGEMENT_NETWORK"
      messagePrint " ▪ vSphere WAN Network"                "$VSPHERE_WAN_NETWORK"
      messagePrint " ▪ SSH Private Key"                    "$VSPHERE_SSH_PRIVATE_KEY_FILE"
      messagePrint " ▪ SSH Public Key"                     "$VSPHERE_SSH_PUBLIC_KEY_FILE"
    fi

    # --- VERIFY OVA-IMAGES / OVFTOOL BUNDLE ---
    found=1; file_list=""
    for n in $TDH_TKGMC_TKG_IMAGES; do
      if [ ! -f $n ]; then found=0; file_list="$file_list $n"; fi
    done

    if [ $found -eq 0 ]; then 
      txt="ERROR: Missing OVA Images for TKG:"
      for n in $file_list; do 
        echo "$txt $n"; txt="                                  "
      done

      echo "Please download them from the VMware Download page:"
      echo "=> https://my.vmware.com/group/vmware/downloads/details?downloadGroup=TKG-121&productId=988"
      exit 1
    fi

    if [ ! -f $TDH_TKGMC_OVFTOOL_BUNDLE ]; then 
      echo "ERROR: Missing OVFTOOL Bundle, please download it from the VMware Download page:"
      echo "=> https://my.vmware.com/group/vmware/downloads/details?downloadGroup=OVFTOOL441&productId=734"
    fi
  fi

  if [ "${TDH_DEPLOYMENT_ENV_NAME}" == "GCP" ]; then
    #if [ "${GCP_SERVICE_ACCOUNT}" == "" -o "${GCP_REGION}" == "" -o "${GCP_PROJECT}" == "" ]; then
    if [ "${GCP_REGION}" == "" -o "${GCP_PROJECT}" == "" ]; then
      missing_variables=1
      echo ""
      echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
      echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

      #if [ "${GCP_SERVICE_ACCOUNT}" == "" ]; then
      #  echo "  GCP_SERVICE_ACCOUNT           (required) GCP Service Account"
      #fi

      if [ "${GCP_PROJECT}" == "" ]; then
        echo "  GCP_PROJECT                   (required) GCP Project"
        echo "                                  => gcloud projects list | sed '1d' | awk '{ print \$1 }'"
      fi

      if [ "${GCP_REGION}" == "" ]; then
        echo "  GCP_REGION                    (required) Choose the GCP Region where your installation should run"
        echo "                                  => gcloud compute zones list | sed '1d' | awk '{ print $2 }' | sort -u"
        echo "                                     europe-west1, us-east1, europe-west4  etc."
      fi
    else
      echo "GCP Access Credentials"
      #messagePrint " ▪ GCP Service Account"          "$GCP_SERVICE_ACCOUNT"
      messagePrint " ▪ GCP Project"                  "$GCP_PROJECT"
      messagePrint " ▪ GCP Region"                   "$GCP_REGION"
    fi
  fi

  if [ "${TDH_DEPLOYMENT_ENV_NAME}" == "AWS" ]; then
    if [ "${AWS_ACCESS_KEY}" == "" -o "${AWS_SECRET_KEY}" == "" -o "${AWS_PRIMARY_AZ}" == "" -o \
         "${AWS_REGION}" == "" ]; then

      missing_variables=1
      echo ""
      echo "  5MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
      echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

      if [ "${AWS_ACCESS_KEY}" == "" ]; then
        echo "  AWS_ACCESS_KEY                  (required) AWS Acess Key"
      fi

      if [ "${AWS_SECRET_KEY}" == "" ]; then
        echo "  AWS_SECRET_KEY                  (required) AWS Secret Key"
      fi

      if [ "${AWS_REGION}" == "" ]; then
        echo "  AWS_REGION                    (required) Choose the AWS Region where your installation should run"
        echo "                                  => aws ec2 --region=eu-west-3 describe-regions --output text | awk '{ print $NF }'"
        echo "                                     eu-north-1, eu-west-3, eu-central-1 etc."
      fi

      if [ "${AWS_PRIMARY_AZ}" == "" ]; then
        echo "  AWS_PRIMARY_AZ                (required) Choose the Primary Availability Zone of a AWS Region"
        echo "                                  => aws ec2 --region=eu-west-3 describe-availability-zones --output text"
        echo "                                     eu-north-1a, eu-north-1b, eu-north-1c"
      fi
    else
      messageTitle "AWS Access Credentials"
      messagePrint " ▪ AWS AccwssKey"                    $(maskPassword "$AWS_ACCESS_KEY")
      messagePrint " ▪ AWS SecretKey"                    $(maskPassword "$AWS_SECRET_KEY")
      messagePrint " ▪ AWS Region"                       "$AWS_REGION"
      messagePrint " ▪ AWS Primary Availability Zone"    "$AWS_PRIMARY_AZ"
    fi
  fi

  if [ "${TDH_DEPLOYMENT_ENV_NAME}" == "Azure" ]; then
    if [ "${AZURE_SUBSCRIPTION_ID}" == "" -o "${AZURE_TENANT_ID}" == "" -o "${AZURE_LOCATION}" == "" ]; then

      missing_variables=1
      echo ""
      echo "  1MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
      echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

      if [ "${AZURE_SUBSCRIPTION_ID}" == "" ]; then
        echo "  AZURE_SUBSCRIPTION_ID           (required) has the format <xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx>"
      fi

      if [ "${AZURE_TENANT_ID}" == "" ]; then
        echo "  AZURE_TENANT_ID                 (required) has the format <xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx>"
      fi

      #if [ "${AZURE_CLIENT_ID}" == "" ]; then
      #  echo "  AZURE_CLIENT_ID                 (required) has the format <xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx>"
      #  echo "                                  => az ad sp create-for-rbac --name \"TanzuDemoHub-$TDH_USER\" ## appId"
      #fi

      #if [ "${AZURE_CLIENT_SECRET}" == "" ]; then
      #  echo "  AZURE_CLIENT_SECRET             (required) has the format <xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx>"
      #  echo "                                  => az ad sp create-for-rbac --name \"TanzuDemoHub-$TDH_USER\" ## password"
      #fi

      if [ "${AZURE_LOCATION}" == "" ]; then
        echo "  AZURE_LOCATION                    (required) Choose the Azure Region where your installation should run"
        echo "                                  => az account list-locations -o table | awk '{ print $NF }'"
        echo "                                     westeurope, northeurope, switzerlandnorth etc."
      fi
    else
      messageTitle "Azure Access Credentials"
      messagePrint " ▪ Azure SubscriptionId"         $(maskPassword "$AZURE_SUBSCRIPTION_ID")
      messagePrint " ▪ Azure TennantId"              $(maskPassword "$AZURE_TENANT_ID")
      #messagePrint " ▪ Azure ClientId"               $(maskPassword "$AZURE_CLIENT_ID")
      #messagePrint " ▪ Azure Client Secret"          $(maskPassword "$AZURE_CLIENT_SECRET")
      messagePrint " ▪ Azure Region"                 "$AZURE_LOCATION"

      az ad app list > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "Please login to Microsoft Azure as described below:"
        az login --tenant $AZURE_TENANT_ID -o none
      fi
    fi
  fi

  if [ ${missing_variables} -eq 1 ]; then
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    echo "  IMPORTANT: Please set the missing environment variables either in your shell or in the tanzu-demo-hub"
    echo "             configuration file ~/.tanzu-demo-hub.cfg and set all variables with the 'export' notation"
    echo "             ie. => export <environment_variable>=\"<value>\""
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    exit 1
  fi
}

checkTDHenvironment() {
  missing_variables=0

  if [ "${TDH_MYVMWARE_USER}" == "" -o "${TDH_MYVMWARE_PASS}" == "" -o "${PCF_PIVNET_TOKEN}" == "" ]; then
    missing_variables=1
    echo ""
    echo "  ?MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

    if [ "${PCF_PIVNET_TOKEN}" == "" ]; then
      echo "  PCF_PIVNET_TOKEN                 (required) PIVNET Access Token to download software. Get a UAA API TOKEN from"
      echo "                                   => https://network.pivotal.io/users/dashboard/edit-profile"
    fi

    if [ "${TDH_MYVMWARE_USER}" == "" ]; then
      echo "  TDH_MYVMWARE_USER                (required) myVMware (http://my.vmware.com/group/vmware/home) User"
    fi

    if [ "${TDH_MYVMWARE_PASS}" == "" ]; then
      echo "  TDH_MYVMWARE_PASS                (required) myVMware (http://my.vmware.com/group/vmware/home) Password"
    fi
#  else
#    messageTitle "Pivotal Network Access (network.pivotal.io)"
#    messagePrint " ▪ Pivotal Network Token"    $(maskPassword "$PCF_PIVNET_TOKEN")
#    messageTitle "myVMware (http://my.vmware.com) Access for Software Download"
#    messagePrint " ▪ myVMare User:"            "$TDH_MYVMWARE_USER"
#    messagePrint " ▪ myVMare Password:"        $(maskPassword "$TDH_MYVMWARE_PASS")
  fi

  if [ ${missing_variables} -eq 1 ]; then
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    echo "  IMPORTANT: Please set the missing environment variables either in your shell or in the tanzu-demo-hub"
    echo "             configuration file ~/.tanzu-demo-hub.cfg and set all variables with the 'export' notation"
    echo "             export TDH_MYVMWARE_USER=xxxxxxx"
    echo "             export TDH_MYVMWARE_PASS='xxxxxxxxxxxx'"
    echo "  IMPORTANT: Place the password in Single Quotes (') especialy if you use special characgters"
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    exit 1
  fi
}

checkTMCintegration() {
  TMC_INTEGRATION=$1

  TDH_DEPLOYMENT_TYPE=$(getConfigMap tanzu-demo-hub TDH_DEPLOYMENT_TYPE)
  TDH_CLUSTER_NAME=$(getConfigMap tanzu-demo-hub TDH_CLUSTER_NAME)
  TDH_CLUSTER_GROUP=$(getConfigMap tanzu-demo-hub TDH_CLUSTER_GROUP)
  TDH_MANAGEMENT_CLUSTER=$(getConfigMap tanzu-demo-hub TDH_MANAGEMENT_CLUSTER)
  TDH_PROVISIONER_NAME=$(getConfigMap tanzu-demo-hub TDH_PROVISIONER_NAME)

  # Clusters created NOT with aws_hosted management-clusters show as attached
  [ "$TDH_DEPLOYMENT_TYPE" == "tmc" ] && MGMG_CLUSTER="$TDH_MANAGEMENT_CLUSTER" || MGMG_CLUSTER="attached"

  if [ "${TMC_INTEGRATION}" == "TO" ]; then
    if [ "${TDH_SERVICE_TANZU_OBSERVABILITY}" == "true" ]; then
      messageTitle "Tanzu Observability Integration into TMC"

      NAMESPACE=wavefront
      kubectl get ns $NAMESPACE > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        # --- VERIFY SERVICE CONFIGURATION ---
        checkKubernetesServices tanzu_observability

        # --- INSTALL WAVEFRONT HELM CHART ---
        helmRepoAdd wavefront https://wavefronthq.github.io/helm/

        deleteHelmChart $NAMESPACE wavefront
        createNamespace $NAMESPACE > /dev/null 2>&1
        patchServiceAccount default $NAMESPACE
        patchServiceAccount wavefront-collector $NAMESPACE

        # --- GRACE TIME ---
        ret=1; cnt=0
        while [ $ret -ne 0 -a $cnt -lt 50 ]; do
          helm install wavefront wavefront/wavefront \
              --set wavefront.url=$TDH_TANZU_OBSERVABILITY_URL \
              --set wavefront.token=$TDH_TANZU_OBSERVABILITY_API_TOKEN \
              --set clusterName=$TDH_CLUSTER_NAME --namespace wavefront --wait --wait-for-jobs --timeout 10m > /dev/null 2>&1; ret=$?
          [ $ret -eq 0 ] && break
          let cnt=cnt+1
          sleep 60
        done

        if [ $ret -ne 0 ]; then
          echo "ERROR: failed to install wavefront Helm Chart"
          if [ "$NATIVE" == "0" ]; then
            echo "       => tools/${TDH_TOOLS}.sh"
            echo "          tdh-tools:/$ helm install wavefront wavefront/wavefront \\"
            echo "           --set wavefront.url=$TDH_TANZU_OBSERVABILITY_URL \\"
            echo "           --set wavefront.token=$TDH_TANZU_OBSERVABILITY_API_TOKEN \\"
            echo "           --set clusterName=$TDH_CLUSTER_NAME --namespace wavefront"
          else
            echo "       => helm install wavefront wavefront/wavefront \\"
            echo "           --set wavefront.url=$TDH_TANZU_OBSERVABILITY_URL \\"
            echo "           --set wavefront.token=$TDH_TANZU_OBSERVABILITY_API_TOKEN \\"
            echo "           --set clusterName=$TDH_CLUSTER_NAME --namespace wavefront"
          fi
          exit 1
        else
          messagePrint " ▪ Install Wavefront Helm Chart"  "installed"      
        fi
      else
        messagePrint " ▪ Verify Wavefront Helm Chart"  "installed"      
      fi

      cnt=$(tmc cluster integration list --cluster-name $TDH_CLUSTER_NAME -m $MGMG_CLUSTER  \
            -p $TDH_PROVISIONER_NAME --name tanzu-observability-saas 2>/dev/null | egrep -c "tanzu-observability-saas") 
      if [ $cnt -eq 0 ]; then 
        TANZU_OBSERVABILITY=/tmp/tanzu_observability.yaml; rm -f $TANZU_OBSERVABILITY
        echo ""                                                                      >  $TANZU_OBSERVABILITY
        echo "{"                                                                     >> $TANZU_OBSERVABILITY
        echo "    \"full_name\": {"                                                  >> $TANZU_OBSERVABILITY
        echo "        \"provisionerName\": \"$TDH_PROVISIONER_NAME\","               >> $TANZU_OBSERVABILITY
        echo "        \"managementClusterName\": \"$MGMG_CLUSTER\","                 >> $TANZU_OBSERVABILITY
        echo "        \"clusterName\": \"$TDH_CLUSTER_NAME\","                       >> $TANZU_OBSERVABILITY
        echo "        \"name\": \"tanzu-observability-saas\""                        >> $TANZU_OBSERVABILITY
        echo "    },"                                                                >> $TANZU_OBSERVABILITY
        echo "    \"spec\": {"                                                       >> $TANZU_OBSERVABILITY
        echo "        \"configurations\": {"                                         >> $TANZU_OBSERVABILITY
        echo "            \"url\": \"$TDH_TANZU_OBSERVABILITY_URL\""                 >> $TANZU_OBSERVABILITY
        echo "        },"                                                            >> $TANZU_OBSERVABILITY
        echo "        \"secrets\": {"                                                >> $TANZU_OBSERVABILITY
        echo "            \"token\": \"$TDH_TANZU_OBSERVABILITY_API_TOKEN\""         >> $TANZU_OBSERVABILITY
        echo "        }"                                                             >> $TANZU_OBSERVABILITY
        echo "    }"                                                                 >> $TANZU_OBSERVABILITY
        echo "}"                                                                     >> $TANZU_OBSERVABILITY

        tmc cluster integration create -f $TANZU_OBSERVABILITY > /dev/null 2>&1; ret=$?
        if [ $ret -ne 0 ]; then 
          messageLine
          echo "ERROR: failed to add integration"
          if [ "$NATIVE" == "0" ]; then 
            echo "       => tools/${TDH_TOOLS}.sh"
            echo "          tdh-tools:/$ tmc cluster integration create -f $TANZU_OBSERVABILITY"
            echo "          tdh-tools:/$ exit"
          else
            echo "       => tmc cluster integration create -f $TANZU_OBSERVABILITY"
          fi
          messageLine
        else
          messagePrint " ▪ Setup Cluster Integration for ($TDH_CLUSTER_NAME)"  "configured"      
          rm -f $TANZU_OBSERVABILITY
        fi
      else
        messagePrint " ▪ Verify Cluster Integration for ($TDH_CLUSTER_NAME)"  "configured"      
      fi

      uodateConfigMap tanzu-demo-hub TDH_SERVICE_TANZU_OBSERVABILITY        "true"
    else
      uodateConfigMap tanzu-demo-hub TDH_SERVICE_TANZU_OBSERVABILITY        "false"
    fi
  fi

  if [ "${TMC_INTEGRATION}" == "TDP" ]; then
    if [ "${TDH_SERVICE_TANZU_DATA_PROTECTION}" == "true" ]; then
      messageTitle "Data Protection Integration into TMC"
      tmc cluster dataprotection get --cluster-name $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER  > /dev/null 2>&1; ret=$?
      if [ $ret -ne 0 ]; then 
        messagePrint " ▪ Enable Data Protection for cluster"  "$TDH_CLUSTER_NAME"      
        tmc cluster dataprotection create --cluster-name $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER  > /tmp/error.log 2>&1; ret=$?
        if [ $ret -ne 0 ]; then
          logMessages /tmp/error.log
          echo "ERROR: failed to enable Data Protection for cluster $TDH_CLUSTER_NAME"
          if [ "$NATIVE" == "0" ]; then
            echo "       => tools/${TDH_TOOLS}.sh"
            echo "          tdh-tools:/$ tmc cluster dataprotection create --cluster-name $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER"
            echo "          tdh-tools:/$ exit"
          else
            echo "       => tmc cluster dataprotection create --cluster-name $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER"
          fi
  
          exit 1
        fi
      fi

      cnt=$(tmc cluster dataprotection list --cluster-name $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER 2>/dev/null | grep -c " $TDH_CLUSTER_NAME ")
      if [ $cnt -eq 0 ]; then
        # --- VERIFY SERVICE CONFIGURATION ---
        checkKubernetesServices tanzu_data_protection

        # --- VERIFY BACKUP LOCATION ---
        cnt=$(tmc cluster dataprotection backuplocation list --cluster-name $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER | grep -c " $TDH_CLUSTER_NAME ") 
        if [ $cnt -eq 0 ]; then
          ret=1; cnt=0
          while [ $ret -ne 0 -a $cnt -lt 50 ]; do
          tmc cluster dataprotection create -c $TDH_CLUSTER_NAME -m $MGMG_CLUSTER  -p $TDH_PROVISIONER_NAME \
              --backup-location-names $TDH_TANZU_DATA_PROTECTION_BACKUP_LOCATION > /dev/null 2>&1; ret=$?
            [ $ret -eq 0 ] && break
            let cnt=cnt+1
            sleep 30
          done

          if [ $? -ne 0 ]; then
            messageLine
            echo "ERROR: failed to enable dataprotection for cluster $TDH_CLUSTER_NAME"
            if [ "$NATIVE" == "0" ]; then
              echo "       => tools/${TDH_TOOLS}.sh"
              echo "          tdh-tools:/$ tmc cluster dataprotection create -c $TDH_CLUSTER_NAME -m $MGMG_CLUSTER  \\"
              echo "                          -p $TDH_PROVISIONER_NAME --backup-location-names $TDH_TANZU_DATA_PROTECTION_BACKUP_LOCATION"
              echo "          tdh-tools:/$ exit"
            else
              echo "       => tmc cluster dataprotection create -c $TDH_CLUSTER_NAME -m $MGMG_CLUSTER  \\"
              echo "            -p $TDH_PROVISIONER_NAME --backup-location-names $TDH_TANZU_DATA_PROTECTION_BACKUP_LOCATION"
            fi
            messageLine
          fi
        fi

        messagePrint " ▪ Enable DataProtection for Cluster ($TDH_CLUSTER_NAME)"    "enabled"      
        uodateConfigMap tanzu-demo-hub TDH_TANZU_DATA_PROTECTION_ARN               "$TDH_TANZU_DATA_PROTECTION_ARN"
        uodateConfigMap tanzu-demo-hub TDH_TANZU_DATA_PROTECTION_BACKUP_LOCATION   "$TDH_TANZU_DATA_PROTECTION_BACKUP_LOCATION"
        uodateConfigMap tanzu-demo-hub TDH_SERVICE_TANZU_DATA_PROTECTION           "true"

      else
        messagePrint " ▪ Verify DataProtection for Cluster ($TDH_CLUSTER_NAME)"    "enabled"
        messagePrint " ▪ Backuo Lockation"                                         "$TDH_TANZU_DATA_PROTECTION_BACKUP_LOCATION" 
        messagePrint " ▪ AWS CloudFormation ARN"                                   "$TDH_TANZU_DATA_PROTECTION_ARN" 
      fi

      # --- CONFIGURE FULL BACKUP ---
      cnt=$(tmc cluster dataprotection backup list --cluster-name $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER --name "full-backup" 2>/dev/null | egrep -c " full-backup ")
      if [ $cnt -gt 0 ]; then 
        stt=$(tmc cluster dataprotection backup list --cluster-name $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER --name "full-backup" | egrep " full-backup " | awk '{ print $NF }')
        if [ "$stt" != "COMPLETED" -a "$stt" != "INPROGRESS" ]; then 
          # --- DELETE OLD BACKUP ---
          tmc cluster dataprotection backup delete --cluster-name $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER "full-backup"  > /dev/null 2>&1
          #tmc cluster dataprotection delete --cluster-name tdh-azure-sadubois -p attached -m attached --delete-backups 
          cnt=0
        fi
      fi

      sleep 300

      if [ $cnt -eq 0 ]; then 
        messagePrint " ▪ Configure Full Backup (full-backup) configuration"            "enabled"

        ret=1; cnt=0
        while [ $ret -ne 0 -a $cnt -lt 5 ]; do
          tmc cluster dataprotection backup create -c $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER  \
            --name "full-backup" --backup-location-name $TDH_TANZU_DATA_PROTECTION_BACKUP_LOCATION > /dev/null 2>&1; ret=$?
          [ $ret -eq 0 ] && break
          let cnt=cnt+1
          sleep 30
        done

        if [ $ret -ne 0 ]; then
          messageLine
          echo "ERROR: failed to configure a full-backup"
          if [ "$NATIVE" == "0" ]; then
            echo "       => tools/${TDH_TOOLS}.sh"
            echo "          tdh-tools:/$ tmc cluster dataprotection backup create -c $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME \\"
            echo "                          -m $MGMG_CLUSTER  --name \"full-backup\" --backup-location-name $TDH_TANZU_DATA_PROTECTION_BACKUP_LOCATION"
            echo "          tdh-tools:/$ exit"
          else
            echo "       => tmc cluster dataprotection backup create -c $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME \\"
            echo "            -m $MGMG_CLUSTER  --name \"full-backup\" --backup-location-name $TDH_TANZU_DATA_PROTECTION_BACKUP_LOCATION"
          fi
          messageLine
        fi
      else
        messagePrint " ▪ Verify Full Backup (full-backup) configuration"            "enabled"
      fi

      uodateConfigMap tanzu-demo-hub TDH_SERVICE_TANZU_DATA_PROTECTION             "false"
    fi
  fi

  if [ "${TMC_INTEGRATION}" == "TCI" ]; then
    messageTitle "Cluster Inspections"

    scanCluster   LITE
    scanCluster   CIS
    scanCluster   CONFORMANCE
  fi
}

scanCluster() {
  TYPE=$1

  [ "$TYPE" == "LITE" ] && NAME=lite && TITLE="Lite Inspections"
  [ "$TYPE" == "CIS" ]  && NAME=cis && TITLE="CIS Inspections"
  [ "$TYPE" == "CONFORMANCE" ] && NAME=conformance && TITLE="Conformance Inspections"

  completed=$(tmc cluster inspection scan list --cluster-name $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER -o json | \
             jq -r '.scans[].status.report.info.scanType' | egrep -c "^$NAME")

  if [ $completed -eq 0 ]; then 
    # --- VERIFY ACRIVE SCANNS ---
    cnt=$(tmc cluster inspection scan list --cluster-name $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER  -o json | \
       jq -r '.scans[].status.phase' | grep -v "COMPLETE" | wc -l | sed 's/  *//g')
    while [ $cnt -ne 0 ]; do
      cnt=$(tmc cluster inspection scan list --cluster-name $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER  -o json | \
        jq -r '.scans[].status.phase' | grep -v "COMPLETE" | wc -l | sed 's/  *//g')
      sleep 10
    done

    stt=$(tmc cluster inspection scan list --cluster-name $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER  -o json | \
      jq -r --arg key $NAME '.scans[].status.report.info | select(.scanType == $key).result' | tail -1)
    if [ "$stt" == "" ]; then 
      messagePrint " ▪ Scan Cluster ($TDH_CLUSTER_NAME) with $TYPE"       "scanning"
      tmc cluster inspection scan create -c $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER  --inspection-type $TYPE > /dev/null 2>&1
      if [ $? -ne 0 ]; then 
        messageLine
        echo "ERROR: failed to scan cluster $TDH_CLUSTER_NAME"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ tmc cluster inspection scan create -c $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME \\"
          echo "                         -m $MGMG_CLUSTER  --inspection-type $TYPE"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => tmc cluster inspection scan create -c $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME \\"
          echo "             -m $MGMG_CLUSTER  --inspection-type $TYPE"
        fi
        messageLine
      fi
    else
      dat=$(tmc cluster inspection scan list --cluster-name $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER  -o json | \
        jq -r --arg key $NAME '.scans[].status.report.info | select(.scanType == $key).runDatetime' | sort -n | tail -1)
      messagePrint " ▪ Cluster Scan ($TDH_CLUSTER_NAME) with $TYPE completed"       "$dat"
    fi
  else
    stt=$(tmc cluster inspection scan list --cluster-name $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER  -o json | \
        jq -r --arg key $NAME '.scans[].status.report.info | select(.scanType == $key).result' | tail -1)
    scd=$(tmc cluster inspection scan list --cluster-name $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $MGMG_CLUSTER  -o json | \
        jq -r --arg key $NAME '.scans[].status.report.info | select(.scanType == $key).runDatetime' | tail -1)
    [ "$stt" == "null" ] && stt="scanning"

    messagePrint " ▪ Cluster (tdh-minikube-sadubois) Scanned with $NAME"   "$scd $stt"      
  fi
}

checkTMCAccess_old() {
  missing_variables=0

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "AWS" ]; then
    if [ "${TDH_MANAGEMENT_CLUSTER}" == "" -o "${TDH_PROVISIONER_NAME}" == "" -o "${AWS_PRIMARY_AZ}" == "" ]; then
      missing_variables=1
      echo ""
      echo "  2MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
      echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
  
      if [ "${TDH_MANAGEMENT_CLUSTER}" == "" ]; then
        echo "  TDH_MANAGEMENT_CLUSTER          (required) A Management Cluster deployed on AWS"
      fi
  
      if [ "${TDH_PROVISIONER_NAME}" == "" ]; then
        echo "  TDH_PROVISIONER_NAME            (required) The TMC Provisionert Name for AWS"
      fi

      if [ "${AWS_HOSTED_DNS_DOMAIN}" == "" ]; then
        echo "  AWS_HOSTED_DNS_DOMAIN           (required) A DNS Domain managed by AWS Route 53 ie. yourdomain.com with a"
        echo "                                  valid DNS Zone. => https://console.aws.amazon.com/route53"
      fi

      if [ "${AWS_PRIMARY_AZ}" == "" ]; then
        echo "  AWS_PRIMARY_AZ                  (required) Choose the Primary Availability Zone of a AWS Region"
        echo "                                    => aws ec2 --region=eu-west-3 describe-availability-zones --output text"
        echo "                                       eu-north-1a, eu-north-1b, eu-north-1c"
      fi
    else
      [ "$TDH_TKGMC_ENVNAME" == "" ] && TDH_TKGMC_ENVNAME=$TDH_ENVNAME
      messageTitle "Supporting services access (Pivotal Network, AWS Route53)"
      messagePrint " ▪ Pivotal Network Token"            "$PCF_PIVNET_TOKEN"
      messagePrint " ▪ AWS Route53 Hosted DNS Domain"    "$TDH_TKGMC_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"

      # --- CHECK IF AWS CLI IS CONFIGURED ---
      [ ! -d $HOME/.aws ] && mkdir -p $HOME/.aws
      if [ ! -f $HOME/.aws/credentials ]; then
        if [ "$AWS_ACCESS_KEY" != "" -a "$AWS_SECRET_KEY" != "" ]; then
          echo "[default]"                               >  $HOME/.aws/credentials
          echo "aws_access_key_id = $AWS_ACCESS_KEY"     >> $HOME/.aws/credentials
          echo "aws_secret_access_key = $AWS_SECRET_KEY" >> $HOME/.aws/credentials
        else
          echo "ERROR: AWS CLI is not configured yet, please run aws configure"
          if [ "$NATIVE" == "0" ]; then
            echo "       => tools/${TDH_TOOLS}.sh"
            echo "          tdh-tools:/$ aws configure"
            echo "          tdh-tools:/$ exit"
          else
            echo "       => aws configure"
          fi
        fi
        exit 1
      fi
    fi
  fi

  if [ ${missing_variables} -eq 1 ]; then
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    echo "  IMPORTANT: Please set the missing environment variables either in your shell or in the tanzu-demo-hub"
    echo "             configuration file ~/.tanzu-demo-hub.cfg and set all variables with the 'export' notation"
    echo "             ie. => export TDH_MANAGEMENT_CLUSTER=xxxxxxxx"
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    exit 1
  fi
}

checkTDHAccess() {
  missing_variables=0

  if [ "${PCF_PIVNET_TOKEN}" == "" -o "${AWS_HOSTED_DNS_DOMAIN}" == "" -o "${AWS_ACCESS_KEY}" == "" -o "${AWS_SECRET_KEY}" == "" \
    -o "${TDH_USER}" == "" -o "${AWS_REGION}" == "" ]; then

    missing_variables=1
    echo ""
    echo "  9MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"

    if [ "${PCF_PIVNET_TOKEN}" == "" ]; then
      echo "  PCF_PIVNET_TOKEN                (required) PIVNET Access Token to download software. Get a UAA API TOKEN from"
      echo "                                  => https://network.pivotal.io/users/dashboard/edit-profile"
    fi

    if [ "${AWS_ACCESS_KEY}" == "" ]; then
      echo "  AWS_ACCESS_KEY                  (required) A DNS Domain managed by AWS Route 53 ie. yourdomain.com with a"
      echo "                                  valid DNS Zone. => https://console.aws.amazon.com/route53"
    fi

    if [ "${AWS_SECRET_KEY}" == "" ]; then
      echo "  AWS_SECRET_KEY                  (required) A DNS Domain managed by AWS Route 53 ie. yourdomain.com with a"
      echo "                                  valid DNS Zone. => https://console.aws.amazon.com/route53"
    fi

    if [ "${AWS_REGION}" == "" ]; then
      echo "  AWS_REGION                      (required) Choose the AWS Region where your installation should run"
      echo "                                    => aws ec2 --region=eu-west-3 describe-regions --output text | awk '{ print $NF }'"
      echo "                                       eu-north-1, eu-west-3, eu-central-1 etc."
    fi

    if [ "${AWS_HOSTED_DNS_DOMAIN}" == "" ]; then
      echo "  AWS_HOSTED_DNS_DOMAIN           (required) A DNS Domain managed by AWS Route 53 ie. yourdomain.com with a"
      echo "                                  valid DNS Zone. => https://console.aws.amazon.com/route53"
    fi

    if [ "${TDH_USER}" == "" ]; then
      echo "  TDH_USER                        (required) Tanzu Demo Hub User to tag TKG Clusters with your Name"
      echo "                                   => (usually you VMware User Name without @vmware.com)"
    fi
  else
    [ "$TDH_TKGMC_ENVNAME" == "" ] && TDH_TKGMC_ENVNAME=$TDH_ENVNAME
    messageTitle "Supporting services access (Pivotal Network, AWS Route53)"
    messagePrint " ▪ Pivotal Network Token"            "$PCF_PIVNET_TOKEN"
    messagePrint " ▪ Tanzu Demo Hube User"             "$TDH_USER"
    messagePrint " ▪ AWS Route53 Hosted DNS Domain"    "$TDH_TKGMC_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"

    # --- VERIFY PIVNET ACCESS ---
    pivnetAPI $PCF_PIVNET_TOKEN
    if [ $? -ne 0 ]; then
      write_line
      echo "ERROR: Pivnet Token: $PCF_PIVNET_TOKEN does not seam to be valid. Please greate a new one by"
      echo "       login to http://network.pivotal.io => Your login Name => Edit Profile => UAA API TOKEN"
      exit 1
    fi

    # --- CHECK IF AWS CLI IS CONFIGURED ---
    [ ! -d $HOME/.aws ] && mkdir -p $HOME/.aws
    if [ ! -f $HOME/.aws/credentials ]; then
      if [ "$AWS_ACCESS_KEY" != "" -a "$AWS_SECRET_KEY" != "" ]; then 
        echo "[default]"                               >  $HOME/.aws/credentials
        echo "aws_access_key_id = $AWS_ACCESS_KEY"     >> $HOME/.aws/credentials
        echo "aws_secret_access_key = $AWS_SECRET_KEY" >> $HOME/.aws/credentials
      else
        echo "ERROR: AWS CLI is not configured yet, please run aws configure"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ aws configure"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => aws configure"
        fi

        exit 1
      fi
    fi

    if [ -d $HOME/.aws -a -f $HOME/.aws/credentials ]; then 
      aky=$(egrep -c "$AWS_ACCESS_KEY" $HOME/.aws/credentials) 
      sky=$(egrep -c "$AWS_SECRET_KEY" $HOME/.aws/credentials) 
      if [ $aky -eq 0 -o $sky -eq 0 ]; then 
        echo "ERROR: AWS_ACCESS_KEY and AWS_SECRET_KEY in ~/.tanzu-demo-hub.cfg do not match"
        echo "       with the aws credentials defined in $HOME/.aws/credentials. Please configure manually"
        if [ "$NATIVE" == "0" ]; then 
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ aws configure"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => aws configure"
        fi
        exit 1
      fi
    else
      echo "ERROR: AWS Credentials not configured on local system, Please configure manually"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ aws configure"
        echo "          tdh-tools:/$ exit 1"
      else
        echo "       => aws configure"
      fi
      exit 1
    fi

    export AWS_HOSTED_ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?starts_with(to_string(Name), '${AWS_HOSTED_DNS_DOMAIN}.')]" | \
                        jq -r '.[].Id' | awk -F '/' '{ print $NF }')
    if [ $? -ne 0 -o "${AWS_HOSTED_ZONE_ID}" == "" ]; then
      echo "ERROR: failed to get domain information for ($AWS_HOSTED_DNS_DOMAIN)"
      echo "       => aws route53 list-hosted-zones --query \"HostedZones[?starts_with(to_string(Name), '${AWS_HOSTED_DNS_DOMAIN}.')]\""
      exit 1
    else
      messagePrint " ▪ AWS Route53 ZoneID"              "$AWS_HOSTED_ZONE_ID"
    fi
  fi

  if [ ${missing_variables} -eq 1 ]; then
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    echo "  IMPORTANT: Please set the missing environment variables either in your shell or in the tanzu-demo-hub"
    echo "             configuration file ~/.tanzu-demo-hub.cfg and set all variables with the 'export' notation"
    echo "             ie. => export <environment_variable>=\"<value>\""
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    exit 1
  fi

} 

pivnetAPI() {
  REFRESH_TOKEN="$1"
  token=$(curl -X POST https://network.pivotal.io/api/v2/authentication/access_tokens \
               -d "{\"refresh_token\":\"$REFRESH_TOKEN\"}" 2>/dev/null | jq -r '.access_token')
  if [ "${token}" == "" -o "${token}" == "null" ]; then return 1; fi

  # --- LOGIN INTO PIVNET ---
  curl -X GET https://network.pivotal.io/api/v2/authentication -H "Authorization: Bearer $token"

  # --- PIVNET ACCESS TEST ---
  if [ "${2}" == "" ]; then
    return 0
  fi

  curl -H "Authorization: Bearer $token" -X $2 https://network.pivotal.io/api/v2/$3

  return 0
}

write_line() {
  messageTitle "-------------------------------------------------------------------------------------------------------------------------------------------------------"
}

createENVfile() {
  dep="$1"
  envFile="$2"

  rm -f $env
  . ${TANZU_DEMO_HUB}/deployments/$dep

  cat ${TANZU_DEMO_HUB}/deployments/$dep | sed -e '/^$/d' -e '/^#/d' -e 's/#.*$//g' > $envFile

  echo "PCF_DEPLOYMENT_DEBUG=$PCF_DEPLOYMENT_DEBUG"                              >> $envFile
  echo "AWS_HOSTED_ZONE_ID=$AWS_HOSTED_ZONE_ID"                                  >> $envFile
  echo "AWS_HOSTED_DNS_DOMAIN=$AWS_HOSTED_DNS_DOMAIN"                            >> $envFile
  echo "PCF_PIVNET_TOKEN=$PCF_PIVNET_TOKEN"                                      >> $envFile

  echo "PCF_TILE_PKS_ADMIN_USER=$PCF_TILE_PKS_ADMIN_USER"                        >> $envFile
  echo "PCF_TILE_PKS_ADMIN_PASS=$PCF_TILE_PKS_ADMIN_PASS"                        >> $envFile
  echo "PCF_TILE_PKS_ADMIN_EMAIL=$PCF_TILE_PKS_ADMIN_EMAIL"                      >> $envFile
  echo "PCF_TILE_PAS_ADMIN_USER=$PCF_TILE_PAS_ADMIN_USER"                        >> $envFile
  echo "PCF_TILE_PAS_ADMIN_PASS=$PCF_TILE_PAS_ADMIN_PASS"                        >> $envFile
  echo "PCF_TILE_PAS_ADMIN_EMAIL=$PCF_TILE_PAS_ADMIN_EMAIL"                      >> $envFile

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "GCP" ]; then
    #echo "GCP_SERVICE_ACCOUNT=/tmp/$(basename $GCP_SERVICE_ACCOUNT)"             >> $envFile
    echo "GCP_REGION=\"$GCP_REGION\""                                            >> $envFile
    echo "GCP_PROJECT=\"$GCP_PROJECT\""                                          >> $envFile

    echo "PCF_TERRAFORMS_TEMPLATE_BUNDLE=\"$PCF_TERRAFORMS_TEMPLATE_BUNDLE\""    >> $envFile
    echo "PCF_TERRAFORMS_TEMPLATE_NAME=\"$PCF_TERRAFORMS_TEMPLATE_NAME\""        >> $envFile
    echo "PCF_TERRAFORMS_TEMPLATE_VERSION=\"$PCF_TERRAFORMS_TEMPLATE_VERSION\""  >> $envFile

    if [ "${PCF_TILE_PKS_DEPLOY}" == "true" ]; then
      if [ "${GCP_PKS_TLS_CERTIFICATE}" != "" -a "${GCP_PKS_TLS_FULLCHAIN}" != "" -a \
           "${GCP_PKS_TLS_PRIVATE_KEY}" != "" ]; then

        echo "TLS_CERTIFICATE=\"$GCP_PKS_TLS_CERTIFICATE\""              >> $envFile
        echo "TLS_FULLCHAIN=\"$GCP_PKS_TLS_FULLCHAIN\""                  >> $envFile
        echo "TLS_PRIVATE_KEY=\"$GCP_PKS_TLS_PRIVATE_KEY\""              >> $envFile
      fi
    fi

    if [ "${PCF_TILE_PAS_DEPLOY}" == "true" ]; then
      if [ "${GCP_PAS_TLS_CERTIFICATE}" != "" -a "${GCP_PAS_TLS_FULLCHAIN}" != "" -a \
           "${GCP_PAS_TLS_PRIVATE_KEY}" != "" ]; then

        echo "TLS_CERTIFICATE=\"$GCP_PAS_TLS_CERTIFICATE\""              >> $envFile
        echo "TLS_FULLCHAIN=\"$GCP_PAS_TLS_FULLCHAIN\""                  >> $envFile
        echo "TLS_PRIVATE_KEY=\"$GCP_PAS_TLS_PRIVATE_KEY\""              >> $envFile
      fi
    fi

    if [ "${PCF_TILE_HARBOR_DEPLOY}" == "true" ]; then
      echo "PCF_TILE_HARBOR_ADMIN_PASS=\"$PCF_TILE_HARBOR_ADMIN_PASS\"" >> $envFile
    fi
    if [ "${PCF_TILE_PBS_DEPLOY}" == "true" ]; then
      echo "PCF_TILE_PBS_DEPLOY=\"$PCF_TILE_PBS_DEPLOY\""               >> $envFile
      echo "PCF_TILE_PBS_NAME=\"$PCF_TILE_PBS_NAME\""                   >> $envFile
      echo "PCF_TILE_PBS_SLUG=\"$PCF_TILE_PBS_SLUG\""                   >> $envFile
      echo "PCF_TILE_PBS_VERSION=\"$PCF_TILE_PBS_VERSION\""             >> $envFile
      echo "PCF_TILE_PBS_ADMIN_USER=\"$PCF_TILE_PBS_ADMIN_USER\""       >> $envFile
      echo "PCF_TILE_PBS_ADMIN_PASS=\"$PCF_TILE_PBS_ADMIN_PASS\""       >> $envFile
      echo "PCF_TILE_PBS_ADMIN_EMAIL=\"$PCF_TILE_PBS_ADMIN_EMAIL\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_REPO=\"$PCF_TILE_PBS_DOCKER_REPO\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_USER=\"$PCF_TILE_PBS_DOCKER_USER\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_PASS=\"$PCF_TILE_PBS_DOCKER_PASS\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_REPO=\"$PCF_TILE_PBS_GITHUB_REPO\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_USER=\"$PCF_TILE_PBS_GITHUB_USER\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_PASS=\"$PCF_TILE_PBS_GITHUB_PASS\""     >> $envFile
    fi
  fi

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "AWS" ]; then
    echo "AWS_ACCESS_KEY=\"$AWS_ACCESS_KEY\""                                    >> $envFile
    echo "AWS_SECRET_KEY=\"$AWS_SECRET_KEY\""                                    >> $envFile
    echo "AWS_REGION=\"$AWS_REGION\""                                            >> $envFile

    echo "PCF_TERRAFORMS_TEMPLATE_BUNDLE=\"$PCF_TERRAFORMS_TEMPLATE_BUNDLE\""    >> $envFile
    echo "PCF_TERRAFORMS_TEMPLATE_NAME=\"$PCF_TERRAFORMS_TEMPLATE_NAME\""        >> $envFile
    echo "PCF_TERRAFORMS_TEMPLATE_VERSION=\"$PCF_TERRAFORMS_TEMPLATE_VERSION\""  >> $envFile

    if [ "${PCF_TILE_PKS_DEPLOY}" == "true" ]; then
      if [ "${AWS_PKS_TLS_CERTIFICATE}" != "" -a "${AWS_PKS_TLS_FULLCHAIN}" != "" -a \
           "${AWS_PKS_TLS_PRIVATE_KEY}" != "" ]; then

        echo "TLS_CERTIFICATE=\"$AWS_PKS_TLS_CERTIFICATE\""              >> $envFile
        echo "TLS_FULLCHAIN=\"$AWS_PKS_TLS_FULLCHAIN\""                  >> $envFile
        echo "TLS_PRIVATE_KEY=\"$AWS_PKS_TLS_PRIVATE_KEY\""              >> $envFile
      fi
    fi

    if [ "${PCF_TILE_PAS_DEPLOY}" == "true" ]; then
      if [ "${AWS_PAS_TLS_CERTIFICATE}" != "" -a "${AWS_PAS_TLS_FULLCHAIN}" -o \
           "${AWS_PAS_TLS_PRIVATE_KEY}" != "" ]; then

        echo "TLS_CERTIFICATE=\"$AWS_PAS_TLS_CERTIFICATE\""              >> $envFile
        echo "TLS_FULLCHAIN=\"$AWS_PAS_TLS_FULLCHAIN\""                  >> $envFile
        echo "TLS_PRIVATE_KEY=\"$AWS_PAS_TLS_PRIVATE_KEY\""              >> $envFile
      fi
    fi

    if [ "${PCF_TILE_HARBOR_DEPLOY}" == "true" ]; then
      echo "PCF_TILE_HARBOR_ADMIN_PASS=\"$PCF_TILE_HARBOR_ADMIN_PASS\"" >> $envFile
    fi

    if [ "${PCF_TILE_PBS_DEPLOY}" == "true" ]; then
      echo "PCF_TILE_PBS_DEPLOY=\"$PCF_TILE_PBS_DEPLOY\""               >> $envFile
      echo "PCF_TILE_PBS_NAME=\"$PCF_TILE_PBS_NAME\""                   >> $envFile
      echo "PCF_TILE_PBS_SLUG=\"$PCF_TILE_PBS_SLUG\""                   >> $envFile
      echo "PCF_TILE_PBS_VERSION=\"$PCF_TILE_PBS_VERSION\""             >> $envFile
      echo "PCF_TILE_PBS_ADMIN_USER=\"$PCF_TILE_PBS_ADMIN_USER\""       >> $envFile
      echo "PCF_TILE_PBS_ADMIN_PASS=\"$PCF_TILE_PBS_ADMIN_PASS\""       >> $envFile
      echo "PCF_TILE_PBS_ADMIN_EMAIL=\"$PCF_TILE_PBS_ADMIN_EMAIL\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_REPO=\"$PCF_TILE_PBS_DOCKER_REPO\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_USER=\"$PCF_TILE_PBS_DOCKER_USER\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_PASS=\"$PCF_TILE_PBS_DOCKER_PASS\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_REPO=\"$PCF_TILE_PBS_GITHUB_REPO\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_USER=\"$PCF_TILE_PBS_GITHUB_USER\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_PASS=\"$PCF_TILE_PBS_GITHUB_PASS\""     >> $envFile
    fi
  fi

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "Azure" ]; then
    appid=$(az ad app list --display-name "TanzuDemoHub-$TDH_USER" | jq -r '.[].appId')

    echo "AZURE_SUBSCRIPTION_ID=\"$AZURE_SUBSCRIPTION_ID\""                      >> $envFile
    echo "AZURE_TENANT_ID=\"$AZURE_TENANT_ID\""                                  >> $envFile
    echo "AZURE_CLIENT_ID=\"$AZURE_CLIENT_ID\""                                  >> $envFile
    echo "AZURE_CLIENT_SECRET=\"$AZURE_CLIENT_SECRET\""                          >> $envFile
    echo "AZURE_LOCATION=\"$AZURE_LOCATION\""                                    >> $envFile

    echo "PCF_TERRAFORMS_TEMPLATE_BUNDLE=\"$PCF_TERRAFORMS_TEMPLATE_BUNDLE\""    >> $envFile
    echo "PCF_TERRAFORMS_TEMPLATE_NAME=\"$PCF_TERRAFORMS_TEMPLATE_NAME\""        >> $envFile
    echo "PCF_TERRAFORMS_TEMPLATE_VERSION=\"$PCF_TERRAFORMS_TEMPLATE_VERSION\""  >> $envFile

    if [ "${PCF_TILE_PKS_DEPLOY}" == "true" ]; then
      if [ "${AZURE_PKS_TLS_CERTIFICATE}" != "" -a "${AZURE_PKS_TLS_FULLCHAIN}" -o \
           "${AZURE_PKS_TLS_PRIVATE_KEY}" != "" ]; then

        echo "TLS_CERTIFICATE=\"$AZURE_PKS_TLS_CERTIFICATE\""           >> $envFile
        echo "TLS_FULLCHAIN=\"$AZURE_PKS_TLS_FULLCHAIN\""               >> $envFile
        echo "TLS_PRIVATE_KEY=\"$AZURE_PKS_TLS_PRIVATE_KEY\""           >> $envFile
      fi
    fi

    if [ "${PCF_TILE_PAS_DEPLOY}" == "true" ]; then
      if [ "${AZURE_PAS_TLS_CERTIFICATE}" != "" -a "${AZURE_PAS_TLS_FULLCHAIN}" -o \
           "${AZURE_PAS_TLS_PRIVATE_KEY}" != "" ]; then

        echo "TLS_CERTIFICATE=\"$AZURE_PAS_TLS_CERTIFICATE\""           >> $envFile
        echo "TLS_FULLCHAIN=\"$AZURE_PAS_TLS_FULLCHAIN\""               >> $envFile
        echo "TLS_PRIVATE_KEY=\"$AZURE_PAS_TLS_PRIVATE_KEY\""           >> $envFile
      fi
    fi

    if [ "${PCF_TILE_HARBOR_DEPLOY}" == "true" ]; then
      echo "PCF_TILE_HARBOR_ADMIN_PASS=\"$PCF_TILE_HARBOR_ADMIN_PASS\"" >> $envFile
    fi

    if [ "${PCF_TILE_PBS_DEPLOY}" == "true" ]; then
      echo "PCF_TILE_PBS_DEPLOY=\"$PCF_TILE_PBS_DEPLOY\""               >> $envFile
      echo "PCF_TILE_PBS_NAME=\"$PCF_TILE_PBS_NAME\""                   >> $envFile
      echo "PCF_TILE_PBS_SLUG=\"$PCF_TILE_PBS_SLUG\""                   >> $envFile
      echo "PCF_TILE_PBS_VERSION=\"$PCF_TILE_PBS_VERSION\""             >> $envFile
      echo "PCF_TILE_PBS_ADMIN_USER=\"$PCF_TILE_PBS_ADMIN_USER\""       >> $envFile
      echo "PCF_TILE_PBS_ADMIN_PASS=\"$PCF_TILE_PBS_ADMIN_PASS\""       >> $envFile
      echo "PCF_TILE_PBS_ADMIN_EMAIL=\"$PCF_TILE_PBS_ADMIN_EMAIL\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_REPO=\"$PCF_TILE_PBS_DOCKER_REPO\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_USER=\"$PCF_TILE_PBS_DOCKER_USER\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_PASS=\"$PCF_TILE_PBS_DOCKER_PASS\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_REPO=\"$PCF_TILE_PBS_GITHUB_REPO\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_USER=\"$PCF_TILE_PBS_GITHUB_USER\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_PASS=\"$PCF_TILE_PBS_GITHUB_PASS\""     >> $envFile
    fi
  fi
}

configureLDAP() {
  #if [ "${TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE}" != "ldap" ]; then return; fi
  #VSPHERE_JUMPHOST_NAME

  ldapwhoami -H ldap://${JUMP_HOST} -x -ZZ >/dev/null 2>&1
  if [ $? -ne 0 ]; then 
    messageTitle "Install LDAP Server and (phpLDAPadmin) on Jump Host (${JUMP_HOST})"
    $SSH_COMMAND -n "[ -d $SSH_HOME/tanzu-demo-hub ] && chmod a+x ${TDHHOME}/scripts/InstallOpenLDAP.sh && \
        sudo ${TDHHOME}/scripts/InstallOpenLDAP.sh \"${TDH_TKGMC_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}\" $DEBUG"; ret=$?

    if [ ${ret} -ne 0 ]; then
      echo "ERROR: Failed to install CLI Utilities (InstallOpenLDAP.sh)"
      echo "       => $SSH_COMMAND"
      exit 1
    fi
    sleep 300
  fi
  
  messagePrint "Verify LDAP Server" "$JUMP_HOST"
  ret=$(ldapwhoami -H ldap://${JUMP_HOST} -x -ZZ)
  if [ "$ret" == "anonymous" ]; then 
    messagePrint " ▪ LDAP CLI Access (ldapwhoami)"   "succeeded"
  else
    echo "ERROR: LDAP Access (ldapwhoami) failed with '$ret'"
    echo "       => ldapwhoami -H ldap://${JUMP_HOST} -x -ZZ"
    exit 1
  fi

  messagePrint " ▪ Verify LDAP Management (phpLDAPadmin)"   "http://${JUMP_HOST}/phpldapadmin"
} 

# ------------------------------------------------------------------------------------------
# Function Name ......: buildTDHToolsContainer
# Function Purpose ...: Build the TDH Tools Container
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: tkg / tce
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
buildTDHToolsContainer() {
  TKG_TYPE=$1

  for rel in $(ls -1 $TDHPATH/files/tdh-tools/tdh-tools-tkg-*.cfg | sed -e 's/^.*tools-tkg-//g' -e 's/\.cfg//g'); do
    $SSH_COMMAND -n "[ -f $SSH_HOME/tanzu-demo-hub/scripts/buildTDHToolsContainer.sh ] && tanzu-demo-hub/scripts/buildTDHToolsContainer.sh $TKG_TYPE $rel"; ret=$?
    if [ $ret -ne 0 ]; then
      echo "ERROR: failed to TDH Tools container on the Jumop Host"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ $SSH_COMMAND -n \"[ -f $SSH_HOME/tanzu-demo-hub/scripts/buildTDHToolsContainer.sh ] && tanzu-demo-hub/scripts/buildTDHToolsContainer.sh $TKG_TYPE $rel\""
        echo "          tdh-tools:/$ exit"
      else
        echo "       => $SSH_COMMAND -n \"[ -f $SSH_HOME/tanzu-demo-hub/scripts/buildTDHToolsContainer.sh ] && tanzu-demo-hub/scripts/buildTDHToolsContainer.sh $TKG_TYPE $rel\""
      fi

      exit 1
    fi

  done
}

# ------------------------------------------------------------------------------------------
# Function Name ......: configureJumpHost
# Function Purpose ...: Configure Juno Hosts, Downloading TDH_HOME Directory
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: none
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
configureJumpHost() {
  messageTitle "Configure Jump Server: $SSH_HOST"

  # --- WAIT UNTIL SSH DEAMON IS READY NO JUMPHOST ----
  ret=1
  while [ $ret -ne 0 ]; do
    $SSH_COMMAND -n id > /dev/null 2>&1; ret=$?
    [ $ret -ne 0 ] && sleep 10
  done

  GITTDH="https://github.com/pivotal-sadubois/tanzu-demo-hub.git"
  TDHHOME="${SSH_HOME}/tanzu-demo-hub"

  messagePrint " ▪ Clone TDH GIT Repository" "$GITTDH"

  $SSH_COMMAND -n "[ ! -d /home/ubuntu/tanzu-demo-hub ] && git clone $GITTDH > /dev/null 2>&1"
  $SSH_COMMAND -n "[ -d /home/ubuntu/tanzu-demo-hub ] && cd $SSH_HOME/tanzu-demo-hub; git pull > /dev/null 2>&1"

  uid=$($SSH_COMMAND -n sudo whoami 2>/dev/null) 
  if [ "$uid" != "root" ]; then 
    messagePrint " ▪ Enable SuDO Access for user" "/etc/sudoers.d/$SSH_USER"
    $SSH_COMMAND -n "echo $VSPHERE_JUMPHOST_PASSWORD | sudo -S $SSH_HOME/tanzu-demo-hub/tools/tdh-modify-sudoers.sh $SSH_USER" > /dev/null 2>&1; ret=$?
    if [ $ret -eq 0 ]; then 
      # --- TEST SUDO ROOT ACCESS ---
      uid=$($SSH_COMMAND -n sudo whoami 2>/dev/null) 
      if [ "$uid" != "root" ]; then 
        echo "ERROR: Failed to configure Sudo Access for user $SH_USER on $JUMP_HOST"
        echo "       please try manually (used the VSPHERE_JUMPHOST_PASSWORD password):"
        echo "       => $SSH_DISPLAY"
        echo "       => sudo echo \"$SSH_USER    ALL=(ALL:ALL) ALL\" > /etc/sudoers.d/$SSH_USER"
        echo "       => chmod 440 /etc/sudoers.d/$SSH_USER"
        exit 1
      fi
    else
      echo "ERROR: Failed to configure Sudo Access for user $SH_USER on $JUMP_HOST"
      echo "       please try manually (used the VSPHERE_JUMPHOST_PASSWORD password):"
      echo "       => $SSH_DISPLAY"
      echo "       => sudo echo \"$SSH_USER    ALL=(ALL:ALL) ALL\" > /etc/sudoers.d/$SSH_USER"
      echo "       => chmod 440 /etc/sudoers.d/$SSH_USER"
      exit 1
    fi
  else
    messagePrint " ▪ Verify SuDO Access for user" "/etc/sudoers.d/$SSH_USER"
  fi

  # --- TEST FOR RUNNING REMOTE SCRIPT ---
  stt=$($SSH_COMMAND -n "[ -f /tmp/tanzu-demo-hub.pid ] && pgrep -F /tmp/tanzu-demo-hub.pid")
  stt=$($SSH_COMMAND -n "[ -f /jump_software_installed ] && echo true")

  if [ "${stt}" == "" ]; then
    messageTitle "Install CLI Utilities (aws,az,gcp,bosh,pivnet,cf,om,jq) on Jump Host (jump.${TDH_TKGMC_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN})"
    ret=1; cnt=0
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      $SSH_COMMAND -n "[ -d $SSH_HOME/tanzu-demo-hub ] && chmod a+x ${TDHHOME}/scripts/InstallUtilities.sh && \
          sudo ${TDHHOME}/scripts/InstallUtilities.sh $PCF_PIVNET_TOKEN $DEBUG"; ret=$?
      [ $ret -ne 0 ] && break
      let cnt=cnt+1
      sleep 30
    done

    if [ ${ret} -ne 0 ]; then
      echo "ERROR: Failed to install CLI Utilities (InstallUtilities.sh) on Jump Host"
      echo "       => $SSH_COMMAND"
      exit 1
    else
      messagePrint " ▪ Rebooting Jump Server"   "wait for services comming up ..."
      $SSH_COMMAND -n "sudo reboot" > /dev/null 2>&1
    fi

    # --- WAIT UNTIL SSH DEAMON IS READY NO JUMPHOST ----
    ret=1
    while [ $ret -ne 0 ]; do
      $SSH_COMMAND -n id > /dev/null 2>&1; ret=$?
      [ $ret -ne 0 ] && sleep 10
    done

    sleep 20
  fi

  $SSH_COMMAND "[ -f ~/tdh_home_unpacked ] && exit 0" > /dev/null 2>&1; ret=$?
  if [ "$ret" -ne 0 ]; then 
    messagePrint " ▪ Copy Tanzu Demo Hub Directory" "$HOME/.tanzu-demo-hub"
    # 02.12.2021 Can not copy the .tanzu-demo-hub/config over to jump, its overwriting cluster kubeconfig files on the jump if deployTKGmc is started again
    (cd $HOME && tar cf /tmp/tdh-home.tar .aws .azure .tanzu-demo-hub/terraform .tanzu-demo-hub/certificates .tanzu-demo-hub/config .tanzu-demo-hub.cfg .tanzu-demo-hub/KeyPair* > /dev/null 2>&1)

    ret=1; cnt=0
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      $SCP_COMMAND /tmp/tdh-home.tar ${SSH_USER}@${SSH_HOST}:$SSH_HOME > /dev/null 2>&1; ret=$?
      [ $ret -eq 0 ] && break
      let cnt=cnt+1
      sleep 30
    done

    stt=""; cnt=0
    while [ "${stt}" != "true" -a $cnt -lt 5 ]; do
      stt=$($SSH_COMMAND -n "tar xf tdh-home.tar && echo true && touch ~/tdh_home_unpacked")
      [ "$stt" == "true" ] && break
      let cnt=cnt+1
      sleep 30
    done 
  fi

  # --- COPY CACHED SOFTWARE FROM MYWMWARE.COM THAT PROBABLY CAN NOT BE DOWNLOADED AGAIN ---
  ver=$(ls -1 $TDHPATH/files/tdh-tools/tdh-tools-tkg-*.cfg | sed -e 's/^.*tools-tkg-//g' -e 's/\.cfg//g' | head -1) 
  $SSH_COMMAND "[ -d ~/.tanzu-demo-hub/cache/tkg-${ver} ] && exit 0" > /dev/null 2>&1; ret=$?
  if [ "$ret" -ne 0 ]; then 
    (cd $HOME && tar cf /tmp/tdh-cache.tar .tanzu-demo-hub/cache/tkg-${ver} > /dev/null 2>&1)

   ret=1; cnt=0
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      $SCP_COMMAND /tmp/tdh-cache.tar ${SSH_USER}@${SSH_HOST}:$SSH_HOME > /dev/null 2>&1; ret=$?
      [ $ret -eq 0 ] && break
      let cnt=cnt+1
      sleep 30
    done

    stt=""; cnt=0
    while [ "${stt}" != "true" -a $cnt -lt 5 ]; do
      stt=$($SSH_COMMAND -n "tar xf tdh-cache.tar && echo true && touch ~/tdh_cache_unpacked")
      [ "$stt" == "true" ] && break
      let cnt=cnt+1
      sleep 30
    done
  fi

  $SSH_COMMAND "[ -f ~/tdh_cloud_unpacked ] && exit 0" > /dev/null 2>&1; ret=$?
  if [ "$ret" -ne 0 ]; then    
    if [ "$TDH_TKGMC_INFRASTRUCTURE" == "AWS" ]; then
      messagePrint " ▪ Copy AWS Configuration" "$HOME/.aws"
      (cd $HOME && tar cf /tmp/tdh-cloud.tar .aws > /dev/null 2>&1)
    else
      messagePrint " ▪ Copy Azure Configuration" "$HOME/.azure"
      (cd $HOME && tar cf /tmp/tdh-cloud.tar .azure > /dev/null 2>&1)
    fi

    ret=1; cnt=0
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      $SCP_COMMAND /tmp/tdh-cloud.tar ${SSH_USER}@${SSH_HOST}:$SSH_HOME > /dev/null 2>&1; ret=$?
      [ $ret -eq 0 ] && break
      let cnt=cnt+1
      sleep 30
    done
 
    stt=""; cnt=0
    while [ "${stt}" != "true" -a $cnt -lt 5 ]; do
      stt=$($SSH_COMMAND -n "tar xf tdh-cloud.tar && echo true && touch ~/tdh_cloud_unpacked")
      [ "$stt" == "true" ] && break
      let cnt=cnt+1
      sleep 30
    done
  fi

  # --- COPY SOFTWARE ---
  $SSH_COMMAND "[ -d /home/ubuntu/tanzu-demo-hub/software ] && exit 0" > /dev/null 2>&1; ret=$?
  if [ "$ret" -ne 0 ]; then
    # --- COPY THE AWS DIRECTORY ---
    messagePrint " ▪ Copy Tools and Software" "$HOME/tanzu-demo-hub/software"
    $SSH_COMMAND -n "[ ! -d $SSH_HOME/tanzu-demo-hub/software ] && mkdir -p $SSH_HOME/tanzu-demo-hub/software"
    if [ "$TDH_TKGMC_INFRASTRUCTURE" == "Azure" ]; then
      $SCP_COMMAND -r ${TDHPATH}/software/tkg-linux* ${SSH_USER}@${SSH_HOST}:${SSH_HOME}/tanzu-demo-hub/software > /dev/null 2>&1
      $SCP_COMMAND -r ${TDHPATH}/software/tanzu-cli-bundle-linux* ${SSH_USER}@${SSH_HOST}:${SSH_HOME}/tanzu-demo-hub/software > /dev/null 2>&1
      $SCP_COMMAND -r ${TDHPATH}/software/velero-linux* ${SSH_USER}@${SSH_HOST}:${SSH_HOME}/tanzu-demo-hub/software > /dev/null 2>&1
    fi

    if [ "$TDH_TKGMC_INFRASTRUCTURE" == "vSphere" ]; then
      $SCP_COMMAND -r ${TDHPATH}/software/tkg-* ${SSH_USER}@${SSH_HOST}:${SSH_HOME}/tanzu-demo-hub/software > /dev/null 2>&1
      $SCP_COMMAND -r ${TDHPATH}/software/pho* ${SSH_USER}@${SSH_HOST}:${SSH_HOME}/tanzu-demo-hub/ > /dev/null 2>&1
    fi
  fi

  # --- INSTALL TKG UTILITES ---
  stt=$($SSH_COMMAND -n "[ -f /tkg_software_installed ] && echo true")
  if [ "${stt}" == "" ]; then
    messageTitle "Install TKG Utilities (tkg, ytt, kapp, kbld, kubectl, kind) on Jump Host (jump.${TDH_TKGMC_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN})"

    # --- WAIT UNTIL SSH DEAMON IS READY NO JUMPHOST ----
    ret=1
    while [ $ret -ne 0 ]; do
      $SSH_COMMAND -n id > /dev/null 2>&1; ret=$?
      [ $ret -ne 0 ] && sleep 10
    done

    ret=1; cnt=0
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      $SSH_COMMAND -n "[ -d $SSH_HOME/tanzu-demo-hub ] && chmod a+x ${TDHHOME}/scripts/InstallTKGutilities.sh && \
          sudo ${TDHHOME}/scripts/InstallTKGutilities.sh \"$SSH_HOME/tanzu-demo-hub\" \"$TDH_TKGMC_INFRASTRUCTURE\" $DEBUG"; ret=$?
      [ $ret -eq 0 ] && break
      let cnt=cnt+1
      sleep 30
    done

    if [ ${ret} -ne 0 ]; then
      echo "ERROR: Failed to install TKG Utilities (InstallTKGutilities.sh) on Jump Host"
      echo "       => $SSH_DISPLAY"
      echo "       => sudo ${TDHHOME}/scripts/InstallTKGutilities.sh \"$SSH_HOME/tanzu-demo-hub\" \"$TDH_TKGMC_INFRASTRUCTURE\" $DEBUG"
      exit 1
    #else
    #  messagePrint " ▪ Rebooting Jump Server"   "wait for services comming up ..."
    #  $SSH_COMMAND -n "sudo reboot" > /dev/null 2>&1
    fi

    # --- WAIT UNTIL SSH DEAMON IS READY NO JUMPHOST ----
    #ret=1
    #while [ $ret -ne 0 ]; do
    #  $SSH_COMMAND -n id > /dev/null 2>&1; ret=$?
    #  [ $ret -ne 0 ] && sleep 10
    #done
    #
    #sleep 30
  fi

#  if [ ! -f /etc/motd ]; then
#    echo ""                                                                                         >  /tmp/motd
#    echo '     _____                       ____                         _   _       _'              >> /tmp/motd
#    echo '    |_   _|_ _ _ __  _____   _  |  _ \  ___ _ __ ___   ___   | | | |_   _| |__'           >> /tmp/motd
#    echo '      | |/ _  |  _ \|_  / | | | | | | |/ _ \  _   _ \ / _ \  | |_| | | | |  _ \'          >> /tmp/motd
#    echo '      | | (_| | | | |/ /| |_| | | |_| |  __/ | | | | | (_) | |  _  | |_| | |_) |'         >> /tmp/motd
#    echo '      |_|\__,_|_| |_/___|\__,_| |____/ \___|_| |_| |_|\___/  |_| |_|\__,_|_.__/'          >> /tmp/motd
#    echo ""                                                                                         >> /tmp/motd
#    echo "    ----------------------------------------------------------------------------"         >> /tmp/motd
#    echo "                   Demonstration for VMware Tanzu Product Portfolio"                      >> /tmp/motd
#    echo "                            by Sacha Dubois, VMware Inc"                                  >> /tmp/motd
#    echo "    ----------------------------------------------------------------------------"         >> /tmp/motd
#    echo ""                                                                                         >> /tmp/motd
#    echo "1.) Verify Envrionment"                                                                   >> /tmp/motd
#    echo "    => export KUBECONFIG=~/.tanzu-demo-hub/config/kubeconfig_$TDH_TKGMC_NAME.yaml"        >> /tmp/motd
#    echo "    => kubectl config get-clusters"                                                       >> /tmp/motd
#    echo "    => kubectl config get-contexts"                                                       >> /tmp/motd
#    echo ""                                                                                         >> /tmp/motd
#    echo "2.) Verify Tanzu Management Cluster Status"                                               >> /tmp/motd
#    echo "    => tanzu management-cluster get"                                                      >> /tmp/motd
#    echo ""                                                                                         >> /tmp/motd
#    echo "3.) Create Workload Cluster"                                                              >> /tmp/motd
#    echo "    => tanzu cluster create"                                                              >> /tmp/motd
#    echo ""                                                                                         >> /tmp/motd
#
#    cnt=$(egrep -c "export KUBECONFIG" ~/.bashrc)
#    [ $cnt -eq 0 ] && sed -i '/export KUBECONFIG=.*$/d' ~/.bashrc
#    echo "export KUBECONFIG=~/.tanzu-demo-hub/config/kubeconfig_$TDH_TKGMC_NAME.yaml" >> ~/.bashrc
#    sudo mv /tmp/motd /etc/motd
#  fi
}

verifyHostedZone() {
  zone=$1

  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${zone} | jq -r ".HostedZones[] | select(.Name | \
            scan(\"^$zone.\")).Id")
  if [ "${ZONE_ID}" == "" ]; then
    route53createHostedZone $zone
  fi
}

route53createHostedZone() {
  zone=$1

  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${zone} | jq -r ".HostedZones[] | select(.Name | \
            scan(\"^$zone.\")).Id")
  if [ "${ZONE_ID}" != "" ]; then
    route53deleteHostedZone $ZONE_ID
  fi

  aws route53 create-hosted-zone --name $zone --caller-reference "$(date)" \
       --hosted-zone-config Comment="Managed by tanzu-demo-hub" > /dev/null 2>&1
  if [ $? -ne 0 ]; then
      echo "ERROR: failed to create AWS Route53 zone $ZONE_ID"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/ols.sh"
        echo "          tdh-tools:/$ aws route53 create-hosted-zone --name $zone --caller-reference \"$(date)\" \\"
        echo "                         --hosted-zone-config Comment=\"command-line version\""
        echo "          tdh-tools:/$ exit"
      else
        echo "       => aws route53 create-hosted-zone --name $zone --caller-reference \"$(date)\" \\"
        echo "          --hosted-zone-config Comment=\"command-line version\""
      fi
      exit 1
  fi

  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${zone} | jq -r ".HostedZones[] | select(.Name | \
            scan(\"^$zone.\")).Id")
  NAME_SERVERS=$(aws route53 get-hosted-zone --id $ZONE_ID | jq -r '.DelegationSet.NameServers[]')

  # --- CREATE ZONE ---
  route53setNSrecord $zone "$AWS_HOSTED_DNS_DOMAIN" "$NAME_SERVERS"
}

route53setNSrecord () {
  hnm=$1; dom=$2; dns="$3"

  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${dom} | jq -r ".HostedZones[] | select(.Name | \
            scan(\"^$dom.\")).Id")
  ZONE_ID_STR=$(echo "${ZONE_ID}" | awk -F'/' '{ print $NF }')
  if [ "${ZONE_ID}" == "" ]; then
    echo "ERROR: ZoneID for domain $dom can not be optained, please veriofy manually"
    echo "       aws route53 list-hosted-zones-by-name --dns-name ${dom}"
    exit 1
  fi

  DNS1=$(echo $dns | awk '{ print $1 }')
  DNS2=$(echo $dns | awk '{ print $2 }')
  DNS3=$(echo $dns | awk '{ print $3 }')
  DNS4=$(echo $dns | awk '{ print $4 }')
  TMPROUTE53=/tmp/$$_tmp_route53.json
  echo "{"                                                   >  $TMPROUTE53
  echo "  \"Comment\": \"CREATE/DELETE/UPSERT a record \","  >> $TMPROUTE53
  echo "  \"Changes\": [{"                                   >> $TMPROUTE53
  echo "  \"Action\": \"UPSERT\","                           >> $TMPROUTE53
  echo "  \"ResourceRecordSet\": {"                          >> $TMPROUTE53
  echo "    \"Name\": \"${hnm}\","                           >> $TMPROUTE53
  echo "    \"Type\": \"NS\","                               >> $TMPROUTE53
  echo "    \"TTL\": 300,"                                   >> $TMPROUTE53
  echo "    \"ResourceRecords\": [ "                         >> $TMPROUTE53
  echo "      { \"Value\": \"${DNS1}\" },"                   >> $TMPROUTE53
  echo "      { \"Value\": \"${DNS2}\" },"                   >> $TMPROUTE53
  echo "      { \"Value\": \"${DNS3}\" },"                   >> $TMPROUTE53
  echo "      { \"Value\": \"${DNS4}\" }"                    >> $TMPROUTE53
  echo "    ]"                                               >> $TMPROUTE53
  echo "}}]"                                                 >> $TMPROUTE53
  echo "}"                                                   >> $TMPROUTE53

  aws route53 change-resource-record-sets --hosted-zone-id $ZONE_ID \
      --change-batch file://${TMPROUTE53} > /dev/null 2>&1
  if [ $? -ne 0 ]; then
    echo "4ERROR: failed to set DNS for $hnm"
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ aws route53 change-resource-record-sets --hosted-zone-id \"${ZONE_ID}\" \\"
      echo "                           --change-batch file://${TMPROUTE53}"
      echo "          tdh-tools:/$ exit"
    else
      echo "       => aws route53 change-resource-record-sets --hosted-zone-id \"${ZONE_ID}\" \\"
      echo "              --change-batch file://${TMPROUTE53}"
    fi
    cat /tmp/$$_zone_record
    exit 1
  fi
}

route53deleteHostedZone() {
  ZONE_ID=$1

  aws route53 list-resource-record-sets --hosted-zone-id $ZONE_ID | jq -c '.ResourceRecordSets[]' | \
  while read -r resourcerecordset; do
    read -r name type <<<$(echo $(jq -r '.Name,.Type' <<<"$resourcerecordset"))

     if [ $type != "NS" -a $type != "SOA" ]; then
        aws route53 change-resource-record-sets \
          --hosted-zone-id $ZONE_ID \
          --change-batch '{"Changes":[{"Action":"DELETE","ResourceRecordSet":
          '"$resourcerecordset"'
        }]}' \
        --output text --query 'ChangeInfo.Id' >/dev/null 2>&1
     fi
  done

  aws route53 delete-hosted-zone --id $ZONE_ID >/dev/null 2>&1
  if [ $? -ne 0 ]; then
    echo "ERROR: failed to delete AWS Route53 zone $ZONE_ID"
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ aws route53 delete-hosted-zone --id $ZONE_ID"
      echo "          tdh-tools:/$ exit"
    else
      echo "       => aws route53 delete-hosted-zone --id $ZONE_ID"
    fi
    #exit 1
  fi
}

maskPassword() {
  echo "$1" | sed 's/[^-]/X/g'
}

route53getIPaddress() {
  env=$1
  dom=$2
  hst="jump-${env}.${dom}."
  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${dom} | jq -r ".HostedZones[] | select(.Name | \
            scan(\"^$dom.\")).Id")
  ZONE_ID_STR=$(echo "${ZONE_ID}" | awk -F'/' '{ print $NF }')

  aws route53 list-resource-record-sets --hosted-zone-id $ZONE_ID_STR --query "ResourceRecordSets[?Name == '${hst}']" | \
  jq -r '.[].ResourceRecords[].Value'
}

#route53setDNSrecord "$pip" "$JUMP_HOST" "$AWS_HOSTED_DNS_DOMAIN"
route53setDNSrecord() {
  ipa=$1; hnm=$2; dom=$3

  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${dom} | jq -r '.HostedZones[0].Id')
  ZONE_ID_STR=$(echo "${ZONE_ID}" | awk -F'/' '{ print $NF }')
  if [ "${ZONE_ID}" == "" ]; then
    echo "ERROR: ZoneID for domain $dom can not be optained, please veriofy manually"
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ aws route53 list-hosted-zones-by-name --dns-name ${dom}"
      echo "          tdh-tools:/$ exit"
    else
      echo "       => aws route53 list-hosted-zones-by-name --dns-name ${dom}"
    fi
    exit 1
  fi

cat << EOF | sed -e "s/FQHN/$hnm/g" -e "s/IPADDR/$ipa/g" > /tmp/$$_zone_record
{
            "Comment": "CREATE/DELETE/UPSERT a record ",
            "Changes": [{
            "Action": "UPSERT",
                        "ResourceRecordSet": {
                                    "Name": "FQHN",
                                    "Type": "A",
                                    "TTL": 300,
                                 "ResourceRecords": [{ "Value": "IPADDR"}]
}}]
}
EOF

   aws route53 change-resource-record-sets --hosted-zone-id "${ZONE_ID}" --change-batch file:///tmp/$$_zone_record > /dev/null 2>&1
   if [ $? -ne 0 ]; then
     echo "5ERROR: failed to set DNS for $hnm"
     if [ "$NATIVE" == "0" ]; then
       echo "       => tools/${TDH_TOOLS}.sh"
       echo "          tdh-tools:/$ aws route53 change-resource-record-sets --hosted-zone-id "${ZONE_ID}" --change-batch file:///tmp/$$_zone_record"
       echo "          tdh-tools:/$ exit"
     else
       echo "       => aws route53 change-resource-record-sets --hosted-zone-id "${ZONE_ID}" --change-batch file:///tmp/$$_zone_record"
     fi
     cat /tmp/$$_zone_record
     exit 1
   fi
}

createTKGMCcluster() {
  unset TKG_CONFIG
  [ "$1" != "" ] && TDH_TKGMC_NAME="$1"    ## CLUSTER_NAME SET BY (InstallTKGmc.sh)
  [ ! -d ${HOME}/.tanzu-demo-hub/config ] && mkdir -p ${HOME}/.tanzu-demo-hub/config
  TKG_TEMPLATE=${HOME}/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.yaml; rm -f $TKG_TEMPLATE
  TKG_KUBECONFIG=${HOME}/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.kubeconfig; rm -f $TKG_KUBECONFIG

  cleanKindCluster

  if [ "${TDH_TKGMC_INFRASTRUCTURE}" == "docker" ]; then
    echo "CLUSTER_NAME: $TDH_TKGMC_NAME"                                               >  $TKG_TEMPLATE
    echo "CLUSTER_PLAN: \"dev\""                                                       >> $TKG_TEMPLATE
    echo "ENABLE_MHC: \"false\""                                                       >> $TKG_TEMPLATE
    echo "IDENTITY_MANAGEMENT_TYPE: none"                                              >> $TKG_TEMPLATE
    echo "INFRASTRUCTURE_PROVIDER: docker"                                             >> $TKG_TEMPLATE
    echo "TKG_HTTP_PROXY_ENABLED: \"false\""                                           >> $TKG_TEMPLATE
    #echo "CLUSTER_CIDR: 100.96.0.0/11"                                                 >> $TKG_TEMPLATE
    #echo "SERVICE_CIDR: 100.64.0.0/13"                                                 >> $TKG_TEMPLATE

    # --- LDAP IDENTITY PROVIDOR AVAILABLE ON THE JUMP SERVER ---
    # https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.3/vmware-tanzu-kubernetes-grid-13/GUID-mgmt-clusters-create-config-file.html#identity-mgmt
    #if [ "$TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE" == "ldap" ]; then
    #  echo "IDENTITY_MANAGEMENT_TYPE: ldap"                                                          >> $TKG_TEMPLATE
    #  echo "LDAP_GROUP_SEARCH_BASE_DN: ${LDAP_DOMAIN}"                                               >> $TKG_TEMPLATE
    #  echo "LDAP_HOST: jump.${TDH_TKGMC_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"                           >> $TKG_TEMPLATE
    #  echo "LDAP_GROUP_SEARCH_NAME_ATTRIBUTE: cn"                                                    >> $TKG_TEMPLATE
    #  echo "LDAP_GROUP_SEARCH_USER_ATTRIBUTE: DN"                                                    >> $TKG_TEMPLATE
    #  cho "LDAP_USER_SEARCH_USERNAME: userPrincipalName"                                            >> $TKG_TEMPLATE
    #  echo "TKG_HTTP_PROXY_ENABLED: \"false\""                                                       >> $TKG_TEMPLATE
    #fi

    # --- OIDC IDENTITY PROVIDOR WITH (OKTA) ---
    #if [ "$TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE" == "oidc" ]; then
    #  echo "IDENTITY_MANAGEMENT_TYPE: oidc"                                              >> $TKG_TEMPLATE
    #  echo "OIDC_IDENTITY_PROVIDER_CLIENT_ID: \"$TDH_OKTA_SECRET_ID\""                   >> $TKG_TEMPLATE
    #  echo "OIDC_IDENTITY_PROVIDER_CLIENT_SECRET: \"$TDH_OKTA_CLIENT_SECRET\""           >> $TKG_TEMPLATE
    #  echo "OIDC_IDENTITY_PROVIDER_ISSUER_URL: \"$TDH_OKTA_URL\""                        >> $TKG_TEMPLATE
    #  echo "OIDC_IDENTITY_PROVIDER_SCOPES: \"$TDH_OKTA_SCOPES\""                         >> $TKG_TEMPLATE
    #  echo "OIDC_IDENTITY_PROVIDER_GROUPS_CLAIM:: \"$TDH_OKTA_GROUP_CLAIM\""             >> $TKG_TEMPLATE
    #  echo "OIDC_IDENTITY_PROVIDER_USERNAME_CLAIM: \"$TDH_OKTA_USERNAME_CLAIM\""         >> $TKG_TEMPLATE
    #fi

    if [ "$TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE" == "none" ]; then
      echo "IDENTITY_MANAGEMENT_TYPE: none"                                              >> $TKG_TEMPLATE
    fi

    cnt=$(tanzu management-cluster get 2>/dev/null | egrep -c " $TDH_TKGMC_NAME ")
    if [ ${cnt} -eq 0 ]; then
      # --- CLEANUP OLD CONFIG ---
      export KUBECONFIG=~/.kube-tkg/config

      kubectl config unset clusters.${TDH_TKGMC_NAME}.log > /dev/null 2>&1
      kubectl config unset contexts.${TDH_TKGMC_NAME}-admin@${TDH_TKGMC_NAME} > /dev/null 2>&1
      kubectl config unset users.${TDH_TKGMC_NAME}-admin > /dev/null 2>&1

      # --- CLEAN OLD MANAGEMENT CLUSTER CONFIG ---
      rm -f ~/.tanzu/config.yaml /tmp/$TDH_TKGMC_NAME.log

      # --- CLEAN OLD MANAGEMENT CLUSTER CONFIG ---
      rm -f ~/.tanzu/config.yaml /tmp/$TDH_TKGMC_NAME.log
      rm -rf $HOME/.config/tanzu $HOME/.kube-tkg/config
      cleanKubeconfig           $HOME/.kube/config
      cleanKubeconfig           $HOME/.kube-tkg/config

      # --- DELETE KIND CLUSTER ---
      cnt=$(kind get clusters 2>/dev/null | wc -l | awk '{ print $1 }')
      if [ $cnt -gt 0 ]; then
        id=$(kind get clusters)
        messagePrint " ▪ Manually delete leftover Kind Cluster"        "$id"
        kind delete clusters -all
      fi

      ret=1; cnt=0
      while [ $ret -ne 0 -a $cnt -lt 5 ]; do
        messagePrint " ▪ Management Cluster Creating (Take-$cnt)"        "This may take up to 25min ..."
        CONFIG_FILE=$(ls -1 $HOME/.config/tanzu/tkg/bom/tkg-bom-*.yaml 2>/dev/null)
        if [ "$CONFIG_FILE" != "" ]; then 
          messageTitle "Download docker images to prevent (ImagePullBackOff)"
          if [ -f "$CONFIG_FILE" ]; then
            for n in $(egrep "imagePath|tag:" $CONFIG_FILE | paste - - | awk '{ printf("projects.registry.vmware.com/tkg/%s:%s\n",$2,$4)}' | \
                       egrep "cluster-api" | egrep -v "azure|aws|vsphere"); do
    
              dockerPullImages $n
            done
          fi
        fi

        if [ $DEBUG -gt 0 ]; then
          messageLine
          tanzu management-cluster create -i docker --file $TKG_TEMPLATE -v 0 --log-file=/tmp/${TDH_TKGMC_NAME}.log; ret=$?
          messageLine
        else
          tanzu management-cluster create -i docker --file $TKG_TEMPLATE -v 0 --log-file=/tmp/${TDH_TKGMC_NAME}.log > /tmp/error.log 2>&1; ret=$?
        fi

        [ $ret -eq 0 ] && break

        sleep 300
        let cnt=cnt+1
      done

      cnt=$(tanzu management-cluster get 2>/dev/null | grep -c Error)
      if [ ${ret} -ne 0 -o ${cnt} -gt 0 ]; then
        echo "ERROR: failed to create TKG Management Cluster"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ tanzu management-cluster create --timeout 2h --file $TKG_TEMPLATE -v 0 --log-file=/tmp/$TDH_TKGMC_NAME.log"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => tanzu management-cluster create --file $TKG_TEMPLATE -v 0 --log-file=/tmp/$TDH_TKGMC_NAME.log"
        fi
        echo "-------------------------------------------------------------------------------------------------------------------------------------------------------"
        cat /tmp/$TDH_TKGMC_NAME.log
        echo "-------------------------------------------------------------------------------------------------------------------------------------------------------"

        exit 1
      fi

      messagePrint " ▪ Management Cluster Creating Completed"    "/tmp/$TDH_TKGMC_NAME.log"

      tanzu management-cluster kubeconfig get --admin > /dev/null 2>&1
      kubectl config set-cluster $TDH_TKGMC_NAME > /dev/null 2>&1
      kubectl config use-context ${TDH_TKGMC_NAME}-admin@$TDH_TKGMC_NAME > /dev/null 2>&1
      context=$(kubectl config view -o json | jq -r '.contexts[].name' 2>/dev/null)
      tanzu management-cluster kubeconfig get --admin --export-file=${TKG_KUBECONFIG} > /dev/null 2>&1; ret=$?
      if [ $ret -ne 0 ]; then
        echo "ERROR: failed to generate kubeconfig:"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ tanzu management-cluster kubeconfig get --admin --export-file=${TKG_KUBECONFIG}"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => tanzu management-cluster kubeconfig get --admin --export-file=${TKG_KUBECONFIG}"
        fi
        messageLine
        tanzu management-cluster kubeconfig get --admin --export-file=${TKG_KUBECONFIG}
        messageLine
        exit 1
      fi
    fi
  fi

  if [ "${TDH_TKGMC_INFRASTRUCTURE}" == "vSphere" ]; then
    # --- SET DEFAULT VALUES ---
    [ "$TDH_TKGMC_CONTROL_PLANE_MEM_MIB" == "" ] && TDH_TKGMC_CONTROL_PLANE_MEM_MIB=8192
    [ "$TDH_TKGMC_CONTROL_PLANE_NUM_CPUS" == "" ] && TDH_TKGMC_CONTROL_PLANE_NUM_CPUS=4
    [ "$TDH_TKGMC_CONTROL_PLANE_DISK_GIB" == "" ] && TDH_TKGMC_CONTROL_PLANE_DISK_GIB=40
    [ "$TDH_TKGMC_WORKER_DISK_GIB" == "" ] && TDH_TKGMC_WORKER_DISK_GIB=40
    [ "$TDH_TKGMC_WORKER_MEM_MIB" == "" ] && TDH_TKGMC_WORKER_MEM_MIB=8192
    [ "$TDH_TKGMC_WORKER_NUM_CPUS" == "" ] && TDH_TKGMC_WORKER_NUM_CPUS=2

    messageTitle "Creating TKG Managment Cluster"
    messagePrint " ▪ Cluster Name"                  "$TDH_TKGMC_NAME"
    messagePrint " ▪ Configuration File"            "\${HOME}/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.yaml"
    messagePrint " ▪ Control Plane Machine CPU"     "$TDH_TKGMC_CONTROL_PLANE_NUM_CPUS"
    messagePrint " ▪ Control Plane Machine Memory"  "$TDH_TKGMC_CONTROL_PLANE_MEM_MIB"
    messagePrint " ▪ Control Plane Machine Disk"    "$TDH_TKGMC_CONTROL_PLANE_DISK_GIB"
    messagePrint " ▪ Worker Node Machine CPU"       "$TDH_TKGMC_WORKER_NUM_CPUS"
    messagePrint " ▪ Worker Node Machine Memory"    "$TDH_TKGMC_WORKER_MEM_MIB"
    messagePrint " ▪ Worker Node Machine Disk"      "$TDH_TKGMC_WORKER_DISK_GIB"
    messagePrint " ▪ Cluster CIDR"                  "$TDH_TKGMC_CLUSTER_CIDR"
    messagePrint " ▪ Service CIDR"                  "$TDH_TKGMC_SERVICE_CIDR"
    messagePrint " ▪ Health Check Enabled"          "$TDH_TKGMC_MACHINE_HEALTH_CHECK_ENABLED"

    tanzu management-cluster get > /dev/null 2>&1
    tanzu management-cluster create > /dev/null 2>&1

    echo "INFRASTRUCTURE_PROVIDER: vsphere"                                            >  $TKG_TEMPLATE
    echo "DEPLOY_TKG_ON_VSPHERE7: true"                                                >> $TKG_TEMPLATE
    echo "SERVICE_CIDR: 100.64.0.0/13"                                                 >> $TKG_TEMPLATE
    echo "TKG_HTTP_PROXY_ENABLED: \"false\""                                           >> $TKG_TEMPLATE
    echo "CLUSTER_CIDR: 100.96.0.0/11"                                                 >> $TKG_TEMPLATE
    echo "CLUSTER_NAME: $TDH_TKGMC_NAME"                                               >> $TKG_TEMPLATE
    echo "CLUSTER_PLAN: dev"                                                           >> $TKG_TEMPLATE
    echo "ENABLE_CEIP_PARTICIPATION: \"true\""                                         >> $TKG_TEMPLATE
    echo "ENABLE_MHC: \"true\""                                                        >> $TKG_TEMPLATE

    # --- AVI ---
    echo "AVI_CA_DATA_B64: \"\""                                                       >> $TKG_TEMPLATE
    echo "AVI_CLOUD_NAME: \"\""                                                        >> $TKG_TEMPLATE
    echo "AVI_CONTROLLER: \"\""                                                        >> $TKG_TEMPLATE
    echo "AVI_DATA_NETWORK: \"\""                                                      >> $TKG_TEMPLATE
    echo "AVI_DATA_NETWORK_CIDR: \"\""                                                 >> $TKG_TEMPLATE
    echo "AVI_ENABLE: \"false\""                                                       >> $TKG_TEMPLATE
    echo "AVI_LABELS: \"\""                                                            >> $TKG_TEMPLATE
    echo "AVI_PASSWORD: \"\""                                                          >> $TKG_TEMPLATE
    echo "AVI_SERVICE_ENGINE_GROUP: \"\""                                              >> $TKG_TEMPLATE
    echo "AVI_USERNAME: \"\""                                                          >> $TKG_TEMPLATE

    SSH_KEY=$(cat $VSPHERE_TKGM_SSH_PUBLIC_KEY_FILE) 

    # --- VSPHERE ---
    echo "VSPHERE_CONTROL_PLANE_DISK_GIB: \"$TDH_TKGMC_CONTROL_PLANE_DISK_GIB\""                        >> $TKG_TEMPLATE
    echo "VSPHERE_CONTROL_PLANE_ENDPOINT: $VSPHERE_TKGM_CONTROL_PLANE_ENDPOINT"                         >> $TKG_TEMPLATE
    echo "VSPHERE_CONTROL_PLANE_MEM_MIB: \"$TDH_TKGMC_CONTROL_PLANE_MEM_MIB\""                          >> $TKG_TEMPLATE
    echo "VSPHERE_CONTROL_PLANE_NUM_CPUS: \"$TDH_TKGMC_CONTROL_PLANE_NUM_CPUS\""                        >> $TKG_TEMPLATE
    echo "VSPHERE_DATACENTER: /$VSPHERE_TKGM_DATACENTER"                                                >> $TKG_TEMPLATE
    echo "VSPHERE_DATASTORE: /$VSPHERE_TKGM_DATACENTER/datastore/$VSPHERE_TKGM_DATASTORE"               >> $TKG_TEMPLATE
    echo "VSPHERE_FOLDER: /$VSPHERE_TKGM_DATACENTER/vm/Templates"                                       >> $TKG_TEMPLATE
    echo "VSPHERE_NETWORK: \"$VSPHERE_TKGM_NETWORK\""                                                   >> $TKG_TEMPLATE
    echo "VSPHERE_PASSWORD: <encoded:$(eval printf \"$VSPHERE_TKGM_VCENTER_PASSWORD\" | base64)>"       >> $TKG_TEMPLATE
    echo "VSPHERE_RESOURCE_POOL: /$VSPHERE_TKGM_DATACENTER/host/$VSPHERE_TKGM_CLUSTER/Resources"        >> $TKG_TEMPLATE
    echo "VSPHERE_SERVER: $VSPHERE_TKGM_VCENTER_SERVER"                                                 >> $TKG_TEMPLATE
    echo "VSPHERE_SSH_AUTHORIZED_KEY: $SSH_KEY"                                                         >> $TKG_TEMPLATE
    echo "VSPHERE_INSECURE: true"                                                                       >> $TKG_TEMPLATE
    echo "#VSPHERE_TLS_THUMBPRINT: F3:C6:9D:6E:B1:4B:BC:49:E3:68:6F:F6:7C:AE:5F:24:96:E8:C8:9C"         >> $TKG_TEMPLATE
    echo "VSPHERE_USERNAME: administrator@vsphere.local"                                                >> $TKG_TEMPLATE
    echo "VSPHERE_WORKER_DISK_GIB: \"$TDH_TKGMC_WORKER_DISK_GIB\""                                      >> $TKG_TEMPLATE
    echo "VSPHERE_WORKER_MEM_MIB: \"$TDH_TKGMC_WORKER_MEM_MIB\""                                        >> $TKG_TEMPLATE
    echo "VSPHERE_WORKER_NUM_CPUS: \"$TDH_TKGMC_WORKER_NUM_CPUS\""                                      >> $TKG_TEMPLATE

    # --- LDAP IDENTITY PROVIDOR AVAILABLE ON THE JUMP SERVER ---
    # https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.3/vmware-tanzu-kubernetes-grid-13/GUID-mgmt-clusters-create-config-file.html#identity-mgmt
    if [ "$TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE" == "ldap" ]; then
      echo "IDENTITY_MANAGEMENT_TYPE: ldap"                                                          >> $TKG_TEMPLATE
      echo "LDAP_GROUP_SEARCH_BASE_DN: ${LDAP_DOMAIN}"                                               >> $TKG_TEMPLATE
      echo "LDAP_HOST: jump.${TDH_TKGMC_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"                           >> $TKG_TEMPLATE
      echo "LDAP_GROUP_SEARCH_NAME_ATTRIBUTE: cn"                                                    >> $TKG_TEMPLATE
      echo "LDAP_GROUP_SEARCH_USER_ATTRIBUTE: DN"                                                    >> $TKG_TEMPLATE
      echo "LDAP_USER_SEARCH_USERNAME: userPrincipalName"                                            >> $TKG_TEMPLATE
      echo "TKG_HTTP_PROXY_ENABLED: \"false\""                                                       >> $TKG_TEMPLATE
    fi

    # --- OIDC IDENTITY PROVIDOR WITH (OKTA) ---
    if [ "$TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE" == "oidc" ]; then
      echo "IDENTITY_MANAGEMENT_TYPE: oidc"                                              >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_CLIENT_ID: \"$TDH_OKTA_SECRET_ID\""                   >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_CLIENT_SECRET: \"$TDH_OKTA_CLIENT_SECRET\""           >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_ISSUER_URL: \"$TDH_OKTA_URL\""                        >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_SCOPES: \"$TDH_OKTA_SCOPES\""                         >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_GROUPS_CLAIM:: \"$TDH_OKTA_GROUP_CLAIM\""             >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_USERNAME_CLAIM: \"$TDH_OKTA_USERNAME_CLAIM\""         >> $TKG_TEMPLATE
    fi

    if [ "$TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE" == "none" ]; then
      echo "IDENTITY_MANAGEMENT_TYPE: none"                                              >> $TKG_TEMPLATE
    fi

    cnt=$(tanzu management-cluster get 2>/dev/null | egrep -c " $TDH_TKGMC_NAME ")
    if [ ${cnt} -eq 0 ]; then
      # --- CLEAN OLD MANAGEMENT CLUSTER CONFIG ---
      rm -f ~/.tanzu/config.yaml /tmp/$TDH_TKGMC_NAME.log $TKG_KUBECONFIG
      rm -rf /home/ubuntu/.config/tanzu

      messagePrint " ▪ Management Cluster Creating"        "This may take up to 15min ..."
      ret=1; cnt=0
      while [ $ret -ne 0 -a $cnt -lt 3 ]; do
        if [ $DEBUG -gt 0 ]; then
          messageLine
          tanzu management-cluster create --file $TKG_TEMPLATE -v 0 --log-file=/tmp/$TDH_TKGMC_NAME.log; ret=$?
          messageLine
        else
          tanzu management-cluster create --file $TKG_TEMPLATE -v 0 --log-file=/tmp/$TDH_TKGMC_NAME.log > /dev/null 2>&1; ret=$?
        fi

        [ $ret -eq 0 ] && break

        sleep 30
        let cnt=cnt+1
      done

      cnt=$(tanzu management-cluster get | grep -c Error)
      if [ ${ret} -ne 0 -o ${cnt} -gt 0 ]; then
        echo "ERROR: failed to create TKG Management Cluster"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ tanzu management-cluster create --file $TKG_TEMPLATE -v 0 --log-file=/tmp/$TDH_TKGMC_NAME.log"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => tanzu management-cluster create --file $TKG_TEMPLATE -v 0 --log-file=/tmp/$TDH_TKGMC_NAME.log"
        fi
        echo "-------------------------------------------------------------------------------------------------------------------------------------------------------"
        cat /tmp/$TDH_TKGMC_NAME.log
        echo "-------------------------------------------------------------------------------------------------------------------------------------------------------"

        exit 1
      fi
      
      messagePrint " ▪ Management Cluster Creating Completed"    "/tmp/$TDH_TKGMC_NAME.log"
    fi

    if [ ! -f $TKG_KUBECONFIG ]; then 
      messagePrint " ▪ Export Kubeconfig"    "$TKG_KUBECONFIG"
      tanzu management-cluster kubeconfig get --admin > /dev/null 2>&1
      kubectl config set-cluster $TDH_TKGMC_NAME > /dev/null 2>&1
      kubectl config use-context ${TDH_TKGMC_NAME}-admin@$TDH_TKGMC_NAME > /dev/null 2>&1
      context=$(kubectl config view -o json | jq -r '.contexts[].name' 2>/dev/null)
      tanzu management-cluster kubeconfig get --admin --export-file=${TKG_KUBECONFIG} > /dev/null 2>&1; ret=$?
      if [ $ret -ne 0 ]; then
        echo "ERROR: failed to generate kubeconfig:"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ tanzu management-cluster kubeconfig get --admin --export-file=${TKG_KUBECONFIG}"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => tanzu management-cluster kubeconfig get --admin --export-file=${TKG_KUBECONFIG}"
        fi
        messageLine
        tanzu management-cluster kubeconfig get --admin --export-file=${TKG_KUBECONFIG}
        messageLine
        exit 1
      fi
    fi
  fi

  if [ "${TDH_TKGMC_INFRASTRUCTURE}" == "AWS" ]; then
    messageTitle "Creating TKG Managment Cluster"
    messagePrint " ▪ Cluster Name"                  "$TDH_TKGMC_NAME"
    messagePrint " ▪ Configuration File"            "\${HOME}/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.yaml"
    messagePrint " ▪ Cluster CIDR"                  "$TDH_TKGMC_CLUSTER_CIDR"
    messagePrint " ▪ Service CIDR"                  "$TDH_TKGMC_SERVICE_CIDR"
    messagePrint " ▪ Health Check Enabled"          "$TDH_TKGMC_MACHINE_HEALTH_CHECK_ENABLED"

    [ -f ${HOME}/.tkg/config.yaml ] && rm -f ${HOME}/.tkg/config.yaml
    tkg get mc > /dev/null 2>&1
    tanzu management-cluster get > /dev/null 2>&1
    tanzu management-cluster create > /dev/null 2>&1
    #cp ${HOME}/.tanzu/config.yaml $TKG_TEMPLATE

    # AWS_SSH_PUBLIC_KEY_B64=$(cat ~/.tanzu-demo-hub/KeyPair-${SSH_KEY_NAME}-${AWS_REGION}.pem) | base64 -w 10000)

    # convert to dc=...,dc=
    LDAP_DOMAIN=$(echo ${TDH_TKGMC_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN} | \
      awk -F'.' '{ for (i = 1; i <= 3; i++) { printf(",dc=%s",$i) }}END { printf "\n"}' | sed 's/^,//g')

    echo "INFRASTRUCTURE_PROVIDER: aws"                                                >  $TKG_TEMPLATE
    echo "AWS_REGION: $AWS_REGION"                                                     >> $TKG_TEMPLATE
    echo "AWS_NODE_AZ: \"$AWS_PRIMARY_AZ\""                                            >> $TKG_TEMPLATE
    echo "AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY"                                          >> $TKG_TEMPLATE
    echo "AWS_SECRET_ACCESS_KEY: $AWS_SECRET_KEY"                                      >> $TKG_TEMPLATE
    echo "AWS_SSH_KEY_NAME: tanzu-demo-hub"                                            >> $TKG_TEMPLATE
    echo "BASTION_HOST_ENABLED: false"                                                 >> $TKG_TEMPLATE
    echo "#AWS_AMI_ID: ami-066dad3403314512a"                                          >> $TKG_TEMPLATE
    echo "CLUSTER_PLAN: dev"                                                           >> $TKG_TEMPLATE
    echo "CLUSTER_NAME: $TDH_TKGMC_NAME"                                               >> $TKG_TEMPLATE

    # --- RESOURCE DEFINITIONS ---
    #echo "CONTROL_PLANE_MACHINE_TYPE: c4.2xlarge"                                      >> $TKG_TEMPLATE
    #echo "NODE_MACHINE_TYPE: c4.2xlarge"                                               >> $TKG_TEMPLATE
    echo "CONTROL_PLANE_MACHINE_TYPE: $TDH_TKGMC_CONTROL_PLANE_MACHINE_TYPE"           >> $TKG_TEMPLATE
    echo "NODE_MACHINE_TYPE: $TDH_TKGMC_MACHINE_TYPE"                                  >> $TKG_TEMPLATE

    echo "#AWS_ACCESS_KEY_ID:"                                                         >> $TKG_TEMPLATE
    echo "#AWS_B64ENCODED_CREDENTIALS: "                                               >> $TKG_TEMPLATE
    echo "#AWS_NODE_AZ: eu-central-1a"                                                 >> $TKG_TEMPLATE
    echo "#AWS_NODE_AZ_1: \"\""                                                        >> $TKG_TEMPLATE
    echo "#AWS_NODE_AZ_2: \"\""                                                        >> $TKG_TEMPLATE
    echo "#AWS_PRIVATE_NODE_CIDR: 10.0.16.0/20"                                        >> $TKG_TEMPLATE
    echo "#AWS_PRIVATE_NODE_CIDR_1: \"\""                                              >> $TKG_TEMPLATE
    echo "#AWS_PRIVATE_NODE_CIDR_2: \"\""                                              >> $TKG_TEMPLATE
    echo "#AWS_PRIVATE_SUBNET_ID: \"\""                                                >> $TKG_TEMPLATE
    echo "#AWS_PRIVATE_SUBNET_ID_1: \"\""                                              >> $TKG_TEMPLATE
    echo "#AWS_PRIVATE_SUBNET_ID_2: \"\""                                              >> $TKG_TEMPLATE
    echo "#AWS_PUBLIC_NODE_CIDR: 10.0.0.0/20"                                          >> $TKG_TEMPLATE
    echo "#AWS_PUBLIC_NODE_CIDR_1: \"\""                                               >> $TKG_TEMPLATE
    echo "#AWS_PUBLIC_NODE_CIDR_2: \"\""                                               >> $TKG_TEMPLATE
    echo "#AWS_PUBLIC_SUBNET_ID: \"\""                                                 >> $TKG_TEMPLATE
    echo "#AWS_PUBLIC_SUBNET_ID_1: \"\""                                               >> $TKG_TEMPLATE
    echo "#AWS_PUBLIC_SUBNET_ID_2: \"\""                                               >> $TKG_TEMPLATE
    echo "#AWS_VPC_CIDR: 10.0.0.0/16"                                                  >> $TKG_TEMPLATE
    echo "#AWS_VPC_ID: \"\""                                                           >> $TKG_TEMPLATE
    echo "#BASTION_HOST_ENABLED: \"true\""                                             >> $TKG_TEMPLATE
    echo "#CLUSTER_CIDR: 100.96.0.0/11"                                                >> $TKG_TEMPLATE
    echo "#ENABLE_CEIP_PARTICIPATION: \"false\""                                       >> $TKG_TEMPLATE
    echo "#ENABLE_MHC: \"true\""                                                       >> $TKG_TEMPLATE
    echo "#IDENTITY_MANAGEMENT_TYPE: oidc"                                             >> $TKG_TEMPLATE
    echo "#INFRASTRUCTURE_PROVIDER: aws"                                               >> $TKG_TEMPLATE
    echo "#LDAP_BIND_DN: \"\""                                                         >> $TKG_TEMPLATE
    echo "#LDAP_BIND_PASSWORD: \"\""                                                   >> $TKG_TEMPLATE
    echo "#LDAP_GROUP_SEARCH_BASE_DN: \"\""                                            >> $TKG_TEMPLATE
    echo "#LDAP_GROUP_SEARCH_FILTER: \"\""                                             >> $TKG_TEMPLATE
    echo "#LDAP_GROUP_SEARCH_GROUP_ATTRIBUTE: \"\""                                    >> $TKG_TEMPLATE
    echo "#LDAP_GROUP_SEARCH_NAME_ATTRIBUTE: cn"                                       >> $TKG_TEMPLATE
    echo "#LDAP_GROUP_SEARCH_USER_ATTRIBUTE: DN"                                       >> $TKG_TEMPLATE
    echo "#LDAP_HOST: \"\""                                                            >> $TKG_TEMPLATE
    echo "#LDAP_ROOT_CA_DATA_B64: \"\""                                                >> $TKG_TEMPLATE
    echo "#LDAP_USER_SEARCH_BASE_DN: \"\""                                             >> $TKG_TEMPLATE
    echo "#LDAP_USER_SEARCH_FILTER: \"\""                                              >> $TKG_TEMPLATE
    echo "#LDAP_USER_SEARCH_NAME_ATTRIBUTE: \"\""                                      >> $TKG_TEMPLATE
    echo "#LDAP_USER_SEARCH_USERNAME: userPrincipalName"                               >> $TKG_TEMPLATE

    # --- LDAP IDENTITY PROVIDOR AVAILABLE ON THE JUMP SERVER ---
    # https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.3/vmware-tanzu-kubernetes-grid-13/GUID-mgmt-clusters-create-config-file.html#identity-mgmt
    if [ "$TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE" == "ldap" ]; then 
      echo "IDENTITY_MANAGEMENT_TYPE: ldap"                                             >> $TKG_TEMPLATE
      echo "LDAP_GROUP_SEARCH_BASE_DN: ${LDAP_DOMAIN}"                                  >> $TKG_TEMPLATE
      echo "LDAP_HOST: jump.${TDH_TKGMC_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"              >> $TKG_TEMPLATE
      echo "LDAP_GROUP_SEARCH_NAME_ATTRIBUTE: cn"                                       >> $TKG_TEMPLATE
      echo "LDAP_GROUP_SEARCH_USER_ATTRIBUTE: DN"                                       >> $TKG_TEMPLATE
      echo "LDAP_USER_SEARCH_USERNAME: userPrincipalName"                               >> $TKG_TEMPLATE
      echo "TKG_HTTP_PROXY_ENABLED: \"false\""                                          >> $TKG_TEMPLATE

#$ grep LDAP tkg-mgmt-cluster.yaml
#LDAP_BIND_DN: cn=Administrator,cn=Users,dc=corp,dc=tanzu
#LDAP_BIND_PASSWORD: <encoded:Vk13YXJlMSE=>
#LDAP_GROUP_SEARCH_BASE_DN: cn=Users,dc=corp,dc=tanzu
#LDAP_GROUP_SEARCH_FILTER: (objectClass=group)
#LDAP_GROUP_SEARCH_GROUP_ATTRIBUTE: member
#LDAP_GROUP_SEARCH_NAME_ATTRIBUTE: cn
#LDAP_GROUP_SEARCH_USER_ATTRIBUTE: DN
#LDAP_HOST: controlcenter.corp.tanzu:636
#LDAP_ROOT_CA_DATA_B64: XXXXXXXXX
#LDAP_USER_SEARCH_BASE_DN: cn=Users,dc=corp,dc=tanzu
#LDAP_USER_SEARCH_FILTER: (objectClass=person)
#LDAP_USER_SEARCH_NAME_ATTRIBUTE: sAMAccountName
#LDAP_USER_SEARCH_USERNAME: sAMAccountName
    fi

    # --- OIDC IDENTITY PROVIDOR WITH (OKTA) ---
    if [ "$TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE" == "oidc" ]; then 
      echo "IDENTITY_MANAGEMENT_TYPE: oidc"                                              >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_CLIENT_ID: \"$TDH_OKTA_SECRET_ID\""                   >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_CLIENT_SECRET: \"$TDH_OKTA_CLIENT_SECRET\""           >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_ISSUER_URL: \"$TDH_OKTA_URL\""                        >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_SCOPES: \"$TDH_OKTA_SCOPES\""                         >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_GROUPS_CLAIM:: \"$TDH_OKTA_GROUP_CLAIM\""             >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_USERNAME_CLAIM: \"$TDH_OKTA_USERNAME_CLAIM\""         >> $TKG_TEMPLATE
    fi

    if [ "$TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE" == "none" ]; then 
      echo "IDENTITY_MANAGEMENT_TYPE: none"                                              >> $TKG_TEMPLATE
    fi

    cnt=$(tanzu management-cluster get 2>/dev/null | egrep -c " $TDH_TKGMC_NAME ")
    if [ ${cnt} -eq 0 ]; then
      # --- CLEANUP OLD CONFIG ---
      export KUBECONFIG=~/.kube-tkg/config

      kubectl config unset clusters.${TDH_TKGMC_NAME}.log > /dev/null 2>&1
      kubectl config unset contexts.${TDH_TKGMC_NAME}-admin@${TDH_TKGMC_NAME} > /dev/null 2>&1
      kubectl config unset users.${TDH_TKGMC_NAME}-admin > /dev/null 2>&1

      # --- CLEAN OLD MANAGEMENT CLUSTER CONFIG ---
      rm -f ~/.tanzu/config.yaml /tmp/$TDH_TKGMC_NAME.log
      rm -rf /home/ubuntu/.config/tanzu

      messagePrint " ▪ Management Cluster Creating"        "This may take up to 15min ..."
      ret=1; cnt=0
      while [ $ret -ne 0 -a $cnt -lt 3 ]; do
        if [ $DEBUG -gt 0 ]; then
          messageLine
          tanzu management-cluster create --timeout 2h --file $TKG_TEMPLATE -v 0 --log-file=/tmp/$TDH_TKGMC_NAME.log; ret=$?
          messageLine
        else
          tanzu management-cluster create --timeout 2h --file $TKG_TEMPLATE -v 0 --log-file=/tmp/$TDH_TKGMC_NAME.log > /dev/null 2>&1; ret=$?
        fi

        [ $ret -eq 0 ] && break

        sleep 30
        let cnt=cnt+1
      done

      cnt=$(tanzu management-cluster get 2>/dev/null | grep -c Error)
      if [ ${ret} -ne 0 -o ${cnt} -gt 0 ]; then
        echo "ERROR: failed to create TKG Management Cluster"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ tanzu management-cluster create --file $TKG_TEMPLATE -v 0 --log-file=/tmp/$TDH_TKGMC_NAME.log"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => tanzu management-cluster create --file $TKG_TEMPLATE -v 0 --log-file=/tmp/$TDH_TKGMC_NAME.log"
        fi
        echo "-------------------------------------------------------------------------------------------------------------------------------------------------------"
        cat /tmp/$TDH_TKGMC_NAME.log
        echo "-------------------------------------------------------------------------------------------------------------------------------------------------------"

        exit 1
      fi

      messagePrint " ▪ Management Cluster Creating Completed"    "/tmp/$TDH_TKGMC_NAME.log"
    else
      messagePrint " ▪ Management Cluster already installed"    "/tmp/$TDH_TKGMC_NAME.log"
    fi

    if [ ! -f $TKG_KUBECONFIG ]; then
      tanzu management-cluster kubeconfig get --admin > /dev/null 2>&1
      kubectl config set-cluster $TDH_TKGMC_NAME > /dev/null 2>&1
      kubectl config use-context ${TDH_TKGMC_NAME}-admin@$TDH_TKGMC_NAME > /dev/null 2>&1
      context=$(kubectl config view -o json | jq -r '.contexts[].name' 2>/dev/null)
      tanzu management-cluster kubeconfig get --admin --export-file=${TKG_KUBECONFIG} > /dev/null 2>&1; ret=$?
      if [ ${ret} -ne 0 ]; then
        echo "ERROR: failed to export kubeconfig"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ tanzu management-cluster kubeconfig get --admin --export-file=${TKG_KUBECONFIG}"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => tanzu management-cluster kubeconfig get --admin --export-file=${TKG_KUBECONFIG}"
        fi
        exit 1
      fi
    fi

    kubectl config set-cluster $TDH_TKGMC_NAME > /dev/null 2>&1
    kubectl config use-context ${TDH_TKGMC_NAME}-admin@$TDH_TKGMC_NAME > /dev/null 2>&1

    context=$(kubectl config view -o json | jq -r '.contexts[].name' 2>/dev/null)
    messageTitle "Verify TKG Managment Cluster"
    messagePrint " ▪ Cluster Name"        "$TDH_TKGMC_NAME"
    messagePrint " ▪ Cluster Context"     "$context"
    messagePrint " ▪ Cluster Config"      "~/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.cfg"
    messagePrint " ▪ Cluster Kubeconfig"  "~/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.kubeconfig"

    echo ""                                                                                         >  /tmp/motd
    echo '     _____                       ____                         _   _       _'              >> /tmp/motd
    echo '    |_   _|_ _ _ __  _____   _  |  _ \  ___ _ __ ___   ___   | | | |_   _| |__'           >> /tmp/motd
    echo '      | |/ _  |  _ \|_  / | | | | | | |/ _ \  _   _ \ / _ \  | |_| | | | |  _ \'          >> /tmp/motd
    echo '      | | (_| | | | |/ /| |_| | | |_| |  __/ | | | | | (_) | |  _  | |_| | |_) |'         >> /tmp/motd
    echo '      |_|\__,_|_| |_/___|\__,_| |____/ \___|_| |_| |_|\___/  |_| |_|\__,_|_.__/'          >> /tmp/motd
    echo ""                                                                                         >> /tmp/motd
    echo "    ----------------------------------------------------------------------------"         >> /tmp/motd
    echo "                   Demonstration for VMware Tanzu Product Portfolio"                      >> /tmp/motd
    echo "                            by Sacha Dubois, VMware Inc"                                  >> /tmp/motd
    echo "    ----------------------------------------------------------------------------"         >> /tmp/motd
    echo ""                                                                                         >> /tmp/motd
    echo "1.) Verify Envrionment"                                                                   >> /tmp/motd
    echo "    => export KUBECONFIG=~/.tanzu-demo-hub/config/kubeconfig_$TDH_TKGMC_NAME.yaml"        >> /tmp/motd
    echo "    => kubectl config get-clusters"                                                       >> /tmp/motd
    echo "    => kubectl config get-contexts"                                                       >> /tmp/motd
    echo ""                                                                                         >> /tmp/motd
    echo "2.) Verify Tanzu Management Cluster Status"                                               >> /tmp/motd
    echo "    => tanzu management-cluster get"                                                      >> /tmp/motd
    echo ""                                                                                         >> /tmp/motd
    echo "3.) Create Workload Cluster"                                                              >> /tmp/motd
    echo "    => tanzu cluster create"                                                              >> /tmp/motd
    echo ""                                                                                         >> /tmp/motd

    cnt=$(egrep -c "export KUBECONFIG" ~/.bashrc)
    [ $cnt -eq 0 ] && sed -i '/export KUBECONFIG=.*$/d' ~/.bashrc
    echo "export KUBECONFIG=~/.tanzu-demo-hub/config/kubeconfig_$TDH_TKGMC_NAME.yaml" >> ~/.bashrc

    sudo mv /tmp/motd /etc/motd
  fi

  if [ "${TDH_TKGMC_INFRASTRUCTURE}" == "Azure" ]; then
    # --- SET DEFAULT VALUES ---
    [ "$TDH_TKGMC_CONTROL_PLANE_MACHINE_TYPE" == "" ] && TDH_TKGMC_CONTROL_PLANE_MACHINE_TYPE=Standard_D3_v2
    [ "$TDH_TKGMC_MACHINE_TYPE" == "" ] && TDH_TKGMC_MACHINE_TYPE=Standard_D2_v4

    messageTitle "Creating TKG Managment Cluster"
    messagePrint " ▪ Cluster Name"                  "$TDH_TKGMC_NAME"
    messagePrint " ▪ Configuration File"            "\${HOME}/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.yaml"
    messagePrint " ▪ Cluster CIDR"                  "$TDH_TKGMC_CLUSTER_CIDR"
    messagePrint " ▪ Service CIDR"                  "$TDH_TKGMC_SERVICE_CIDR"
    messagePrint " ▪ Health Check Enabled"          "$TDH_TKGMC_MACHINE_HEALTH_CHECK_ENABLED"
    messagePrint " ▪ Control Plane Machine Type"    "$TDH_TKGMC_CONTROL_PLANE_MACHINE_TYPE"
    messagePrint " ▪ Worker Node Machine Type"      "$TDH_TKGMC_MACHINE_TYPE"

    [ -f ${HOME}/.tkg/config.yaml ] && rm -f ${HOME}/.tkg/config.yaml
    tkg get mc > /dev/null 2>&1
    tanzu management-cluster get > /dev/null 2>&1
    tanzu management-cluster create > /dev/null 2>&1
    #cp ${HOME}/.tanzu/config.yaml $TKG_TEMPLATE

    # ste 5. Oct 2021 - /home/ubuntu/.config/tanzu was owned by root:root, change it to ubuntu:ubuntu
    sudo chown -R ubuntu:ubuntu /home/ubuntu/.config > /dev/null 2>&1

    az ad app list --display-name "TanzuDemoHub-$TDH_USER" > /dev/null 2>&1
    if [ $? -ne 0 ]; then 
      echo "ERROR: failed to list apps"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ az ad app list --display-name \"TanzuDemoHub-$TDH_USER\""
        echo "          tdh-tools:/$ exit"
      else
        echo "       => az ad app list --display-name \"TanzuDemoHub-$TDH_USER\""
      fi
      exit 1
    fi

    AZURE_SSH_PUBLIC_KEY_B64=$(cat ~/.tanzu-demo-hub/KeyPair-Azure.pub | base64 -w 10000)
    AZURE_CLIENT_ID=$(az ad app list --display-name "TanzuDemoHub-$TDH_USER" 2>/dev/null | jq -r '.[].appId')

    echo "CLUSTER_NAME: $TDH_TKGMC_NAME"                                              >> $TKG_TEMPLATE
    echo "AZURE_CLIENT_ID: $AZURE_CLIENT_ID"                                          >> $TKG_TEMPLATE
    echo "AZURE_CLIENT_SECRET: $AZURE_CLIENT_SECRET"                                  >> $TKG_TEMPLATE
    echo "AZURE_CONTROL_PLANE_MACHINE_TYPE: $TDH_TKGMC_CONTROL_PLANE_MACHINE_TYPE"    >> $TKG_TEMPLATE
    echo "AZURE_CONTROL_PLANE_SUBNET_CIDR: 10.0.0.0/16"                               >> $TKG_TEMPLATE
    echo "AZURE_CONTROL_PLANE_SUBNET_NAME: control-plane-vnet"                        >> $TKG_TEMPLATE
    echo "AZURE_LOCATION: $AZURE_LOCATION"                                            >> $TKG_TEMPLATE
    echo "AZURE_NODE_MACHINE_TYPE: $TDH_TKGMC_MACHINE_TYPE"                           >> $TKG_TEMPLATE
    echo "AZURE_NODE_SUBNET_CIDR: 10.1.0.0/16"                                        >> $TKG_TEMPLATE
    echo "AZURE_NODE_SUBNET_NAME: worker-node-vnet"                                   >> $TKG_TEMPLATE
    echo "AZURE_RESOURCE_GROUP: ${TDH_TKGMC_NAME}"                                    >> $TKG_TEMPLATE
    echo "AZURE_SSH_PUBLIC_KEY_B64: $AZURE_SSH_PUBLIC_KEY_B64"                        >> $TKG_TEMPLATE
    echo "AZURE_SUBSCRIPTION_ID: $AZURE_SUBSCRIPTION_ID"                              >> $TKG_TEMPLATE
    echo "AZURE_TENANT_ID: $AZURE_TENANT_ID"                                          >> $TKG_TEMPLATE
    echo ""                                                                           >> $TKG_TEMPLATE
    echo "AZURE_VNET_CIDR: 10.0.0.0/8"                                                >> $TKG_TEMPLATE
    echo "AZURE_VNET_NAME: ${TDH_TKGMC_NAME}-vnet"                                    >> $TKG_TEMPLATE
    echo "AZURE_VNET_RESOURCE_GROUP: $TDH_TKGMC_NAME"                                 >> $TKG_TEMPLATE
    echo "CLUSTER_CIDR: $TDH_TKGMC_CLUSTER_CIDR"                                      >> $TKG_TEMPLATE
    echo "CLUSTER_PLAN: $TDH_TKGMC_PLAN"                                              >> $TKG_TEMPLATE
    echo "ENABLE_CEIP_PARTICIPATION: \"false\""                                       >> $TKG_TEMPLATE
    echo "SERVICE_CIDR: $TDH_TKGMC_SERVICE_CIDR"                                      >> $TKG_TEMPLATE

    echo "ENABLE_MHC: \"true\""                                                       >> $TKG_TEMPLATE
    echo "MHC_UNKNOWN_STATUS_TIMEOUT: 10m"                                            >> $TKG_TEMPLATE
    echo "MHC_FALSE_STATUS_TIMEOUT: 20m"                                              >> $TKG_TEMPLATE

    echo ""                                                                           >> $TKG_TEMPLATE
    echo "INFRASTRUCTURE_PROVIDER: azure"                                             >> $TKG_TEMPLATE

    # --- LDAP IDENTITY PROVIDOR AVAILABLE ON THE JUMP SERVER ---
    # https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.3/vmware-tanzu-kubernetes-grid-13/GUID-mgmt-clusters-create-config-file.html#identity-mgmt
    if [ "$TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE" == "ldap" ]; then
      echo "IDENTITY_MANAGEMENT_TYPE: ldap"                                             >> $TKG_TEMPLATE
      echo "LDAP_GROUP_SEARCH_BASE_DN: dc={TDH_TKGMC_ENVNAME},dc=pcfsdu,dc=com"         >> $TKG_TEMPLATE
      echo "LDAP_HOST: jump.${TDH_TKGMC_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"              >> $TKG_TEMPLATE
      echo "INFRASTRUCTURE_PROVIDER: azure"                                             >> $TKG_TEMPLATE
      echo "LDAP_GROUP_SEARCH_NAME_ATTRIBUTE: cn"                                       >> $TKG_TEMPLATE
      echo "LDAP_GROUP_SEARCH_USER_ATTRIBUTE: DN"                                       >> $TKG_TEMPLATE
      echo "LDAP_USER_SEARCH_USERNAME: userPrincipalName"                               >> $TKG_TEMPLATE
      echo "SERVICE_CIDR: 100.64.0.0/13"                                                >> $TKG_TEMPLATE
      echo "TKG_HTTP_PROXY_ENABLED: \"false\""                                          >> $TKG_TEMPLATE
    fi

    # --- OIDC IDENTITY PROVIDOR WITH (OKTA) ---
    if [ "$TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE" == "oidc" ]; then
      echo "IDENTITY_MANAGEMENT_TYPE: oidc"                                              >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_CLIENT_ID: \"$TDH_OKTA_SECRET_ID\""                   >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_CLIENT_SECRET: \"$TDH_OKTA_CLIENT_SECRET\""           >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_ISSUER_URL: \"$TDH_OKTA_URL\""                        >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_SCOPES: \"$TDH_OKTA_SCOPES\""                         >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_GROUPS_CLAIM:: \"$TDH_OKTA_GROUP_CLAIM\""             >> $TKG_TEMPLATE
      echo "OIDC_IDENTITY_PROVIDER_USERNAME_CLAIM: \"$TDH_OKTA_USERNAME_CLAIM\""         >> $TKG_TEMPLATE
    fi

    if [ "$TDH_TKGMC_IDENTITY_MANAGEMENT_TYPE" == "none" ]; then
      echo "IDENTITY_MANAGEMENT_TYPE: none"                                              >> $TKG_TEMPLATE
    fi

    scripts/InstallTKGmcContainer.sh $TDH_TKGMC_NAME $TKG_TEMPLATE $TKG_KUBECONFIG; ret=$?
    if [ ${ret} -ne 0 ]; then
      echo "ERROR: failed to create management clster"
      echo "       => scripts/InstallTKGmcContainer.sh $TDH_TKGMC_NAME $TKG_TEMPLATE $TKG_KUBECONFIG"
      exit 1
    fi

    export KUBECONFIG=$TKG_KUBECONFIG
    cmdLoop kubectl config set-cluster $TDH_TKGMC_NAME > /dev/null 2>&1
    cmdLoop kubectl config use-context ${TDH_TKGMC_NAME}-admin@$TDH_TKGMC_NAME > /dev/null 2>&1

    cmdLoop kubectl config view -o json > /tmp/output.json
    context=$(jq -r '.contexts[].name'  /tmp/output.json 2>/dev/null)
    messageTitle "Verify TKG Managment Cluster"
    messagePrint " ▪ Cluster Name"        "$TDH_TKGMC_NAME"
    messagePrint " ▪ Cluster Context"     "$context"
    messagePrint " ▪ Cluster Config"      "~/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.cfg"
    messagePrint " ▪ Cluster Kubeconfig"  "~/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.kubeconfig"
  fi
}

setTKGclusterDNS() {
  TKG _CLUSTER="$1"
  TKG_IPADRESS="$2"
  TKG_INGRESS="$3"
  DNS_PREFIX="$TDH_TKGMC_ENVNAME"
  DNS_SUFFIX="$AWS_HOSTED_DNS_DOMAIN"

  if [ "${TDH_TKGMC_INFRASTRUCTURE}" == "AWS" ]; then
    AWS_ID=$(echo $DNSLB | awk -F '-' '{ print $1 }')
    AWS_LB="k8s-master-$TKG_CLUSTER"
    AWS_SG=$(aws elb --region $AWS_REGION describe-load-balancers --load-balancer-names $AWS_ID | \
             jq -r '.LoadBalancerDescriptions[].SecurityGroups[]')
    AWS_VP=$(aws elb --region $AWS_REGION describe-load-balancers --load-balancer-names $AWS_ID | \
             jq -r '.LoadBalancerDescriptions[].VPCId')
  
    cnt=$(echo "${AWS_SG}" | egrep -c "$AWS_SG_NEW")
    if [ $cnt -eq 0 ]; then
      a=1
    fi
  fi
  
  if [ "${TDH_TKGMC_INFRASTRUCTURE}" == "GCP" ]; then
    DNS_NAME="*.${TKG_INGRESS}-${TKG_CLUSTER}"
    messagePrint "▪ Create DNS Entry for:" "*.${TKG_INGRESS}-${TKG_CLUSTER}.${DNS_PREFIX}.${DNS_SUFFIX}"
  
    cnt=$(gcloud dns record-sets list -z ${DNS_PREFIX}-zone --name "${DNS_NAME}.${DNS_PREFIX}.${DNS_SUFFIX}." \
            --type=A 2> /dev/null | grep -v "^NAME" | wc -l | sed 's/ //g')
    if [ ${cnt} -eq 0 ]; then
      messagePrint " ▪ Creating DNS Entry in (${DNS_PREFIX}-zone)" "${DNS_NAME}"
      gcloud dns record-sets transaction abort -z ${DNS_PREFIX}-zone > /dev/null 2>&1
      gcloud dns record-sets transaction start -z ${DNS_PREFIX}-zone > /dev/null 2>&1
      gcloud dns record-sets transaction add "$DNSLB" --name "${DNS_NAME}.${DNS_PREFIX}.${DNS_SUFFIX}." \
         --type A -z ${DNS_PREFIX}-zone --ttl=300 > /dev/null 2>&1
      gcloud dns record-sets transaction execute -z ${DNS_PREFIX}-zone > /dev/null 2>&1; ret=$?
      if [ ${ret} -ne 0 ]; then
        echo "ERROR: Creating DNS record-sets for zone (${DNS_PREFIX}-zone)"
        echo "       => gcloud dns record-sets transaction execute -z ${DNS_PREFIX}-zone"
        exit 1
      fi
    fi
  fi
  
  if [ "${TDH_TKGMC_INFRASTRUCTURE}" == "Azure" -o "${TDH_TKGMC_INFRASTRUCTURE}" == "vSphere" ]; then
    DNS_NAME="*.${TKG_INGRESS}-${TKG_CLUSTER}"
    messagePrint " ▪ LoadBalancer PublicIP:" "$TKG_IPADRESS"
    messagePrint " ▪ Create DNS Entry for:" "*.${TKG_INGRESS}-${TKG_CLUSTER}.${DNS_PREFIX}.${DNS_SUFFIX}"

    ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${DNS_PREFIX}.${DNS_SUFFIX} | jq -r '.HostedZones[0].Id')
    ZONE_ID_STR=$(echo "${ZONE_ID}" | awk -F'/' '{ print $NF }')

    if [ "${ZONE_ID}" != "" ]; then
      ZONE="*.${TKG_CLUSTER}.${DNS_PREFIX}.${DNS_SUFFIX}"
      ZONE="*.${TKG_INGRESS}-${TKG_CLUSTER}.${DNS_PREFIX}.${DNS_SUFFIX}"

      TMPROUTE53=/tmp/$$_tmp_route53.json
      echo "{"                                                            >  $TMPROUTE53
      echo "  \"Comment\": \"CREATE/DELETE/UPSERT a record \","           >> $TMPROUTE53
      echo "  \"Changes\": [{"                                            >> $TMPROUTE53
      echo "    \"Action\": \"UPSERT\","                                  >> $TMPROUTE53
      echo "    \"ResourceRecordSet\": {"                                 >> $TMPROUTE53
      echo "      \"Name\": \"${ZONE}\","                                 >> $TMPROUTE53
      echo "      \"Type\": \"A\","                                       >> $TMPROUTE53
      echo "      \"TTL\": 300,"                                          >> $TMPROUTE53
      echo "      \"ResourceRecords\": [{ \"Value\": \"$TKG_IPADRESS\"}]" >> $TMPROUTE53
      echo "    }"                                                        >> $TMPROUTE53
      echo "  }]"                                                         >> $TMPROUTE53
      echo "}"                                                            >> $TMPROUTE53

      aws route53 change-resource-record-sets --hosted-zone-id $ZONE_ID \
          --change-batch file://${TMPROUTE53} > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "1ERROR: failed to set DNS for $hnm"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ aws route53 change-resource-record-sets --hosted-zone-id \"${ZONE_ID}\" \\"
          echo "                          --change-batch file://${TMPROUTE53}"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => aws route53 change-resource-record-sets --hosted-zone-id \"${ZONE_ID}\" \\"
          echo "              --change-batch file://${TMPROUTE53}"
        fi
        cat $TMPROUTE53
        exit 1
      fi

      rm -f $TMPROUTE53
    fi
  fi

  if [ "${AWS_HOSTED_ZONE_ID}" != "" -a "${TDH_TKGMC_INFRASTRUCTURE}" == "AWS" ]; then
    ZONE_ID_STR=$(echo "${ZONE_ID}" | awk -F'/' '{ print $NF }')
    DNSLB=$(aws elb --region $AWS_REGION describe-load-balancers --load-balancer-names "$AWS_ID" | \
        jq -r '.LoadBalancerDescriptions[0].DNSName')
    DNSLB_ZONEID=$(aws elb --region $AWS_REGION describe-load-balancers --load-balancer-names "$AWS_ID" | \
        jq -r '.LoadBalancerDescriptions[0].CanonicalHostedZoneNameID')
    ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${DNS_PREFIX}.${DNS_SUFFIX} | jq -r '.HostedZones[0].Id')
  
    ALIAS=$(aws route53 list-resource-record-sets --hosted-zone-id $ZONE_ID \
            --query "ResourceRecordSets[?contains(Name, '${TKG_INGRESS}-${TKG_CLUSTER}.${DNS_PREFIX}.${DNS_SUFFIX}')].AliasTarget.DNSName" | \
            jq -r '.[]' | sed -e 's/dualstack\.//g' -e 's/\.$//g' )
  
    if [ "${ALIAS}" != "$DNSLB" ]; then
      if [ "${ZONE_ID}" != "" ]; then
        echo "Create DNS Entry for *.${TKG_INGRESS}-${TKG_CLUSTER}.${DNS_PREFIX}.${DNS_SUFFIX}:"
        ZONE="*.${TKG_CLUSTER}.${DNS_PREFIX}.${DNS_SUFFIX}"
        ZONE="*.${TKG_INGRESS}-${TKG_CLUSTER}.${DNS_PREFIX}.${DNS_SUFFIX}"
  
        TMPROUTE53=/tmp/$$_tmp_route53.json
        echo "{"                                                   >  $TMPROUTE53
        echo "  \"Comment\": \"CREATE/DELETE/UPSERT a record \","  >> $TMPROUTE53
        echo "  \"Changes\": [{"                                   >> $TMPROUTE53
        echo "  \"Action\": \"UPSERT\","                           >> $TMPROUTE53
        echo "  \"ResourceRecordSet\": {"                          >> $TMPROUTE53
        echo "    \"Name\": \"${ZONE}\","                          >> $TMPROUTE53
        echo "    \"Type\": \"A\","                                >> $TMPROUTE53
        echo "    \"AliasTarget\": {"                              >> $TMPROUTE53
        echo "      \"HostedZoneId\": \"${DNSLB_ZONEID}\","        >> $TMPROUTE53
        echo "      \"DNSName\": \"dualstack.${DNSLB}.\","         >> $TMPROUTE53
        echo "      \"EvaluateTargetHealth\": true"                >> $TMPROUTE53
        echo "    }"                                               >> $TMPROUTE53
        echo "}}]"                                                 >> $TMPROUTE53
        echo "}"                                                   >> $TMPROUTE53
  
        aws route53 change-resource-record-sets --hosted-zone-id $ZONE_ID \
            --change-batch file://${TMPROUTE53} > /dev/null 2>&1
        if [ $? -ne 0 ]; then
          echo "2ERROR: failed to set DNS for $hnm"
          if [ "$NATIVE" == "0" ]; then
            echo "       => tools/${TDH_TOOLS}.sh"
            echo "          tdh-tools:/$ aws route53 change-resource-record-sets --hosted-zone-id \"${ZONE_ID}\" \\"
            echo "                        --change-batch file://${TMPROUTE53}"
            echo "          tdh-tools:/$ exit"
          else
            echo "       => aws route53 change-resource-record-sets --hosted-zone-id \"${ZONE_ID}\" \\"
            echo "              --change-batch file://${TMPROUTE53}"
          fi
          cat $TMPROUTE53
          exit 1
        fi
  
        rm -f $TMPROUTE53
      fi
    fi
  fi
}

verifyTLScertificate() {
  TLS_CERTIFICATE=$1
  TLS_PRIVATE_KEY=$2

  dif=$((openssl x509 -in $TLS_CERTIFICATE -noout -modulus; openssl rsa -in $TLS_PRIVATE_KEY -noout -modulus) | \
      uniq -c | awk '{ print $1 }')
  if [ "$dif" == "" ]; then dif=0; fi
  if [ $dif -ne 2 ]; then
    echo "ERROR: Certificate modulus of $TLS_CERTIFICATE does not match with the private_key $TLS_PRIVATE_KEY"
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ openssl x509 -in $TLS_CERTIFICATE -noout -modulus"
      echo "          tdh-tools:/$ openssl rsa -in $TLS_PRIVATE_KEY -noout -modulus"
      echo "          tdh-tools:/$ exit"
    else
      echo "       => openssl x509 -in $TLS_CERTIFICATE -noout -modulus"
      echo "       => openssl rsa -in $TLS_PRIVATE_KEY -noout -modulus"
    fi
    exit 1
  fi

  if [ ! -f ${TDHPATH}/certificates/ca.pem ]; then
    # --- TRY TO FIND THE ROOT_CA ---
    if [ "${ROOT_CA_NAME}" == "DST Root CA X3" ]; then
      ROOT_CA_URL="https://letsencrypt.org/certs/trustid-x3-root.pem.txt"
      curl --output /tmp/trustid-x3-root.pem $ROOT_CA_URL > /dev/null 2>&1
      TLS_ROOT_CA=/tmp/trustid-x3-root.pem
      #eval export ${CLOUD}_${MODE}_TLS_ROOT_CA=/tmp/trustid-x3-root.pem
    fi

    # --- GENERATE CA.PEM WITH ROOT_CA AND FULLCHAIN ---
    cat $TLS_ROOT_CA $TLS_CERTIFICATE > ${TDHPATH}/certificates/ca.pem

    if [ "${TLS_ROOT_CA}" == "" ]; then
      echo "ERROR: ROOT_CA for $ROOT_CA_NAME could not be found, please download the file manually"
      echo "       and set the variable in the ~/.tanzu-demo-hub.cfg"
      echo "       => export ${CLOUD}_${MODE}_TLS_ROOT_CA=<ca.crt>"
      exit 1
    fi
  fi

}

verifyCertificate () {
  CLOUD="$1"
  TLS_CERTIFICATE="$2"
  TLS_FULLCHAIN="$3"
  TLS_PRIVATE_KEY="$4"
  TLS_CHAIN="$5"
  TLS_ROOT_CA="$6"


echo "CLOUD:$CLOUD"
echo "CLOUD:$CLOUD"
echo "CLOUD:$CLOUD"
echo "CLOUD:$CLOUD"

exit

  if [ "${TLS_CERTIFICATE}" == "" -o "${TLS_PRIVATE_KEY}" == "" -o \
       "${TLS_FULLCHAIN}" == "" ]; then

    echo ""
    echo "  3MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
    echo "  -------------------------------------------------------------------------------------------------------------------------------------------------------"
    echo "  To allow TLS encrypted httpd traffic a Certificate and key needs to be created for your DNS Domain. Free"
    echo "  certificates can be optained through https://letsencrypt.org."
    echo ""

    if [ "${TLS_CERTIFICATE}" == "" ]; then
      echo "  ${CLOUD}_${MODE}_TLS_CERTIFICATE       (optional)  TLS Certificate (type PEM Certificate)"
    fi

    if [ "${TLS_FULLCHAIN}" == "" ]; then
      echo "  ${CLOUD}_${MODE}_TLS_FULLCHAIN         (optional)  TLS Fullchain (type PEM Certificate)"
    fi

    if [ "${TLS_PRIVATE_KEY}" == "" ]; then
      echo "  ${CLOUD}_${MODE}_TLS_PRIVATE_KEY       (optional)  TLS Private Key"
    fi

    if [ "${ROOT_CA}" == "" ]; then
      echo "  ${CLOUD}_${MODE}_TLS_ROOT_CA           (automatic) TLS Root CA"
    fi

    dom="$PCF_DEPLOYMENT_ENV_NAME.$AWS_HOSTED_DNS_DOMAIN"
    echo "                                  "
    echo "                                  $PCF_DEPLOYMENT_ENV_NAME.$AWS_HOSTED_DNS_DOMAIN"
    echo "                                    |      |_________ represented by the PCF_ENVIRONMENT_NAME variable"
    echo "                                    |________________ represented by the AWS_HOSTED_DNS_DOMAIN variable"
    echo ""
    echo "                                  The certificate Should include the following domains:"
    echo "                                  - PKS Services (API, OpsMan, Harbor) .: *.${dom}"
    echo "                                  - PKS Cluster and Applications .......: *.apps.cl1.${dom}"
    echo "                                                                          *.apps.cl2.${dom}"
    echo "                                                                          *.apps.cl3.${dom}"
    echo ""
  else
    messageTitle "TLS Encryption for domain (${PCF_DEPLOYMENT_ENV_NAME}.${AWS_HOSTED_DNS_DOMAIN})"

    for n in $TLS_CERTIFICATE $TLS_FULLCHAIN; do
      cnt=$(grep -c "BEGIN CERTIFICATE" $n)
      if [ $cnt -eq 0 ]; then
        echo "ERROR: The file ($n) is not in (Base64) format"
        echo "       It should begin with '-----BEGIN CERTIFICATE-----' and end with '-----END CERTIFICATE-----'"
        exit 1
      fi
    done

    for n in $TLS_PRIVATE_KEY; do
      cnt=$(egrep -c "BEGIN RSA PRIVATE|BEGIN PRIVATE" $n)
      if [ $cnt -eq 0 ]; then
        echo "ERROR: The file ($n) is not (private-key Base64) format"
        echo "       It should begin with '-----BEGIN RSA PRIVATE' and end with '-----END RSA PRIVATE'"
        exit 1
      fi
    done

    # --- VERIFY THE CERTIFICATE ---
    cnm="${PCF_DEPLOYMENT_ENV_NAME}.${AWS_HOSTED_DNS_DOMAIN}"
    cnt=$(openssl crl2pkcs7 -nocrl -certfile $TLS_FULLCHAIN | openssl pkcs7 -print_certs | egrep "^subject|^issuer" | \
          sed -e 's/CN = /CN=/g' -e 's/^subject=.*CN=//g' -e 's/^issuer=.*CN=//g' | egrep -c "^\*.${cnm}")
    if [ $cnt -eq 0 ]; then
      echo "ERROR: The file ($(basename $TLS_FULLCHAIN)) does not contain the CN=*.${cnm}"
      exit 1
    fi

    ISSUER="*.${cnm}"; item_found=1
    while [ ${item_found} -eq 1 ]; do
      item_found=0
      for item in $(openssl crl2pkcs7 -nocrl -certfile $TLS_FULLCHAIN | openssl pkcs7 -print_certs | \
                    egrep "^subject|^issuer" | sed -e 's/CN = /CN=/g' -e 's/^subject=.*CN=//g' \
                    -e 's/^issuer=.*CN=//g' -e 's/ /~1~/g' | paste -d ':' - -); do

        sub=$(echo "${item}" | awk -F: '{ print $1 }')
        iss=$(echo "${item}" | awk -F: '{ print $2 }')

        if [ "${sub}" == "${ISSUER}" ]; then ISSUER="${iss}"; SUBJECT="${sub}"; item_found=1; fi
      done
    done

    ROOT_CA=$ISSUER
    ROOT_CA_NAME=$(echo $ISSUER | sed 's/~1~/ /g')

    if [ "${TLS_ROOT_CA}" == "" ]; then
      # --- TRY TO FIND THE ROOT_CA ---
      if [ "${ROOT_CA_NAME}" == "DST Root CA X3" ]; then
        ROOT_CA_URL="https://letsencrypt.org/certs/trustid-x3-root.pem.txt"
        curl --output /tmp/trustid-x3-root.pem $ROOT_CA_URL > /dev/null 2>&1
        TLS_ROOT_CA=/tmp/trustid-x3-root.pem
        eval export ${CLOUD}_${MODE}_TLS_ROOT_CA=/tmp/trustid-x3-root.pem
      fi

      # --- GENERATE CA.PEM WITH ROOT_CA AND FULLCHAIN ---
      cat $TLS_ROOT_CA $TLS_FULLCHAIN > $HOME/pcfconfig/certificates/ca.pem

      if [ "${TLS_ROOT_CA}" == "" ]; then
        echo "ERROR: ROOT_CA for $ROOT_CA_NAME could not be found, please download the file manually"
        echo "       and set the variable in the ~/.pcfconfig"
        echo "       => export ${CLOUD}_${MODE}_TLS_ROOT_CA=<ca.crt>"
        exit 1
      fi
    fi

    # --- VERIFY TO ROOT_CA ---
    for n in $TLS_ROOT_CA; do
      if [ ! -f $n ]; then
        echo "ERROR: the file $n does not exist"; exit 1
      fi

      cnt=$(grep -c "BEGIN CERTIFICATE" $n)
      if [ $cnt -eq 0 ]; then
        echo "ERROR: The file ($(basename $n)) is not in (Base64) format"
        echo "       It should begin with '-----BEGIN CERTIFICATE-----' and end with '-----END CERTIFICATE-----'"
        exit 1
      fi
    done

    # --- VERIFY THE CERTIFICATE ISSUER AND SUBJECT ---
    tmp=$(openssl crl2pkcs7 -nocrl -certfile $TLS_ROOT_CA | openssl pkcs7 -print_certs | \
          egrep "^subject|^issuer" | sed -e 's/CN = /CN=/g' -e 's/^subject=.*CN=//g' -e 's/^issuer=.*CN=//g' -e 's/ /~1~/g' | \
          paste -d ':' - -)
    sub=$(echo "${tmp}" | awk -F: '{ print $1 }' | sed 's/~1~/ /g')
    iss=$(echo "${tmp}" | awk -F: '{ print $2 }' | sed 's/~1~/ /g')
    if [ "${iss}" != "${sub}" ]; then
      echo "ERROR: $TLS_ROOT_CA is not a Root CA. Issuer and Subject should be the same"
      echo "       => File: $TLS_ROOT_CA Issuer=$sub / Subject=$iss"
      exit 1
    fi

    if [ "${iss}" != "${ROOT_CA_NAME}" ]; then
      echo "ERROR: $TLS_ROOT_CA is not Signed by $ROOT_CA_NAME."
      echo "       => File: $TLS_ROOT_CA Subject=$iss"
      exit 1
    fi

    # --- VERIFY CERTITICATE AND FULLCHAAIN SIGNED BY ROOT_CA ---
    cat $TLS_ROOT_CA $TLS_FULLCHAIN > /tmp/ca.crt
    openssl verify -CAfile /tmp/ca.crt $TLS_CERTIFICATE > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: Certificate ($TLS_CERTIFICATE) was not signed by the Roor CA ($ROOT_CA_NAME)"
      exit 1
    fi

    openssl verify -CAfile /tmp/ca.crt $TLS_FULLCHAIN > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: Certificate ($TLS_FULLCHAIN) was not signed by the Roor CA ($ROOT_CA_NAME)"
      exit 1
    fi

   # --- VERIFY IF PRIVATE KEY MATCH THE CERTIFICATE ---
    pk=$(openssl rsa  -noout -modulus -in $TLS_PRIVATE_KEY | openssl md5)
    cr=$(openssl x509 -noout -modulus -in $TLS_CERTIFICATE | openssl md5)
    fc=$(openssl x509 -noout -modulus -in $TLS_FULLCHAIN | openssl md5)

    if [ "${pk}" != "${cr}" ]; then
      echo "ERROR: Cert ($(basename $TLS_CERTIFICATE)) does not match the private key ($(basename $TLS_PRIVATE_KEY))"
      echo "       => openssl rsa -noout -modulus -in $TLS_PRIVATE_KEY | openssl md5"
      echo "       => openssl x509 -noout -modulus -in $TLS_CERTIFICATE | openssl md5"
      exit 1
    fi

    if [ "${pk}" != "${fc}" ]; then
      echo "ERROR: Cert ($(basename $TLS_FULLCHAIN)) does not match the private key ($(basename $TLS_PRIVATE_KEY))"
      echo "       => openssl rsa -noout -modulus -in $TLS_PRIVATE_KEY | openssl md5"
      echo "       => openssl x509 -noout -modulus -in $TLS_FULLCHAIN | openssl md5"
      exit 1
    fi

    messagePrint " ▪ TLS Certificate"                  "$TLS_CERTIFICATE"
    messagePrint " ▪ TLS fullchain"                    "$TLS_FULLCHAIN"
    messagePrint " ▪ TLS Private Key"                  "$TLS_PRIVATE_KEY"
    messagePrint " ▪ TLS Root CA ($ROOT_CA_NAME)"      "$TLS_ROOT_CA"
  fi
}

setDNSalias() {
  TKG_IPADRESS="$1"
  DNS_PREFIX="$2"
  DNS_SUFFIX="$3"
  HOSTNAME="$4"
  TYPE="$5"

  AWS_LB_NAME=$(aws elb --region $AWS_REGION describe-load-balancers | \
                jq -r --arg n $TKG_IPADRESS '.LoadBalancerDescriptions[] | select(.DNSName == $n).LoadBalancerName')
  AWS_LB_ZONE=$(aws elb --region $AWS_REGION describe-load-balancers | \
                jq -r --arg n $TKG_IPADRESS '.LoadBalancerDescriptions[] | select(.DNSName == $n).CanonicalHostedZoneNameID')

  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${DNS_PREFIX}.${DNS_SUFFIX} | jq -r '.HostedZones[0].Id')
  ZONE_ID_STR=$(echo "${ZONE_ID}" | awk -F'/' '{ print $NF }')

  if [ "${ZONE_ID}" != "" ]; then
    ZONE="${HOSTNAME}.${DNS_PREFIX}.${DNS_SUFFIX}"

    TMPROUTE53=/tmp/$$_tmp_route53.json
    echo "{"                                                   >  $TMPROUTE53
    echo "  \"Comment\": \"CREATE/DELETE/UPSERT a record \","  >> $TMPROUTE53
    echo "  \"Changes\": [{"                                   >> $TMPROUTE53
    echo "  \"Action\": \"UPSERT\","                           >> $TMPROUTE53
    echo "  \"ResourceRecordSet\": {"                          >> $TMPROUTE53
    echo "    \"Name\": \"${ZONE}\","                          >> $TMPROUTE53
    echo "    \"Type\": \"A\","                                >> $TMPROUTE53
    echo "    \"AliasTarget\": {"                              >> $TMPROUTE53
    echo "      \"HostedZoneId\": \"${AWS_LB_ZONE}\","         >> $TMPROUTE53
    echo "      \"DNSName\": \"dualstack.${TKG_IPADRESS}.\","  >> $TMPROUTE53
    echo "      \"EvaluateTargetHealth\": true"                >> $TMPROUTE53
    echo "    }"                                               >> $TMPROUTE53
    echo "}}]"                                                 >> $TMPROUTE53
    echo "}"                                                   >> $TMPROUTE53

    aws route53 change-resource-record-sets --hosted-zone-id $ZONE_ID \
        --change-batch file://${TMPROUTE53} > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "9ERROR: failed to set DNS for $hnm"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"  
        echo "          tdh-tools:/$ aws route53 change-resource-record-sets --hosted-zone-id \"${ZONE_ID}\" \\"
        echo "                         --change-batch file://${TMPROUTE53}"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => aws route53 change-resource-record-sets --hosted-zone-id \"${ZONE_ID}\" \\"
        echo "              --change-batch file://${TMPROUTE53}"
      fi
      cat $TMPROUTE53
      exit 1
    fi

    rm -f $TMPROUTE53
  fi
}



setDNSrecord() {
  TKG_IPADRESS="$1"
  DNS_PREFIX="$2"
  DNS_SUFFIX="$3"
  HOSTNAME="$4"
  TYPE="$5"

  if [ "$TYPE" == "" ]; then TYPE=A; fi
  
  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${DNS_PREFIX}.${DNS_SUFFIX} | jq -r '.HostedZones[0].Id')
  ZONE_ID_STR=$(echo "${ZONE_ID}" | awk -F'/' '{ print $NF }')

  if [ "${ZONE_ID}" != "" ]; then
    ZONE="${HOSTNAME}.${DNS_PREFIX}.${DNS_SUFFIX}"

    TMPROUTE53=/tmp/$$_tmp_route53.json
    echo "{"                                                            >  $TMPROUTE53
    echo "  \"Comment\": \"CREATE/DELETE/UPSERT a record \","           >> $TMPROUTE53
    echo "  \"Changes\": [{"                                            >> $TMPROUTE53
    echo "    \"Action\": \"UPSERT\","                                  >> $TMPROUTE53
    echo "    \"ResourceRecordSet\": {"                                 >> $TMPROUTE53
    echo "      \"Name\": \"${ZONE}\","                                 >> $TMPROUTE53
    echo "      \"Type\": \"$TYPE\","                                   >> $TMPROUTE53
    echo "      \"TTL\": 300,"                                          >> $TMPROUTE53
    echo "      \"ResourceRecords\": [{ \"Value\": \"$TKG_IPADRESS\"}]" >> $TMPROUTE53
    echo "    }"                                                        >> $TMPROUTE53
    echo "  }]"                                                         >> $TMPROUTE53
    echo "}"                                                            >> $TMPROUTE53

    aws route53 change-resource-record-sets --hosted-zone-id $ZONE_ID \
        --change-batch file://${TMPROUTE53} > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "3ERROR: failed to set DNS for $hnm"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"   
        echo "          tdh-tools:/$ aws route53 change-resource-record-sets --hosted-zone-id \"${ZONE_ID}\" \\"
        echo "                        --change-batch file://${TMPROUTE53}"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => aws route53 change-resource-record-sets --hosted-zone-id \"${ZONE_ID}\" \\"
        echo "              --change-batch file://${TMPROUTE53}"
      fi

      cat $TMPROUTE53
      exit 1
    fi

    rm -f $TMPROUTE53
  fi
}

listDeployments() {
  printf "%-32s %-8s  %-15s %-20s %-5s %s\n" "DEPLOYMENT" "PLATFORM" "PROFILE" "MGMT-CLUSTER" "PLAN" "CONFIGURATION"
  echo "-------------------------------------------------------------------------------------------------------------------------------------------------------"
  for deployment in $(ls -1 ${TDHPATH}/deployments/minikube*.cfg) ; do
    mcr=$(egrep "^TDH_MINIKUBE_PROFILE=" $deployment | awk -F= '{ print $NF }') 

    dep=$(basename $deployment)

    printf "%-32s %-8s  %-15s %-20s %-5s %s\n" $dep minikube $mcr "n/a" "n/a" \
           "$TDH_TKGMC_CONFIG"
  done

  echo 
  printf "%-32s %-8s  %-15s %-20s %-5s %s\n" "DEPLOYMENT" "CLOUD" "REGION" "MGMT-CLUSTER" "PLAN" "CONFIGURATION"
  echo "-------------------------------------------------------------------------------------------------------------------------------------------------------"
  for deployment in $(ls -1 ${TDHPATH}/deployments/tkgmc*.cfg) ; do

    mci=$(egrep "^TDH_TKGMC_INFRASTRUCTURE=" $deployment | awk -F= '{ print $NF }') 
    reg=$(egrep "^TDH_TKGMC_REGION=" $deployment | awk -F= '{ print $NF }') 
    mcn=$(egrep "^TDH_TKGMC_NAME=" $deployment | awk -F= '{ print $NF }') 
    mcp=$(egrep "^TDH_TKGMC_PLAN=" $deployment | awk -F= '{ print $NF }') 
    mcc=$(egrep "^TDH_TKGMC_CONFIG=" $deployment | awk -F= '{ print $NF }') 

    dep=$(basename $deployment)

    printf "%-32s %-8s  %-15s %-20s %-5s %s\n" $dep $mci $reg $mcn \
           $mcp "$mcc"

  done

  echo "-------------------------------------------------------------------------------------------------------------------------------------------------------"
}


VerifyDemoEnvironment() {
  export TDH_PLATFORM=unknown
  if [ "${TKG_DEPLOYMENT}" != "" ]; then
    if [ -f ${TDHPATH}/deployments/$TKG_DEPLOYMENT ]; then
      . ${TDHPATH}/deployments/$TKG_DEPLOYMENT
    
      DOMAIN="apps-${TDH_TKGWC_NAME}.${TDH_TKGMC_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"
    else
      echo "ERROR: can not find ${TDHPATH}/deployments/$TKG_DEPLOYMENT"; exit 1
    fi

    if [ "$TDH_MINIKUBE_PROFILE" != "" ]; then 
      cpu=$(cat ~/.minikube/profiles/tanzu-demo-hub/config.json | jq -r '.CPUs')
      mem=$(cat ~/.minikube/profiles/tanzu-demo-hub/config.json | jq -r '.Memory')
      drv=$(cat ~/.minikube/profiles/tanzu-demo-hub/config.json | jq -r '.Driver')
      export TDH_PLATFORM=minikube
      messageTitle "Demo Environment"
      messagePrint " ▪ Deployment Config:"         "$TKG_DEPLOYMENT"
      messagePrint " ▪ Kubernetes Infrastucture"   "minikube"
      messagePrint " ▪ Minikube Profile"           "$TDH_MINIKUBE_PROFILE"
      messagePrint " ▪ Minikube Domain"            "$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"
      messagePrint " ▪ Minikube Memory"            "$mem"
      messagePrint " ▪ Minikube CPU's"             "$cpu"
      messagePrint " ▪ Minikube Driver"            "$drv"
    else
      export TDH_PLATFORM=tkgm
      export WORKLOAD_CLUSTER=tkg-tanzu-demo-hub.cfg
      export TDH_TKGWC_NAME=tdh-1
      . ${TDHPATH}/deployments/$WORKLOAD_CLUSTER

      messageTitle "TKG Demo Environment"
      messagePrint " ▪ Deployment Config:"         "$TKG_DEPLOYMENT"
      messagePrint " ▪ TKG Infrastructure"         "$TDH_TKGMC_INFRASTRUCTURE"
      messagePrint " ▪ TKG Management Cluster"     "$TDH_TKGMC_NAME"
      messagePrint " ▪ TKG Workload Cluster"       "$TDH_TKGWC_NAME"
      messagePrint " ▪ DNS Domain"                 "$TDH_TKGMC_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"
      messagePrint " ▪ Kubernetes Context:"        "$TDH_TKGWC_NAME-admin@$TDH_TKGWC_NAME"
      echo ""

      if [ "${TKG_CONFIG}" == "" ]; then
        echo "ERROR: environment variable TKG_CONFIG has not been set"; exit 1
      fi

      if [ "${KUBECONFIG}" == "" ]; then
        echo "ERROR: environment variable KUBECONFIG has not been set"; exit 1
      fi
    fi

    echo -n "Is the environment setup correct ? (y/n): "; read x
    answer_provided="n"
    while [ "${answer_provided}" == "n" ]; do
      if [ "${x}" == "y" -o "${x}" == "Y" ]; then break; fi
      if [ "${x}" == "n" -o "${x}" == "N" ]; then 
        listDeployments
        echo ""
        echo "Please choose another deployment option:"
        echo "       => export TKG_DEPLOYMENT=<DEPLOYMENT>"; exit 1
      fi
      echo -n "Is the environment setup correct ? (y/n): "; read x
    done
  else
    listDeployments
    echo ""
    echo "ERROR: environment variable TKG_DEPLOYMENT not set. Pleae choose one from the list"
    echo "       => export TKG_DEPLOYMENT=<DEPLOYMENT>"; exit 1
  fi
}

VerifyTKGDemoPlatform() {
echo tkgm
}

VerifyK8SDemoPlatform() {
  if [ "${TDH_INFRASTRUCTURE}" == "minikube" ]; then 
    messageTitle "MiniKube Verify Addons"

    minikube profile $TDH_MINIKUBE_PROFILE > /dev/null 2>&1
    minikube status -p $TDH_MINIKUBE_PROFILE > /dev/null 2>&1
    if [ "$?" -ne 0 ]; then
      echo "ERROR: minikube is not running, please start it by:"
      echo "       => minikube -p $TDH_MINIKUBE_PROFILE start --driver=hyperkit --cpus $TDH_MINIKUBE_CONFIG_CPU --memory $TDH_MINIKUBE_CONFIG_MEMORY --disk-size $TDH_MINIKUBE_CONFIG_DISK"; exit 1
    fi

    ctx=$(kubectl config current-context)
    if [ "$ctx" != "$TDH_MINIKUBE_PROFILE" ]; then
      echo "ERROR: Kubernetes context is currently set to $crx. Please set it by:"
      echo "       => kubectl config use-context $TDH_MINIKUBE_PROFILE"; exit 1
    fi

    # --- CHECK FOR ADDONS ---
    for tmp in default-storageclass:default-storageclass metrics-server:metrics-server metallb-system:metallb; do
      val=$(echo $tmp | awk -F: '{ print $1 }') 
      key=$(echo $tmp | awk -F: '{ print $2 }') 
      stt=$(minikube addons list -o json | jq -r ".\"$key\".Status")
      
      if [ "$stt" != "enabled" ]; then
        messagePrint " ▪ Minikube Addon: $val" "$stt, enabling now"
        minikube addons enable $key -p $TDH_MINIKUBE_PROFILE > /dev/null 2>&1
        if [ $? -ne 0 ]; then 
           echo "ERROR: enabling Minikube Addob: $val"
           echo "       => minikube addons enable $key -p $TDH_MINIKUBE_PROFILE "
           exit 1
        fi
      else
        messagePrint " ▪ Minikube Addon: $val" "$stt"
      fi
    done
  fi
}



VerifyServiceDependancies() {
  if [ "$TDH_DEPENDANCY_CERT_MANAGER" == "true" ]; then 
    INGRESS_NAMESPACE=cert-manager
    HELM_CHART=jetstack/cert-manager

    messagePrint " ▪ Install Cert Manager" "bitnami/nginx-ingress-controller"
    helmRepoAdd bitnami https://charts.bitnami.com/bitnami
    createNamespace $INGRESS_NAMESPACE > /dev/null 2>&1

    stt=$(helm list -n $INGRESS_NAMESPACE -o json | jq -r --arg key "cert-manager-" '.[] | select(.chart | contains($key)).status')
    if [ "$stt" != "deployed" ]; then
      helm install $HELM_CHART -n $INGRESS_NAMESPACE --wait --wait-for-jobs --timeout 10m > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: Failed to install $HELM_CHART"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"    
          echo "          tdh-tools:/$ helm install stable $HELM_CHART -n $INGRESS_NAMESPACE"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => helm install stable $HELM_CHART -n $INGRESS_NAMESPACE"
        fi
      fi
    fi

  fi

  if [ "$TDH_DEPENDANCY_NGINX" == "true" ]; then 
    INGRESS_NAMESPACE=ingress-nginx
    HELM_CHART=bitnami/nginx-ingress-controller

    messagePrint " ▪ Install Ingress Controller" "$HELM_CHART"
    helmRepoAdd bitnami https://charts.bitnami.com/bitnami
    createNamespace $INGRESS_NAMESPACE > /dev/null 2>&1

    stt=$(helm list -n nginx-ingress -o json | jq -r --arg key "ngress-controller" '.[] | select(.chart | contains($key)).status')
    if [ "$stt" != "deployed" ]; then
      helm install bitnami bitnami/nginx-ingress-controller -n $INGRESS_NAMESPACE --wait --wait-for-jobs --timeout 10m > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: Failed to install $HELM_CHART"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"  
          echo "          tdh-tools:/$ helm install stable $HELM_CHART"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => helm install stable $HELM_CHART"
        fi

        exit 1
      fi
    fi

    # --- SET CLUSTER DNS ---
    ipa=$(kubectl get svc bitnami-nginx-ingress-controller -n nginx-ingress -o json 2>/dev/null | \
          jq -r '.status.loadBalancer.ingress[].ip' 2>/dev/null)
    while [ "${ipa}" == "" ]; do
      sleep 10
      ipa=$(kubectl get svc bitnami-nginx-ingress-controller -n nginx-ingress -o json 2>/dev/null | \
            jq -r '.status.loadBalancer.ingress[].ip' 2>/dev/null)
    done
  
    if [ "${ipa}" != "" ]; then
      setTKGclusterDNS "$TDH_TKGWC_NAME" "${ipa}" "nginx"
    else
      echo "ERROR: Unable to get LoadBalancer IP-Adress of the Ingress"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh" 
        echo "          tdh-tools:/$ kubectl get svc envoy -n tanzu-system-ingress"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => kubectl get svc envoy -n tanzu-system-ingress"
      fi
      exit 1
    fi
  fi

  if [ "$TDH_DEPENDANCY_CONTOUR" == "true" ]; then 
    INGRESS_NAMESPACE=ingress-contour
    HELM_CHART=bitnami/contour

    stt=$(kubectl get ns -o json | jq -r --arg key "$INGRESS_NAMESPACE" '.items[] | select(.metadata.name == $key).status.phase')
    if [ "${stt}" == "" ]; then
      createNamespace $INGRESS_NAMESPACE > /dev/null 2>&1
    fi

    helmRepoAdd bitnami https://charts.bitnami.com/bitnami

    stt=$(helm list -n $INGRESS_NAMESPACE -o json | jq -r --arg key "contour-3" '.[] | select(.chart | contains($key)).status')
    if [ "$stt" != "deployed" ]; then
      messageTitle "Install Contour Ingress Controller" "$HELM_CHART"
      helm install bitnami $HELM_CHART -n $INGRESS_NAMESPACE --wait --wait-for-jobs --timeout 10m > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: Failed to install $HELM_CHART"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ helm install stable $HELM_CHART"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => helm install stable $HELM_CHART"
        fi
      fi
    fi
  fi
}

verifyKubernetesContext() {
  echo "-------------------------------------------------------------------------------------------------------------------------------------------------------"
  kubectl config get-contexts
  echo "-------------------------------------------------------------------------------------------------------------------------------------------------------"

  ret=""
  while [ "$ret" != "y" -a "$ret" != "n" ]; do
    echo -e "Is the Kubernetes Context correct ? <y/n>: \c"; read ret
  done
  echo ""

  if [ "$ret" == "n" ]; then exit; fi
}

uodateConfigMap() {
  CONFIG_MAP="$1"
  KEY="$2"
  VALUE="$3"

  [ "$VALUE" == "" ] && VALUE="<empty>"
  [ "$VALUE" == "True" ] && VALUE=true
  [ "$VALUE" == "False" ] && VALUE=false
  
  # --- CREATE CONFIG MAP IF IT DOES NOT EXIST ---
  cmdLoop kubectl get configmap -o json > /tmp/output.json
  nam=$(jq -r --arg key "tanzu-demo-hub" '.items[].metadata | select(.name == $key).name' /tmp/output.json)
  [ "$nam" == "" ] && cmdLoop kubectl create cm tanzu-demo-hub > /dev/null 2>&1

  #DELETEME
  #kubectl get cm tanzu-demo-hub > /dev/null 2>&1
  #[ $? -ne 0 ] && kubectl create cm tanzu-demo-hub > /dev/null 2>&1


  # --- DUMP CURRENT CONFIGMAP CONFIG ---
  cmdLoop kubectl get cm tanzu-demo-hub -o json > /tmp/output.json 
  jq -r '.data' /tmp/output.json  | egrep ": " | \
          sed -e 's/^ *"//g' -e 's/: /=/g' -e 's/"=/=/g' -e 's/\\\"//g' -e 's/,$//g' -e 's/"//g' | \
          egrep -v "^$KEY=" | sort -u > /tmp/$CONFIG_MAP
  echo "$KEY=$VALUE" >> /tmp/$CONFIG_MAP

  #DELETEME
  #kubectl get cm tanzu-demo-hub -o json | jq -r '.data' | egrep ": " | sed -e 's/^ *"//g' -e 's/: /=/g' -e 's/"=/=/g' -e 's/\\\"//g' -e 's/,$//g' -e 's/"//g' | \
  #        egrep -v "^$KEY=" | sort -u > /tmp/$CONFIG_MAP
  #echo "$KEY=$VALUE" >> /tmp/$CONFIG_MAP

  cmdLoop kubectl create configmap $CONFIG_MAP --from-env-file=/tmp/$CONFIG_MAP --dry-run=client -o yaml > /tmp/output.json
  cmdLoop kubectl apply -f /tmp/output.json > /dev/null 2>&1

  #DELETEME
  #kubectl create configmap $CONFIG_MAP --from-env-file=/tmp/$CONFIG_MAP --dry-run=client -o yaml | kubectl apply -f - > /dev/null 2>&1
}

getConfigMap() {
  CONFIG_MAP=$1
  KEY=$2

  cmdLoop kubectl get configmap tanzu-demo-hub -o json > /tmp/output.json
  jq -r ".data.$KEY" /tmp/output.json | sed 's/"//g'

  #DELETEME
  #kubectl get configmap tanzu-demo-hub -o json | jq -r ".data.$KEY" | sed 's/"//g'
}

createConfigMap() {
  cmdLoop kubectl get configmap -o json > /tmp/output.json
  nam=$(jq -r --arg key "tanzu-demo-hub" '.items[].metadata | select(.name == $key).name' /tmp/output.json)
  [ "$nam" == "tanzu-demo-hub" ] && cmdLoop kubectl delete configmap tanzu-demo-hub > /dev/null 2>&1

  cmdLoop kubectl create configmap tanzu-demo-hub \
       --from-literal=TDH_ENVNAME=$TDH_ENVNAME \
       --from-literal=TDH_DOMAIN=$AWS_HOSTED_DNS_DOMAIN \
       --from-literal=TDH_LB_CONTOUR=apps-contour \
       --from-literal=TDH_LB_NGINX=apps-nginx 
}

verifyRequiredServices() {
  flag=$1
  desc="$2"

  stt=$(getConfigMap tanzu-demo-hub $flag)
  if [ "$stt" != "true" ]; then 
    echo "ERROR: Service $desc requires to be enabled and installed"
    echo "       Please set $flag=true in the deployment file"
    exit 1
  fi
}

checkTKGdownloads() {
  kpl=$(ls -1 software | egrep -c "^kp-linux|photon")
  ptl=$(ls -1 software | egrep -c "^photon")
  tkg=$(ls -1 software | egrep -c "^tanzu-cli-bundle-linux")
  if [ $tkg -eq 0 ]; then 
    echo "ERROR: Please download TKG Utilies from https://www.vmware.com/go/get-tkg"
    echo "       => $TANZU_DEMO_HUB/software/tkg-linux-amd64-vx.x.x-vmware.x.tar.gz"
    echo "       => $TANZU_DEMO_HUB/software/tkg-linux-amd64-vx.x.x-vmware.x.tar.gz"
    exit 1
  fi
}

checkCLIcommands() {
  [ -f /tdh_tools_docker_container  ] && return

  cat="$1"
  if [ "$cat" == "TKG" ]; then
    if [ ! -x /usr/local/bin/tkg ]; then
      echo "ERROR: /usr/local/bin/tkg not installed, please download package from https://www.vmware.com/go/get-tkg"
      echo "       => search for VMware Tanzu Kubernetes Grid CLI for Mac"
      exit 1
    fi

    if [ ! -x /usr/local/bin/kp ]; then
      echo "ERROR: /usr/local/bin/kp not installed, please download package from https://network.pivotal.io/products/build-service/"
      echo "       => search for kp-darwin for Mac"
      exit 1
    fi
  fi

  if [ "$cat" == "VSPHERE" ]; then
    if [ "$(uname)" == "Linux" ]; then
      CLI_COMMANDS="/usr/local/bin/govc:brew:govc /usr/local/bin/packer:brew:packer"
    else
      if [ "$(uname -p)" == "arm" ]; then
        CLI_COMMANDS="/opt/homebrew/bin/govc:brew:govc /opt/homebrew/bin/packer:brew:packer"
      else
        CLI_COMMANDS="/usr/local/bin/govc:brew:govc /usr/local/bin/packer:brew:packer"
      fi
    fi

    # --- INSTALL BREW INSTALLABLE VSPHERE ---
    for arg in $CLI_COMMANDS; do
      cmd=$(echo $arg | awk -F: '{ print $1 }')
      sys=$(echo $arg | awk -F: '{ print $2 }')
      pkg=$(echo $arg | awk -F: '{ print $3 }')

      if [ $sys == "brew" ]; then
        if [ ! -x $cmd ]; then
          if [ "$(uname)" == "Linux" ]; then
            echo "ERROR: $cmd not installed, please install with apt"
            echo "       => sudo apt install $pkg -y"
          else
            echo "ERROR: $cmd not installed, please install with brew"
            echo "       => brew install $pkg"
          fi
          exit 1
        fi
      fi
    done
  fi

  if [ "$cat" == "TANZU_DATA" ]; then
    if [ "$(uname)" == "Linux" ]; then
      CLI_COMMANDS="/usr/local/bin/mc:apt:minio/stable/mc /usr/local/bin/psql:apt:libpq /usr/local/bin/s3cmd:brew:s3cmd"
    else
      if [ "$(uname -p)" == "arm" ]; then
        CLI_COMMANDS="/opt/homebrew/bin/mc:brew:minio/stable/mc /opt/homebrew/bin/psql:brew:libpq /opt/homebrew/bin/s3cmd:brew:s3cmd"
      else
        CLI_COMMANDS="/usr/local/bin/mc:brew:minio/stable/mc /usr/local/bin/psql:brew:libpq /usr/local/bin/s3cmd:brew:s3cmd"
      fi
    fi

    # --- INSTALL BREW INSTALLABLE TOOLS ---
    for arg in $CLI_COMMANDS; do
      cmd=$(echo $arg | awk -F: '{ print $1 }')
      sys=$(echo $arg | awk -F: '{ print $2 }')
      pkg=$(echo $arg | awk -F: '{ print $3 }')

      if [ $sys == "brew" ]; then
        if [ ! -x $cmd ]; then
          if [ "$(uname)" == "Linux" ]; then
            echo "ERROR: $cmd not installed, please install with apt"
            echo "       => sudo apt install $pkg -y"
          else
            echo "ERROR: $cmd not installed, please install with brew"
            echo "       => brew install $pkg"
          fi
          exit 1
        fi
      fi
    done
  fi

  if [ "$cat" == "DEMO_TOOLS" ]; then
    if [ "$(uname)" == "Linux" ]; then
      CLI_COMMANDS="/usr/local/bin/gsed:brew:gnu-sed /usr/local/bin/bat:brew:bat"
    else
      if [ "$(uname -p)" == "arm" ]; then
        CLI_COMMANDS="/opt/homebrew/bin/hsed:brew:gnu-sed /opt/homebrew/bin/bat:brew:bat"
      else
        CLI_COMMANDS="/usr/local/bin/hsed:brew:gnu-sed /usr/local/bin/bat:brew:bat"
      fi
    fi

    # --- INSTALL BREW INSTALLABLE TOOLS ---
    for arg in $CLI_COMMANDS; do
      cmd=$(echo $arg | awk -F: '{ print $1 }')
      sys=$(echo $arg | awk -F: '{ print $2 }')
      pkg=$(echo $arg | awk -F: '{ print $3 }')

      if [ $sys == "brew" ]; then
        if [ ! -x $cmd ]; then
          if [ "$(uname)" == "Linux" ]; then
            echo "ERROR: $cmd not installed, please install with brew"
            echo "       => sudo apt install $pkg -y"
          else
            echo "ERROR: $cmd not installed, please install with brew"
            echo "       => brew install $pkg"
          fi
          exit 1
        fi
      fi
    done
  fi

  if [ "$cat" == "BASIC" ]; then
    if [ "$(uname)" == "Linux" ]; then
      CLI_COMMANDS="/usr/bin/jq:apt:jq /usr/bin/wget:apt:wget /usr/bin/unzip:apt:unzip /usr/bin/kubectl:apt:kubectl"
    else
      if [ "$(uname -p)" == "arm" ]; then
        CLI_COMMANDS="/opt/homebrew/bin/jq:brew:jq /opt/homebrew/bin/wget:brew:wget /opt/homebrew/bin/kubectl:brew:kubectl"
        cnt=$(echo $PATH | grep -c "/opt/homebrew/bin") 
        if [ $cnt -eq 0 ]; then 
          echo "ERROR: Software path (/opt/homebrew/bin) not set in your PATH environment variable, please set"
          echo "       it in your $HOME/.bashrc as folows"
          echo "       => export PATH=$PATH:/opt/homebrew/bin"
          exit 1
        fi
      else
        CLI_COMMANDS="/usr/local/bin/jq:brew:jq /usr/local/bin/wget:brew:wget /usr/local/bin/kubectl:brew:kubectl"
        cnt=$(echo $PATH | grep -c "/usr/local/bin") 
        if [ $cnt -eq 0 ]; then 
          echo "ERROR: Software path (/usr/local/bin) not set in your PATH environment variable, please set"
          echo "       it in your $HOME/.bashrc as folows"
          echo "       => export PATH=$PATH:/usr/local/bin"
          exit 1
        fi
      fi
    fi

    # --- INSTALL BREW INSTALLABLE TOOLS ---
    for arg in $CLI_COMMANDS; do
      cmd=$(echo $arg | awk -F: '{ print $1 }')
      sys=$(echo $arg | awk -F: '{ print $2 }')
      pkg=$(echo $arg | awk -F: '{ print $3 }')

      if [ ! -x $cmd -a ! -x /usr/local/bin/$pkg ]; then
	if [ "$(uname)" == "Linux" ]; then
	  if [ $sys == "apt" ]; then
            echo "ERROR: $cmd not installed, please install it"
            echo "       => sudo apt install $pkg -y"
	    exit 1
	  fi

	  if [ $sys == "snap" ]; then
            echo "ERROR: $cmd not installed, please install it"
            echo "       => sudo snap install $pkg"
            exit 1
          fi
	else
	  if [ $sys == "brew" ]; then
	    echo "ERROR: $cmd not installed, please install it"
	    echo "       => brew install $pkg"
	    exit 1
          fi
	fi
      fi
    done
  fi

  if [ "$cat" == "TOOLS" ]; then
    if [ "$(uname)" == "Linux" ]; then
      CLI_COMMANDS="/usr/local/bin/mc:brew:minio/stable/mc /usr/local/bin/helm:brew:helm /usr/local/bin/npm:brew:npm"
    else
      if [ "$(uname -p)" == "arm" ]; then
        CLI_COMMANDS="/opt/homebrew/bin/mc:brew:minio/stable/mc /opt/homebrew/bin/helm:brew:helm /opt/homebrew/bin/npm:brew:npm"
      else
        CLI_COMMANDS="/usr/local/bin/mc:brew:minio/stable/mc /usr/local/bin/helm:brew:helm /usr/local/bin/npm:brew:npm"
      fi
    fi

    # --- INSTALL BREW INSTALLABLE TOOLS ---
    for arg in $CLI_COMMANDS; do
      cmd=$(echo $arg | awk -F: '{ print $1 }') 
      sys=$(echo $arg | awk -F: '{ print $2 }') 
      pkg=$(echo $arg | awk -F: '{ print $3 }') 
  
      if [ $sys == "brew" ]; then
        if [ ! -x $cmd ]; then
          if [ "$(uname)" == "Linux" ]; then
            echo "ERROR: $cmd not installed, please install it"
            echo "       => sudo apt install $pkg -y"
          else
            echo "ERROR: $cmd not installed, please install with brew"
            echo "       => brew install $pkg"
          fi
          exit 1
        fi
      fi
    done
  fi

  if [ "$cat" == "TANZU" ]; then
    if [ ! -f /usr/local/bin/vmw-cli ]; then 
      npm install vmw-cli --global > /dev/null 2>&1
      if [ $? -ne 0 ]; then 
        echo "ERROR: failed to install vmw-cli, please try manually"
        echo "       => npm install vmw-cli --global"
        exit 1
      fi
    fi

    export VMWUSER="$TDH_MYVMWARE_USER"
    export VMWPASS="$TDH_MYVMWARE_PASS"
    cnt=$(vmw-cli ls vmware_tanzu_kubernetes_grid 2>&1 | grep -c "ERROR")
    if [ $cnt -ne 0 ]; then
      echo "ERROR: failed to login to vmw-cli, please make sure that the environment variables"
      echo "       TDH_MYVMWARE_USER and TDH_MYVMWARE_PASS are set correctly. Please try manually"
      echo "       -------------------------------------------------------------------------------------------------------------------------------------------------------"
      echo "       => . ~/.tanzu-demo-hub.cfg"
      echo "       => export VMWUSER=\$TDH_MYVMWARE_USER"
      echo "       => export VMWPASS=\$TDH_MYVMWARE_PASS"
      echo "       => vmw-cli ls vmware_tanzu_kubernetes_grid"
      echo "       -------------------------------------------------------------------------------------------------------------------------------------------------------"
      exit 1
    fi

    messageTitle "Verify Software Downloads from http://my.vmware.com"

    if [ ! -f /usr/local/bin/tanzu ]; then 
      arch=$(uname)
      if [ "$arch" == "Darwin" ]; then 
        vmwfile=$(vmw-cli ls vmware_tanzu_kubernetes_grid 2>/dev/null | egrep "^tanzu-cli-bundle-darwin" | tail -1 | awk '{ print $1 }') 
        if [ ! -f /tmp/$vmwfile ]; then 
          messagePrint " ▪ Download Tanzu CLI:"                        "$vmwfile"
          (cd /tmp/; vmw-cli cp $vmwfile > /dev/null 2>&1)
          if [ ! -f /tmp/$vmwfile ]; then 
            echo "ERROR: failed to download $vmwfile from http://my.vmware.com, please try manually"
            echo "       -------------------------------------------------------------------------------------------------------------------------------------------------------"
            echo "       => . ~/.tanzu-demo-hub.cfg"
            echo "       => export VMWUSER=\$TDH_MYVMWARE_USER"
            echo "       => export VMWPASS=\$TDH_MYVMWARE_PASS"
            echo "       => vmw-cli ls vmware_tanzu_kubernetes_grid"
            echo "       => vmw-cli cp $vmwfile"
            echo "       -------------------------------------------------------------------------------------------------------------------------------------------------------"
            exit 1
          fi
        else
          echo "INFO: The tanzu CLI utility has been downloaded to /tmp/$vmwfile, please folow the instructions described here to install them:"
          echo "      https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.3/vmware-tanzu-kubernetes-grid-13/GUID-install-cli.html"
          exit 1
        fi
      fi
    else
      ver=$(tanzu version | egrep "^version:" | awk '{ print $2 }')
      messagePrint " ▪ Tanzu CLI"                        "$ver"

      # --- VERIFY PLUGINS ---
      plg=$(tanzu plugin list | grep -v "alpha" | grep -c "not installed")
      if [ $plg -ne 0 ]; then 
        messagePrint " ▪ Tanzu CLI Plugins"                        "not installed"
        echo "----------------------------------------------------------------------------------------------------------------------------------------------"
        tanzu plugin list
        echo "----------------------------------------------------------------------------------------------------------------------------------------------"
        echo "INFO: The tanzu CLI plugins are not installed please folow the instructions described here to install them:"
        echo "      https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.3/vmware-tanzu-kubernetes-grid-13/GUID-install-cli.html"
        exit 1
      else
        messagePrint " ▪ Tanzu CLI Plugins"                        "installed"
        echo "----------------------------------------------------------------------------------------------------------------------------------------------"
        tanzu plugin list
        echo "----------------------------------------------------------------------------------------------------------------------------------------------"
      fi
    fi
  fi
}

checkTMCAccess() {
  missing_variables=0

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "vSphere" ]; then
    TMC_ACCOUNT_NAME=$TMC_ACCOUNT_NAME_AWS
    [ "$TDH_CLUSTER_NAME" == "" ] && TDH_CLUSTER_NAME="tdh-vsphere-${TDH_USER}"

    if [ "$TDH_SERVICE_TANZU_MISSION_CONTROL" == "true" ]; then 
     tmc clustergroup create -n tanzu-demo-hub -d "Tanzu Demo Hub by Sacha Dubois" > /dev/null 2>&1
    fi
  fi

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "Azure" ]; then
    TMC_ACCOUNT_NAME=$TMC_ACCOUNT_NAME_AWS
    [ "$TDH_CLUSTER_NAME" == "" ] && TDH_CLUSTER_NAME="tdh-azure-${TDH_USER}"

    if [ "$TDH_SERVICE_TANZU_MISSION_CONTROL" == "true" ]; then
     tmc clustergroup create -n tanzu-demo-hub -d "Tanzu Demo Hub by Sacha Dubois" > /dev/null 2>&1
    fi
  fi

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "AWS" ]; then
    TMC_ACCOUNT_NAME=$TMC_ACCOUNT_NAME_AWS

    if [ "${TDH_MANAGEMENT_CLUSTER}" == "" -o "${TDH_PROVISIONER_NAME}" == "" -o "${TMC_ACCOUNT_NAME_AWS}" == "" -o "${TDH_CLUSTER_GROUP}" == "" ]; then
      missing_variables=1
      echo ""
      echo "  5MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
      echo "  --------------------------------------------------------------------------------------------------------------"

      if [ "${TDH_CLUSTER_GROUP}" == "" ]; then
        echo "  TDH_CLUSTER_GROUP               (required) The name of the TMC Cluster Group"
      fi

      if [ "${TMC_ACCOUNT_NAME_AWS}" == "" ]; then
        echo "  TMC_ACCOUNT_NAME_AWS            (required) The name of the TMC Account Name"
      fi

      if [ "${TDH_MANAGEMENT_CLUSTER}" == "" ]; then
        echo "  TDH_MANAGEMENT_CLUSTER          (required) The name of the TKGm Management Cluster"
      fi

      if [ "${TDH_PROVISIONER_NAME}" == "" ]; then
        echo "  TDH_PROVISIONER_NAME            (required) The TMC Provisioner name"
      fi

      if [ "${TMC_SSH_KEY_NAME_AWS}" == "" ]; then
        echo "  TMC_SSH_KEY_NAME_AWS            (required) AWS SSH Key name for Region"
      fi

      [ "$TDH_CLUSTER_NAME" == "" ] && TDH_CLUSTER_NAME="tdh-aws-${TDH_USER}"
    else
      [ "$TDH_CLUSTER_NAME" == "" ] && TDH_CLUSTER_NAME="tdh-aws-${TDH_USER}"

      messageTitle "Tanzu Mission Control (TMC) - Config" 
      messagePrint " ▪ TMC User"                        "$TDH_USER"
      messagePrint " ▪ TMC Cluster Group"               "$TDH_CLUSTER_GROUP"
      messagePrint " ▪ TMC Cluster Name"                "$TDH_CLUSTER_NAME"
      messagePrint " ▪ TMC Context Name"                "$TMC_CONTEXT_NAME"
      messagePrint " ▪ TMC AWS Account Name"            "$TMC_ACCOUNT_NAME_AWS"
      messagePrint " ▪ TMC Management Cluster"          "$TDH_MANAGEMENT_CLUSTER"
      messagePrint " ▪ TMC Provisioner Name"            "$TDH_PROVISIONER_NAME"

      # --- CHECK IF AWS CLI IS CONFIGURED ---
      [ ! -d $HOME/.aws ] && mkdir -p $HOME/.aws
      if [ ! -f $HOME/.aws/credentials ]; then
        if [ "$AWS_ACCESS_KEY" != "" -a "$AWS_SECRET_KEY" != "" ]; then
          echo "[default]"                               >  $HOME/.aws/credentials
          echo "aws_access_key_id = $AWS_ACCESS_KEY"     >> $HOME/.aws/credentials
          echo "aws_secret_access_key = $AWS_SECRET_KEY" >> $HOME/.aws/credentials
        else
          echo "ERROR: AWS CLI is not configured yet, please run aws configure"
          if [ "$NATIVE" == "0" ]; then
            echo "       => tools/${TDH_TOOLS}.sh"
            echo "          tdh-tools:/$ aws configure"
            echo "          tdh-tools:/$ exit"
          else
            echo "       => aws configure"
          fi
        fi
        exit 1
      fi
    fi


    if [ "$TDH_SERVICE_TANZU_MISSION_CONTROL" == "true" ]; then 
     tmc clustergroup create -n tanzu-demo-hub -d "Tanzu Demo Hub by Sacha Dubois" > /dev/null 2>&1
    fi
  fi

  if [ ${missing_variables} -eq 1 ]; then
    echo "  --------------------------------------------------------------------------------------------------------------"
    echo "  IMPORTANT: Please set the missing environment variables either in your shell or in the tanzu-demo-hub"
    echo "             configuration file ~/.tanzu-demo-hub.cfg and set all variables with the 'export' notation"
    echo "             ie. => export <environment_variable>=\"<value>\""
    echo "  --------------------------------------------------------------------------------------------------------------"
    exit 1
  fi
}

checkTMCcontext() {
  tmc_context=$(tmc system context current -o json 2>/dev/null | jq -r '.full_name.name')
  if [ "$tmc_context" == "" -o "$tmc_context" != "$TMC_CONTEXT_NAME" ]; then 
    tmc system context use $TMC_CONTEXT_NAME > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: Can not set TMC Context to $TMC_CONTEXT_NAME"
      echo "       => tmc system context use $TMC_CONTEXT_NAME"
      exit 1
    fi 
  fi 
}

# ------------------------------------------------------------------------------------------
# Function Name ......: tkgCreateCluster
# Function Purpose ...: Install a Tanzu Kubernetes Grid Workload Cluster
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: None
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
# References: 
# https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/
# ------------------------------------------------------------------------------------------
tkgCreateCluster() {
  [ "$TDH_DEPLOYMENT_CLOUD" == "" ] && TDH_DEPLOYMENT_CLOUD=$TDH_TKGMC_INFRASTRUCTURE

  #########################################################################################################################
  ################################################ INSTALL TKG ON AZURE ###################################################
  #########################################################################################################################
  if [ "$TDH_DEPLOYMENT_CLOUD" == "AWS" ]; then
    [ "$TDH_DEPLOYMENT_CLUSTER_PLAN" == "dev" ] && DEP_SOURCE=$HOME/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.yaml
    [ "$TDH_DEPLOYMENT_CLUSTER_PLAN" == "prod" ] && DEP_SOURCE=$HOME/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.yaml

    # --- GENERATE WORKLOAD CLUSTER CONFIG FILE ---
    CLUSTER_CONFIG_FILE=$HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.yaml

    a=1
  fi

  #########################################################################################################################
  ################################################ INSTALL TKG ON AZURE ###################################################
  #########################################################################################################################
  if [ "$TDH_DEPLOYMENT_CLOUD" == "docker" ]; then
    messageTitle "Tanzu Kubernetes Grid Deployment"
    messagePrint " ▪ Management Cluster Name"               "$TDH_TKGMC_NAME"
    messagePrint " ▪ Management Cluster Configuration"      "$HOME/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.cfg"
    messagePrint " ▪ Management Cluster Kubeconfig"         "$HOME/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.kubeconfig"
    messagePrint " ▪ Workload Cluster Name"                 "$TDH_TKGWC_NAME"
    messagePrint " ▪ Workload Cluster Cloud Config"         "$HOME/.tanzu/tkg/clusterconfigs/$TDH_TKGMC_CONFIG"
    messagePrint " ▪ Workload Cluster Configuration"        "deployments/$TKG_DEPLOYMENT"
    messagePrint " ▪ Workload Cluster consolidated Config"  "$HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.yaml"

    # --- WRITE CLUSTER CONFIG ---
    echo "TDH_TKGMC_NAME=$TDH_TKGMC_NAME"                                                       >  $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.cfg
    echo "TDH_TKGWC_K8S_VERSION=$TDH_TKGWC_KUBERNETES"                                          >> $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.cfg
    echo "TDH_TKGWC_K8S_IMAGE=$TDH_TKGWC_K8S_VERSION"                                           >> $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.cfg
    echo "TDH_TKGMC_CONFIG=\$HOME/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.cfg"                 >> $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.cfg
    echo "TDH_TKGMC_KUBECONFIG=\$HOME/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.kubeconfig"      >> $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.cfg


    [ "$TDH_DEPLOYMENT_CLUSTER_PLAN" == "" ] && TDH_DEPLOYMENT_CLUSTER_PLAN="dev"
    [ "$TDH_DEPLOYMENT_CLUSTER_PLAN" == "dev" ] && DEP_SOURCE=$HOME/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.yaml
    [ "$TDH_DEPLOYMENT_CLUSTER_PLAN" == "prod" ] && DEP_SOURCE=$HOME/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.yaml

    messagePrint " ▪ Kubernetes Version"               "$TDH_TKGWC_KUBERNETES"
    messagePrint " ▪ Kubernetes Image"                 "$TDH_TKGWC_K8S_VERSION"

    # --- GENERATE WORKLOAD CLUSTER CONFIG FILE ---
    CLUSTER_CONFIG_FILE=$HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.yaml

    str="AZURE_SUBSCRIPTION_ID|AZURE_TENANT_ID|AZURE_CLIENT_ID|AZURE_CLIENT_SECRET|AZURE_LOCATION|AZURE_SSH_PUBLIC_KEY_B64|CLUSTER_PLAN"
    egrep "$str" $DEP_SOURCE                                                        >  $CLUSTER_CONFIG_FILE
    echo "CLUSTER_NAME: $TDH_TKGWC_NAME"                                            >> $CLUSTER_CONFIG_FILE
    echo "WORKER_MACHINE_COUNT: $TDH_TKGWC_WORKERNODES"                             >> $CLUSTER_CONFIG_FILE
    echo "CNI: antrea"                                                              >> $CLUSTER_CONFIG_FILE
    echo "ENABLE_MHC: true"                                                         >> $CLUSTER_CONFIG_FILE
    echo "MHC_UNKNOWN_STATUS_TIMEOUT: 10m"                                          >> $CLUSTER_CONFIG_FILE
    echo "MHC_FALSE_STATUS_TIMEOUT: 20m"                                            >> $CLUSTER_CONFIG_FILE

    [ "${TDH_TKGWC_CNI}" == "" ] && TDH_TKGWC_CNI=calico
    if [ "${TDH_TKGWC_CNI}" == "antrea" ]; then
      echo "CNI: $TDH_TKGWC_CNI"                                                    >> $CLUSTER_CONFIG_FILE
      echo "ANTREA_NO_SNAT: false"                                                  >> $CLUSTER_CONFIG_FILE
      echo "ANTREA_TRAFFIC_ENCAP_MODE: \"encap\""                                   >> $CLUSTER_CONFIG_FILE
      echo "ANTREA_PROXY: false"                                                    >> $CLUSTER_CONFIG_FILE
      echo "ANTREA_POLICY: true"                                                    >> $CLUSTER_CONFIG_FILE
      echo "ANTREA_TRACEFLOW: false"                                                >> $CLUSTER_CONFIG_FILE
    else
      echo "CNI: $TDH_TKGWC_CNI"                                                    >> $CLUSTER_CONFIG_FILE
    fi

    if [ $DEBUG -gt 0 ]; then
      echo "-----------------------------------------------------------------------------------------------------------"
      cat $HOME/.tanzu/tkg/clusterconfigs/$TDH_TKGMC_CONFIG deployments/$TKG_DEPLOYMENT | egrep -v "^#" | sed -e '/^$/d'
    fi

#hallo docker

    cmdLoop tanzu cluster list -o json > /tmp/output.json
    stt=$(jq -r --arg key "$TDH_TKGWC_NAME" '.[] | select(.name == $key).status' /tmp/output.json)
    if [ "$stt" != "running" ]; then
      echo "-----------------------------------------------------------------------------------------------------------"
      echo "=> tanzu cluster create $TDH_TKGWC_NAME -f $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.yaml --tkr $TDH_TKGWC_K8S_VERSION"
      tanzu cluster create $TDH_TKGWC_NAME -f $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.yaml --tkr $TDH_TKGWC_K8S_VERSION
      echo "-----------------------------------------------------------------------------------------------------------"

      #dockerPullImages="projects.registry.vmware.com/tkg/antrea/antrea-debian:v0.13.3_vmware.1 \
      #                  projects.registry.vmware.com/tkg/tanzu_core/capabilities/capabilities-controller-manager:v1.4.1"
    fi

    # --- GET CREDENTIALS AND SET ENVIRONMENT ---
    cmdLoop tanzu cluster kubeconfig get $TDH_TKGWC_NAME --admin > /dev/null 2>&1

    # --- CREATE GLOBAL KUBECONFIG FILE ---
    mv $HOME/.kube/config $HOME/.kube/config.bak
    cmdLoop tanzu cluster kubeconfig get $TDH_TKGWC_NAME --admin > /dev/null 2>&1
    mv $HOME/.kube/config $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.kubeconfig
    mv $HOME/.kube/config.bak $HOME/.kube/config
  fi

  #########################################################################################################################
  ################################################ INSTALL TKG ON AZURE ###################################################
  #########################################################################################################################
  if [ "$TDH_DEPLOYMENT_CLOUD" == "Azure" ]; then
    messageTitle "Tanzu Kubernetes Grid Deployment"
    messagePrint " ▪ Management Cluster Name"               "$TDH_TKGMC_NAME"
    messagePrint " ▪ Management Cluster Configuration"      "$HOME/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.cfg"
    messagePrint " ▪ Management Cluster Kubeconfig"         "$HOME/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.kubeconfig"
    messagePrint " ▪ Workload Cluster Name"                 "$TDH_TKGWC_NAME"
    messagePrint " ▪ Workload Cluster Cloud Config"         "$HOME/.tanzu/tkg/clusterconfigs/$TDH_TKGMC_CONFIG"
    messagePrint " ▪ Workload Cluster Configuration"        "deployments/$TKG_DEPLOYMENT"
    messagePrint " ▪ Workload Cluster consolidated Config"  "$HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.yaml"

    [ "$TDH_DEPLOYMENT_CLUSTER_PLAN" == "dev" ] && DEP_SOURCE=$HOME/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.yaml
    [ "$TDH_DEPLOYMENT_CLUSTER_PLAN" == "prod" ] && DEP_SOURCE=$HOME/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.yaml

    # --- GET NODE TYPES ---
    TDH_TKGWC_WORKERNODES_MACHINE=$(nodeType $TDH_TKGWC_WORKERNODES_TYPE)
    TDH_TKGWC_CONTROPLANE_MACHINE=$(nodeType $TDH_TKGWC_CONTROPLANE_NODETYPE)

    # --- GENERATE WORKLOAD CLUSTER CONFIG FILE ---
    CLUSTER_CONFIG_FILE=$HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.yaml

    # --- SET DEFAULT VALUES ---
    [ "$TDH_TKGWC_CONTROPLANE_MACHINE" == "" ] && TDH_TKGWC_CONTROPLANE_MACHINE=Standard_D4s_v3
    [ "$TDH_TKGWC_WORKERNODES_MACHINE" == "" ] && TDH_TKGWC_WORKERNODES_MACHINE=Standard_D4s_v3

    # --- GET KUBERNETES VERSION ---
    TDH_TKGWC_K8S_VERSION=$(tanzu kubernetes-release get | egrep "$TDH_TKGWC_KUBERNETES" | tail -1 | awk '{ print $1 }')

    messagePrint " ▪ Kubernetes Version"               "$TDH_TKGWC_KUBERNETES"
    messagePrint " ▪ Kubernetes Image"                 "$TDH_TKGWC_K8S_VERSION"
    messagePrint " ▪ TKG Cluster Worker Nodes"         "$TDH_TKGWC_WORKERNODES"
    messagePrint " ▪ TKG Cluster Worker Node Type"     "$TDH_TKGWC_WORKERNODES_TYPE ($TDH_TKGWC_WORKERNODES_MACHINE)"
    messagePrint " ▪ TKG Control Plane Nodes"          "$TDH_TKGWC_CONTROPLANE"
    messagePrint " ▪ TKG Control Plane Nodes Type"     "$TDH_TKGWC_CONTROPLANE_NODETYPE ($TDH_TKGWC_CONTROPLANE_MACHINE)"

    str="AZURE_SUBSCRIPTION_ID|AZURE_TENANT_ID|AZURE_CLIENT_ID|AZURE_CLIENT_SECRET|AZURE_LOCATION|AZURE_SSH_PUBLIC_KEY_B64|CLUSTER_PLAN"
    egrep "$str" $DEP_SOURCE                                                        >  $CLUSTER_CONFIG_FILE

    echo "CLUSTER_NAME: $TDH_TKGWC_NAME"                                            >> $CLUSTER_CONFIG_FILE
    echo "WORKER_MACHINE_COUNT: $TDH_TKGWC_WORKERNODES"                             >> $CLUSTER_CONFIG_FILE
    echo "CNI: antrea"                                                              >> $CLUSTER_CONFIG_FILE
    echo "AZURE_CONTROL_PLANE_MACHINE_TYPE: $TDH_TKGWC_CONTROPLANE_MACHINE"         >> $CLUSTER_CONFIG_FILE
    echo "AZURE_NODE_MACHINE_TYPE: $TDH_TKGWC_WORKERNODES_MACHINE"                  >> $CLUSTER_CONFIG_FILE
    echo "AZURE_RESOURCE_GROUP: $TDH_TKGWC_NAME"                                    >> $CLUSTER_CONFIG_FILE
    echo "AZURE_VNET_RESOURCE_GROUP: $TDH_TKGWC_NAME"                               >> $CLUSTER_CONFIG_FILE
    echo "AZURE_VNET_NAME: ${TDH_TKGWC_NAME}-vnet"                                  >> $CLUSTER_CONFIG_FILE
    echo "ENABLE_MHC: true"                                                         >> $CLUSTER_CONFIG_FILE
    echo "MHC_UNKNOWN_STATUS_TIMEOUT: 10m"                                          >> $CLUSTER_CONFIG_FILE
    echo "MHC_FALSE_STATUS_TIMEOUT: 20m"                                            >> $CLUSTER_CONFIG_FILE

    [ "${TDH_TKGWC_CNI}" == "" ] && TDH_TKGWC_CNI=calico
    if [ "${TDH_TKGWC_CNI}" == "antrea" ]; then
      echo "CNI: $TDH_TKGWC_CNI"                                                    >> $CLUSTER_CONFIG_FILE
      echo "ANTREA_NO_SNAT: false"                                                  >> $CLUSTER_CONFIG_FILE
      echo "ANTREA_TRAFFIC_ENCAP_MODE: \"encap\""                                   >> $CLUSTER_CONFIG_FILE
      echo "ANTREA_PROXY: false"                                                    >> $CLUSTER_CONFIG_FILE
      echo "ANTREA_POLICY: true"                                                    >> $CLUSTER_CONFIG_FILE
      echo "ANTREA_TRACEFLOW: false"                                                >> $CLUSTER_CONFIG_FILE
    else
      echo "CNI: $TDH_TKGWC_CNI"                                                    >> $CLUSTER_CONFIG_FILE
    fi

    if [ $DEBUG -gt 0 ]; then
      echo "-----------------------------------------------------------------------------------------------------------"
      cat $HOME/.tanzu/tkg/clusterconfigs/$TDH_TKGMC_CONFIG deployments/$TKG_DEPLOYMENT | egrep -v "^#" | sed -e '/^$/d'
    fi

#hallo azure
    cmdLoop tanzu cluster list -o json > /tmp/output.json
    stt=$(jq -r --arg key "$TDH_TKGWC_NAME" '.[] | select(.name == $key).status' /tmp/output.json)
    if [ "$stt" != "running" ]; then
      echo "-----------------------------------------------------------------------------------------------------------"
      echo "=> tanzu cluster create $TDH_TKGWC_NAME -f $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.yaml --tkr $TDH_TKGWC_K8S_VERSION"
      tanzu cluster create $TDH_TKGWC_NAME -f $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.yaml --tkr $TDH_TKGWC_K8S_VERSION; ret=$?
      echo "-----------------------------------------------------------------------------------------------------------"

      if [ $ret -ne 0 ]; then
        echo "ERROR: failed to create TKG Workload Cluster"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ tanzu cluster create $TDH_TKGWC_NAME -f $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.yaml --tkr $TDH_TKGWC_K8S_VERSION"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => tanzu cluster create $TDH_TKGWC_NAME -f $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.yaml --tkr $TDH_TKGWC_K8S_VERSION"
        fi
        exit 1
      fi
    
      #dockerPullImages="projects.registry.vmware.com/tkg/antrea/antrea-debian:v0.13.3_vmware.1 \
      #                  projects.registry.vmware.com/tkg/tanzu_core/capabilities/capabilities-controller-manager:v1.4.1"
    fi

    # --- GET CREDENTIALS AND SET ENVIRONMENT ---
    tanzu cluster kubeconfig get $TDH_TKGWC_NAME --admin > /dev/null 2>&1

    # --- CREATE GLOBAL KUBECONFIG FILE ---
    mv $HOME/.kube/config $HOME/.kube/config.bak
    tanzu cluster kubeconfig get $TDH_TKGWC_NAME --admin > /dev/null 2>&1
    mv $HOME/.kube/config $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.kubeconfig
    mv $HOME/.kube/config.bak $HOME/.kube/config
  fi

  #########################################################################################################################
  ############################################## INSTALL TKG ON VSPHERE ###################################################
  #########################################################################################################################
  if [ "${TDH_TKGMC_INFRASTRUCTURE}" == "vSphere" ]; then
    # --- GENERATE WORKLOAD CLUSTER CONFIG FILE ---
    CLUSTER_CONFIG_FILE=$HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.yaml

    if [ "$TDH_TKGMC_ENVNAME" != "tkgs" ]; then
      if [ "$TDH_TKGWC_NAME_TAG" == "TKG_CLUSTER_01" -o "$TDH_TKGWC_NAME_TAG" == "TKG_CLUSTER_02" -o \
        "$TDH_TKGWC_NAME_TAG" == "TKG_CLUSTER_03" ]; then

        if [ "$TDH_TKGWC_NAME_TAG" == "TKG_CLUSTER_01" ]; then
          TDH_TKGWC_CPIP_ADDRESS=$VSPHERE_TKGM_WKLD_CLUSTER01_CONTROL_PLANE
          TDH_TKGWC_LB_ADDRESS=$VSPHERE_TKGM_WKLD_CLUSTER01_LOADBALANCER_POOL
        fi

        if [ "$TDH_TKGWC_NAME_TAG" == "TKG_CLUSTER_02" ]; then
          TDH_TKGWC_CPIP_ADDRESS=$VSPHERE_TKGM_WKLD_CLUSTER02_CONTROL_PLANE
          TDH_TKGWC_LB_ADDRESS=$VSPHERE_TKGM_WKLD_CLUSTER03_LOADBALANCER_POOL

        fi
  
        if [ "$TDH_TKGWC_NAME_TAG" == "TKG_CLUSTER_04" ]; then
          TDH_TKGWC_CPIP_ADDRESS=$VSPHERE_TKGM_WKLD_CLUSTER04_CONTROL_PLANE
          TDH_TKGWC_LB_ADDRESS=$VSPHERE_TKGM_WKLD_CLUSTER04_LOADBALANCER_POOL
        fi
      else
        echo "ERROR: Option -tag should be either: TKG_CLUSTER_01, TKG_CLUSTER_02 or TKG_CLUSTER_03"
        usage; exit 0
      fi

      messageTitle "Tanzu Kubernetes Grid Deployment"
      messagePrint " ▪ Management Cluster Name"               "$TDH_TKGMC_NAME"
      messagePrint " ▪ Management Cluster Configuration"      "deployments/${TDH_TKGMC_NAME}.cfg"
      messagePrint " ▪ Management Cluster Kubeconfig"         "deployments/${TDH_TKGMC_NAME}.kubeconfig"
      messagePrint " ▪ Workload Cluster Name"                 "$TDH_TKGWC_NAME"
      messagePrint " ▪ Workload Cluster Configuration"        "deployments/$TKG_DEPLOYMENT"
      messagePrint " ▪ Workload Cluster consolidated Config"  "$HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.yaml"

      # --- GET KUBERNETES VERSION ---
      if [ "$TDH_TKGWC_K8S_VERSION" == "" ]; then
        TDH_TKGWC_K8S_VERSION=$(tanzu kubernetes-release get | egrep "$TDH_TKGWC_KUBERNETES" | tail -1 | awk '{ print $1 }')
      fi

      # --- SET DEFAULT VALUES ---
      [ "$TDH_TKGWC_CONTROL_PLANE_MEM_MIB" == "" ] && TDH_TKGWC_CONTROL_PLANE_MEM_MIB=8192
      [ "$TDH_TKGWC_CONTROL_PLANE_NUM_CPUS" == "" ] && TDH_TKGWC_CONTROL_PLANE_NUM_CPUS=4
      [ "$TDH_TKGWC_CONTROL_PLANE_DISK_GIB" == "" ] && TDH_TKGWC_CONTROL_PLANE_DISK_GIB=40
      [ "$TDH_TKGWC_WORKER_DISK_GIB" == "" ] && TDH_TKGWC_WORKER_DISK_GIB=40
      [ "$TDH_TKGWC_WORKER_MEM_MIB" == "" ] && TDH_TKGWC_WORKER_MEM_MIB=8192
      [ "$TDH_TKGWC_WORKER_NUM_CPUS" == "" ] && TDH_TKGWC_WORKER_NUM_CPUS=2

      messagePrint " ▪ Control Plane Machine CPU"     "$TDH_TKGWC_CONTROL_PLANE_NUM_CPUS"
      messagePrint " ▪ Control Plane Machine Memory"  "$TDH_TKGWC_CONTROL_PLANE_MEM_MIB"
      messagePrint " ▪ Control Plane Machine Disk"    "$TDH_TKGWC_CONTROL_PLANE_DISK_GIB"
      messagePrint " ▪ Worker Node Machine CPU"       "$TDH_TKGWC_WORKER_NUM_CPUS"
      messagePrint " ▪ Worker Node Machine Memory"    "$TDH_TKGWC_WORKER_MEM_MIB"
      messagePrint " ▪ Worker Node Machine Disk"      "$TDH_TKGWC_WORKER_DISK_GIB"

      # --- RESOURCE DEFINITIONS ---
      str="^#|ENDPOINT|AZURE_RESOURCE_GROUP|AZURE_VNET_RESOURCE_GROUP|AZURE_NODE_MACHINE_TYPE|AZURE_CONTROL_PLANE_MACHINE_TYPE|CLUSTER_NAME|AZURE_VNET_NAME"
      egrep -v "$str" $DEP_SOURCE | sed -e '/^$/d'                                    >  $CLUSTER_CONFIG_FILE
      echo "CLUSTER_NAME: $TDH_TKGWC_NAME"                                            >> $CLUSTER_CONFIG_FILE
      echo "WORKER_MACHINE_COUNT: $TDH_TKGWC_WORKERNODES"                             >> $CLUSTER_CONFIG_FILE
      echo "CNI: antrea"                                                              >> $CLUSTER_CONFIG_FILE

      echo "VSPHERE_CONTROL_PLANE_DISK_GIB: \"$TDH_TKGWC_CONTROL_PLANE_DISK_GIB\""    >> $CLUSTER_CONFIG_FILE
      echo "VSPHERE_CONTROL_PLANE_MEM_MIB: \"$TDH_TKGWC_CONTROL_PLANE_MEM_MIB\""      >> $CLUSTER_CONFIG_FILE
      echo "VSPHERE_CONTROL_PLANE_NUM_CPUS: \"$TDH_TKGWC_CONTROL_PLANE_NUM_CPUS\""    >> $CLUSTER_CONFIG_FILE
      echo "VSPHERE_WORKER_DISK_GIB: \"$TDH_TKGWC_WORKER_DISK_GIB\""                  >> $CLUSTER_CONFIG_FILE
      echo "VSPHERE_WORKER_MEM_MIB: \"$TDH_TKGWC_WORKER_MEM_MIB\""                    >> $CLUSTER_CONFIG_FILE
      echo "VSPHERE_WORKER_NUM_CPUS: \"$TDH_TKGWC_WORKER_NUM_CPUS\""                  >> $CLUSTER_CONFIG_FILE
    
      echo "VSPHERE_CONTROL_PLANE_ENDPOINT: $TDH_TKGWC_CPIP_ADDRESS"                  >> $CLUSTER_CONFIG_FILE
    fi

    if [ "$TDH_TKGMC_ENVNAME" == "tkgs" ]; then
      [ -s $HOME/.tanzu-demo-hub/config/${TKG_DEPLOYMENT} ] && . $HOME/.tanzu-demo-hub/config/${TKG_DEPLOYMENT}

      export KUBECTL_VSPHERE_PASSWORD=$TDH_TKGMC_VSPHERE_PASS
      cmdLoop kubectl vsphere login --insecure-skip-tls-verify --server $TDH_TKGMC_SUPERVISORCLUSTER -u $TDH_TKGMC_VSPHERE_USER > /tmp/error.log 2>&1; ret=$?
      if [ $ret -ne 0 ]; then
        logMessages /tmp/error.log
        echo "ERROR: failed to login to vSphere Supervisor Cluster"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ export KUBECTL_VSPHERE_PASSWORD=$TDH_TKGMC_VSPHERE_PASS"
          echo "          tdh-tools:/$ kubectl vsphere login --insecure-skip-tls-verify --server $TDH_TKGMC_SUPERVISORCLUSTER \\"
          echo "                               -u $TDH_TKGMC_VSPHERE_USER"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => export KUBECTL_VSPHERE_PASSWORD=$TDH_TKGMC_VSPHERE_PASS"
          echo "       => kubectl vsphere login --insecure-skip-tls-verify --server $TDH_TKGMC_SUPERVISORCLUSTER \\"
          echo "          -u $TDH_TKGMC_VSPHERE_USER"
        fi
        exit 1
      fi

      # --- SET CONTEXT TO SUPERVISOR CLUSTER ---
      cmdLoop kubectl config use-context $TDH_TKGMC_SUPERVISORCLUSTER > /dev/null 2>&1

      TDH_TKGWC_K8S_VERSION=$(kubectl get tanzukubernetesreleases -n $TDH_TKGMC_VSPHERE_NAMESPACE | \
           egrep "$TDH_TKGWC_KUBERNETES" | tail -1 | awk '{ print $1 }')

      TDH_TKGWC_WORKERNODES_MACHINE=$(nodeType $TDH_TKGWC_WORKERNODES_TYPE)
      TDH_TKGWC_CONTROPLANE_MACHINE=$(nodeType $TDH_TKGWC_CONTROPLANE_NODETYPE)

      messageTitle "Tanzu Kubernetes Grid Deployment"
      messagePrint " ▪ Management Cluster Name"               "$TDH_TKGMC_NAME"
      messagePrint " ▪ Management Cluster Configuration"      "HOME/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.cfg"
      messagePrint " ▪ Management Cluster Kubeconfig"         "HOME/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.kubeconfig"
      messagePrint " ▪ Workload Cluster Name"                 "$TDH_TKGWC_NAME"
      messagePrint " ▪ Workload Cluster Configuration"        "$HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.yaml"
      messagePrint " ▪ Workload Cluster Kubeconfig"           "$HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.kubeconfig"

      messagePrint " ▪ TKG Cluster Kubernetes Release"        "$TDH_TKGWC_K8S_VERSION"
      messagePrint " ▪ TKG Cluster Worker Nodes"              "$TDH_TKGWC_WORKERNODES"
      messagePrint " ▪ TKG Cluster Worker Node Type"          "$TDH_TKGWC_WORKERNODES_TYPE ($TDH_TKGWC_WORKERNODES_MACHINE)"
      messagePrint " ▪ TKG Control Plane Nodes"               "$TDH_TKGWC_CONTROPLANE"
      messagePrint " ▪ TKG Control Plane Nodes Type"          "$TDH_TKGWC_CONTROPLANE_NODETYPE ($TDH_TKGWC_CONTROPLANE_MACHINE)"

      # --- WRITE CLUSTER CONFIG ---
      echo "TDH_TKGMC_NAME=$TDH_TKGMC_NAME"                                                       >  $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.cfg
      echo "TDH_TKGMC_SUPERVISORCLUSTER=$TDH_TKGMC_SUPERVISORCLUSTER"                             >> $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.cfg
      echo "TDH_TKGWC_K8S_VERSION=$TDH_TKGWC_K8S_VERSION"                                         >> $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.cfg
      echo "TDH_TKGMC_STORAGE_CLASS=$TDH_TKGMC_STORAGE_CLASS"                                     >> $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.cfg
      echo "TDH_TKGMC_VSPHERE_NAMESPACE=$TDH_TKGMC_VSPHERE_NAMESPACE"                             >> $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.cfg
      echo "TDH_TKGMC_CONFIG=HOME/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.cfg"                   >> $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.cfg
      echo "TDH_TKGMC_KUBECONFIG=HOME/.tanzu-demo-hub/config/${TDH_TKGMC_NAME}.kubeconfig"        >> $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.cfg

      # --- CLUSTER STATUS ---
      cmdLoop kubectl get clusters -n $TDH_TKGMC_VSPHERE_NAMESPACE -o json > /tmp/output.json 2>/dev/null
      stt=$(jq -r --arg key "$TDH_TKGWC_NAME" '.items[] | select(.metadata.name == $key).status.conditions[] | select(.type == "Ready").status' /tmp/output.json)
      [ "$stt" == "True" ] && sss="Ready" || sss="Failed" 
      if [ "${stt}" != "True" ]; then 
        if [ "${stt}" != "" -a "${stt}" != "null" ]; then
          logMessages /tmp/error.log
          echo "ERROR: TKG Workload Cluster is in a unknown state ($stt), Please verify manually and may delete it"
          if [ "$NATIVE" == "0" ]; then
            echo "       => tools/${TDH_TOOLS}.sh"
            echo "          tdh-tools:/$ export KUBECTL_VSPHERE_PASSWORD=$TDH_TKGMC_VSPHERE_PASS"
            echo "          tdh-tools:/$ kubectl vsphere login --insecure-skip-tls-verify --server $TDH_TKGMC_SUPERVISORCLUSTER \\"
            echo "                               -u $TDH_TKGMC_VSPHERE_USER"
            echo "          tdh-tools:/$ kubectl get cluster $TDH_TKGWC_NAME -n $TDH_TKGMC_VSPHERE_NAMESPACE"
            echo "          tdh-tools:/$ exit"
          else
            echo "       => export KUBECTL_VSPHERE_PASSWORD=$TDH_TKGMC_VSPHERE_PASS"
            echo "       => kubectl vsphere login --insecure-skip-tls-verify --server $TDH_TKGMC_SUPERVISORCLUSTER \\"
            echo "          -u $TDH_TKGMC_VSPHERE_USER"
            echo "       => kubectl get cluster $TDH_TKGWC_NAME -n $TDH_TKGMC_VSPHERE_NAMESPACE"
          fi
          exit 1
        fi

        echo "apiVersion: run.tanzu.vmware.com/v1alpha1"                                >  $CLUSTER_CONFIG_FILE
        echo "kind: TanzuKubernetesCluster"                                             >> $CLUSTER_CONFIG_FILE
        echo "metadata:"                                                                >> $CLUSTER_CONFIG_FILE
        echo "  name: $TDH_TKGWC_NAME"                                                  >> $CLUSTER_CONFIG_FILE
        echo "  namespace: $TDH_TKGMC_VSPHERE_NAMESPACE"                                >> $CLUSTER_CONFIG_FILE
        echo "spec:"                                                                    >> $CLUSTER_CONFIG_FILE
        echo "  distribution:"                                                          >> $CLUSTER_CONFIG_FILE
        echo "    fullVersion: $TDH_TKGWC_K8S_VERSION"                                  >> $CLUSTER_CONFIG_FILE
        echo "    version: """                                                          >> $CLUSTER_CONFIG_FILE
        echo "  settings:"                                                              >> $CLUSTER_CONFIG_FILE
        echo "    network:"                                                             >> $CLUSTER_CONFIG_FILE
        echo "      cni:"                                                               >> $CLUSTER_CONFIG_FILE
        echo "        name: antrea"                                                     >> $CLUSTER_CONFIG_FILE
        echo "      pods:"                                                              >> $CLUSTER_CONFIG_FILE
        echo "        cidrBlocks:"                                                      >> $CLUSTER_CONFIG_FILE
        echo "        - 172.20.0.0/16"                                                  >> $CLUSTER_CONFIG_FILE
        echo "      serviceDomain: cluster.local"                                       >> $CLUSTER_CONFIG_FILE
        echo "      services:"                                                          >> $CLUSTER_CONFIG_FILE
        echo "        cidrBlocks:"                                                      >> $CLUSTER_CONFIG_FILE
        echo "        - 10.96.0.0/16"                                                   >> $CLUSTER_CONFIG_FILE
        echo "    storage: "                                                            >> $CLUSTER_CONFIG_FILE
        echo "      defaultClass: $TDH_TKGMC_STORAGE_CLASS"                             >> $CLUSTER_CONFIG_FILE
        echo "  topology:"                                                              >> $CLUSTER_CONFIG_FILE
        echo "    controlPlane:"                                                        >> $CLUSTER_CONFIG_FILE
        echo "      class: $TDH_TKGWC_CONTROPLANE_MACHINE"                              >> $CLUSTER_CONFIG_FILE
        echo "      count: $TDH_TKGWC_CONTROPLANE"                                      >> $CLUSTER_CONFIG_FILE
        echo "      storageClass: $TDH_TKGMC_STORAGE_CLASS"                             >> $CLUSTER_CONFIG_FILE
        echo "    workers:"                                                             >> $CLUSTER_CONFIG_FILE
        echo "      #class: best-effort-2xlarge"                                        >> $CLUSTER_CONFIG_FILE
        echo "      class: $TDH_TKGWC_WORKERNODES_MACHINE"                              >> $CLUSTER_CONFIG_FILE
        echo "      count: $TDH_TKGWC_WORKERNODES"                                      >> $CLUSTER_CONFIG_FILE
        echo "      storageClass: tanzu"                                                >> $CLUSTER_CONFIG_FILE
        echo "      volumes:"                                                           >> $CLUSTER_CONFIG_FILE
        echo "        - name: containerd"                                               >> $CLUSTER_CONFIG_FILE
        echo "          mountPath: /usr/lib/containerd"                                 >> $CLUSTER_CONFIG_FILE
        echo "          capacity:"                                                      >> $CLUSTER_CONFIG_FILE
        echo "            storage: 30Gi"                                                >> $CLUSTER_CONFIG_FILE
#        echo "        - name: varlib"                                                   >> $CLUSTER_CONFIG_FILE
#        echo "          mountPath: /var/lib"                                            >> $CLUSTER_CONFIG_FILE
#        echo "          capacity:"                                                      >> $CLUSTER_CONFIG_FILE
#        echo "            storage: 50Gi"                                                >> $CLUSTER_CONFIG_FILE
      
        messagePrint " ▪ TKG Workload Cluster ($TDH_TKGWC_NAME)"          "deploying cluster ....."
        cmdLoop kubectl apply -f $CLUSTER_CONFIG_FILE --wait=true > /dev/null 2>&1

        stt=""
        while [ "$stt" != "True" ]; do
          cmdLoop kubectl get clusters -n $TDH_TKGMC_VSPHERE_NAMESPACE -o json > /tmp/output.json 2>/dev/null
          stt=$(jq -r --arg key "$TDH_TKGWC_NAME" '.items[] | select(.metadata.name == $key).status.conditions[] | select(.type == "Ready").status' /tmp/output.json)
          [ "$stt" == "True" ] && break
          sleep 10
        done

        stt=""
        while [ "$stt" != "True" ]; do
          cmdLoop kubectl get tanzukubernetescluster -n $TDH_TKGMC_VSPHERE_NAMESPACE -o json > /tmp/output.json 2>/dev/null
          stt=$(jq -r --arg key "$TDH_TKGWC_NAME" '.items[] | select(.metadata.name == $key).status.conditions[] | select(.type == "Ready").status' /tmp/output.json)
          [ "$stt" == "True" ] && break
          sleep 10
        done

        # --- VERIFY STATZS ---
        cmdLoop kubectl get clusters -n $TDH_TKGMC_VSPHERE_NAMESPACE -o json > /tmp/output.json 2>/dev/null
        stt=$(jq -r --arg key "$TDH_TKGWC_NAME" '.items[] | select(.metadata.name == $key).status.conditions[].status' /tmp/output.json)
        typ=$(jq -r --arg key "$TDH_TKGWC_NAME" '.items[] | select(.metadata.name == $key).status.conditions[].type' /tmp/output.json)
      fi
    fi

    vsphereWorkloadClusterLogin $TKG_DEPLOYMENT $TDH_TKGWC_NAME; ret=$?
    if [ ${ret} -eq 0 ]; then
      messagePrint " ▪ TKG Workload Cluster Status"                 "Ready"
    else
      echo "ERROR: Login to TKG Workload Cluster Failed"
      echo "       => vsphereWorkloadClusterLogin $TKG_DEPLOYMENT $TDH_TKGWC_NAME"
      exit 1
    fi


    # Deploying Workloads and Extensions on TKGS Clusters
    # https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-E39346D8-320F-4B3D-B526-5313D99129CE.html

    # Prepare a Tanzu Kubernetes Cluster Created by Using the Tanzu Kubernetes Grid Service to Run Packages
    # https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-packages-prep-tkgs-kapp.html

    cmdLoop kubectl get PodSecurityPolicy -o json > /tmp/outpus.json
    nam=$(jq -r --arg key "tanzu-system-kapp-ctrl-restricted" '.items[].metadata | select(.name == $key).name' /tmp/output.json) 
    if [ "$nam" != "tanzu-system-kapp-ctrl-restricted" ]; then 
      PSP_CONFIG=/tmp/tanzu-system-kapp-ctrl-restricted.yaml
      echo "apiVersion: policy/v1beta1"                                 >  $PSP_CONFIG
      echo "kind: PodSecurityPolicy"                                    >> $PSP_CONFIG
      echo "metadata:"                                                  >> $PSP_CONFIG
      echo "  name: tanzu-system-kapp-ctrl-restricted"                  >> $PSP_CONFIG
      echo "spec:"                                                      >> $PSP_CONFIG
#      echo "  privileged: false"                                        >> $PSP_CONFIG
#      echo "  allowPrivilegeEscalation: false"                          >> $PSP_CONFIG
      echo "  privileged: true"                                         >> $PSP_CONFIG
      echo "  allowPrivilegeEscalation: true"                           >> $PSP_CONFIG
      echo "  requiredDropCapabilities:"                                >> $PSP_CONFIG
      echo "    - ALL"                                                  >> $PSP_CONFIG
      echo "  volumes:"                                                 >> $PSP_CONFIG
      echo "    - configMap"                                            >> $PSP_CONFIG
      echo "    - emptyDir"                                             >> $PSP_CONFIG
      echo "    - projected"                                            >> $PSP_CONFIG
      echo "    - secret"                                               >> $PSP_CONFIG
      echo "    - downwardAPI"                                          >> $PSP_CONFIG
      echo "    - persistentVolumeClaim"                                >> $PSP_CONFIG
      echo "  hostNetwork: false"                                       >> $PSP_CONFIG
      echo "  hostIPC: false"                                           >> $PSP_CONFIG
      echo "  hostPID: false"                                           >> $PSP_CONFIG
      echo "  runAsUser:"                                               >> $PSP_CONFIG
      echo "    rule: MustRunAsNonRoot"                                 >> $PSP_CONFIG
      echo "  seLinux:"                                                 >> $PSP_CONFIG
      echo "    rule: RunAsAny"                                         >> $PSP_CONFIG
      echo "  supplementalGroups:"                                      >> $PSP_CONFIG
      echo "    rule: MustRunAs"                                        >> $PSP_CONFIG
      echo "    ranges:"                                                >> $PSP_CONFIG
      echo "      - min: 1"                                             >> $PSP_CONFIG
      echo "        max: 65535"                                         >> $PSP_CONFIG
      echo "  fsGroup:"                                                 >> $PSP_CONFIG
      echo "    rule: MustRunAs"                                        >> $PSP_CONFIG
      echo "    ranges:"                                                >> $PSP_CONFIG
      echo "      - min: 1"                                             >> $PSP_CONFIG
      echo "        max: 65535"                                         >> $PSP_CONFIG
      echo "  readOnlyRootFilesystem: false"                            >> $PSP_CONFIG

      if [ $DEBUG -eq 1 ]; then
        messagePrint " ▪ Define PodSecurityPolicy"                 "tanzu-system-kapp-ctrl-restricted"
        messageLine
        kubectl apply -f $PSP_CONFIG 
        messageLine
      else
        messagePrint " ▪ Define PodSecurityPolicy"                 "tanzu-system-kapp-ctrl-restricted"
        kubectl apply -f $PSP_CONFIG > /dev/null 2>&1
      fi
    fi

    cnt=$(kubectl get pods -n tkg-system | grep -c kapp-controller)
    if [ $cnt -eq 0 ]; then 
      if [ $DEBUG -eq 1 ]; then
        messagePrint " ▪ Installing Kapp Controller"               "$TDHPATH/files/kapp-controller.yaml"
        messageLine
        kubectl apply -f $TDHPATH/files/kapp-controller.yaml
        messageLine
      else
        messagePrint " ▪ Define PodSecurityPolicy"                 "tanzu-system-kapp-ctrl-restricted"
        kubectl apply -f $TDHPATH/files/kapp-controller.yaml > /dev/null 2>&1
      fi

      runningPods kube-system
      runningPods tkg-system

      #tanzu package repository add REPOSITORY-NAME -n tanzu-package-repo-global --url projects.registry.vmware.com/tkg/packages/standard/repo:v1.5.0
      #tanzu package repository add tanzu-system-packages --url projects.registry.vmware.com/tkg/packages/standard/repo:v1.5.0
    fi

    # --- RELAXING POD SECUROTY ---
    export KUBECONFIG=$HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.kubeconfig
    kubectl apply -f $TDHPATH/files/psp/vmware-system-privileged.yaml > /dev/null 2>&1
  fi
}

vsphereSupervisorClusterLogin() {
  MC_CONFIG=$1
  
  [ -s $HOME/.tanzu-demo-hub/config/${MC_CONFIG}.cfg ] && . $HOME/.tanzu-demo-hub/config/${MC_CONFIG}.cfg
  [ -s $HOME/.kube/config ] && mv $HOME/.kube/config $HOME/.kube/config.old

  # --- SET LOGIN PASSWORD AS ENVIRONMENT VARIABLE ---
  export KUBECTL_VSPHERE_PASSWORD=$TDH_TKGMC_VSPHERE_PASS

  cnt=$(echo $TDH_TKGMC_SUPERVISORCLUSTER | egrep -c "pez.vmware.com")
  if [ $cnt -gt 0 ]; then 
    curl -m 3 https://pez-portal.int-apps.pcfone.io > /dev/null 2>&1; ret=$?
    if [ $ret -ne 0 ]; then
      echo "ERROR: you are not logged into VMware VPN"
      exit
    fi
  fi

  ret=1; cnt=0
  while [ $ret -ne 0 -a $cnt -lt 5 ]; do
    kubectl vsphere login --insecure-skip-tls-verify --server $TDH_TKGMC_SUPERVISORCLUSTER -u $TDH_TKGMC_VSPHERE_USER > /tmp/error.log 2>&1; ret=$?
    [ $ret -eq 0 ] && break
    let cnt=cnt+1
    sleep 30
  done

  [ -s $HOME/.kube/config ] && mv $HOME/.kube/config $HOME/.tanzu-demo-hub/config/${MC_CONFIG}.kubeconfig
  mv $HOME/.kube/config.old $HOME/.kube/config

  ret=1; cnt=0
  while [ $ret -ne 0 -a $cnt -lt 5 ]; do
    kubectl vsphere login --insecure-skip-tls-verify --server $TDH_TKGMC_SUPERVISORCLUSTER -u $TDH_TKGMC_VSPHERE_USER > /tmp/error.log 2>&1; ret=$?
    [ $ret -eq 0 ] && break
    let cnt=cnt+1
    sleep 30
  done
}

vsphereWorkloadClusterLogin() {
  MC_CONFIG=$1
  TKGWC_NAME=$2

  [ -s $HOME/.tanzu-demo-hub/config/$MC_CONFIG ] && . $HOME/.tanzu-demo-hub/config/$MC_CONFIG
  [ -s $HOME/.kube/config ] && mv $HOME/.kube/config $HOME/.kube/config.old
 
  export KUBECTL_VSPHERE_PASSWORD=$TDH_TKGMC_VSPHERE_PASS

  cmdLoop kubectl vsphere login \
      --tanzu-kubernetes-cluster-name $TKGWC_NAME \
      --tanzu-kubernetes-cluster-namespace $TDH_TKGMC_VSPHERE_NAMESPACE \
      --server $VSPHERE_TKGS_SUPERVISOR_CLUSTER \
      --insecure-skip-tls-verify \
      -u administrator@vsphere.local > /dev/null 2>&1; ret=$?

  if [ $ret -ne 0 ]; then

    mv $HOME/.kube/config.old $HOME/.kube/config 
    return 1
  fi

  mv $HOME/.kube/config $HOME/.tanzu-demo-hub/config/${TDH_TKGWC_NAME}.kubeconfig
  mv $HOME/.kube/config.old $HOME/.kube/config 

  # --- DO THE SAME AGAIN TO UPDATE (HOME/.kube/config) AS WELL ---
  cmdLoop kubectl vsphere login \
      --tanzu-kubernetes-cluster-name $TDH_TKGWC_NAME \
      --tanzu-kubernetes-cluster-namespace $TDH_TKGMC_VSPHERE_NAMESPACE \
      --server $VSPHERE_TKGS_SUPERVISOR_CLUSTER \
      --insecure-skip-tls-verify \
      -u administrator@vsphere.local > /dev/null 2>&1

  return 0
}

tmcCreateCluster() {
  # steve: missing provisioner, returned sacha's cluster

  if [ "$TDH_DEPLOYMENT_CLOUD" == "Azure" ]; then 
    TDH_PROVISIONER_NAME=default
    #TMC_ACCOUNT_NAME=$TMC_ACCOUNT_NAME_AWS
    TMC_ACCOUNT_NAME=sdubois@vnware.com

    stt=$(tmc cluster list -m $TDH_MANAGEMENT_CLUSTER -p $TDH_PROVISIONER_NAME --name $TDH_CLUSTER_NAME -o json 2>/dev/null | \
          jq -r '.clusters[].status.phase' 2>/dev/null) 

    if [ "$stt" == "CREATING" ]; then                                         
       echo "ERROR: Cluster is currently beeing crated, please try again later"; exit 1
    fi

    if [ "$stt" == "DELETING" ]; then                                         
       echo "ERROR: Cluster is currently on deleting, please try again later"; exit 1
    fi

    if [ "$stt" == "" -o "$stt" != "READY" ]; then
      [ "$TMC_KUBERNETES_VERSION" != "" ] && KUBERNETES_VERSION=$TMC_KUBERNETES_VERSION
      [ "$TDH_TKGWC_CONTROPLANE" == "3" ] && TMC_TKGWC_TEMPLATE="aws-ha" || TMC_TKGWC_TEMPLATE="default"

      # --- SEARCH KUBERNETES VERSIONS ---
      TDH_TKGMC_TMC_CONFIG=$(egrep "^TDH_TKGMC_TMC_CONFIG=" $HOME/.tanzu-demo-hub/config/${TDH_MANAGEMENT_CLUSTER}.cfg | awk -F= '{ print $2 }')
      KUBERNETES_NAME=$(egrep "^# TMC_K8S_VERSIONS:" $HOME/.tanzu-demo-hub/config/${TDH_TKGMC_TMC_CONFIG} | \
                        egrep "$TDH_TKGWC_KUBERNETES" | tail -1 | awk -F: '{ print $2 }')
      KUBERNETES_VERSION=$(egrep "^# TMC_K8S_VERSIONS:" $HOME/.tanzu-demo-hub/config/${TDH_TKGMC_TMC_CONFIG} | \
                           egrep "$TDH_TKGWC_KUBERNETES" | tail -1 | awk -F: '{ print $3 }')
      KUBERNETES_OSVER=$(egrep "^# TMC_K8S_OS_REL:$KUBERNETES_NAME:" $HOME/.tanzu-demo-hub/config/${TDH_TKGMC_TMC_CONFIG} | \
                         tail -1 | awk -F: '{ print $4 }')
      KUBERNETES_OSTYP=$(egrep "^# TMC_K8S_OS_REL:$KUBERNETES_NAME:" $HOME/.tanzu-demo-hub/config/${TDH_TKGMC_TMC_CONFIG} | \
                         tail -1 | awk -F: '{ print $3 }')

      messageTitle "TMC Create Workload Cluster"     
      messagePrint " ▪ TMC Cluster Name"                 "$TDH_CLUSTER_NAME"
      messagePrint " ▪ TMC Cluster Group"                "$TDH_CLUSTER_GROUP"
      messagePrint " ▪ TMC Template"                     "$TMC_TKGWC_TEMPLATE"
      messagePrint " ▪ TMC Provisioner Name"             "$TDH_PROVISIONER_NAME"
      messagePrint " ▪ TKG Management Cluster"           "$TDH_MANAGEMENT_CLUSTER"

      messageTitle "TMC Create Workload Cluster"     
      messagePrint " ▪ TMC Cluster Name"                 "$TDH_CLUSTER_NAME"
      messagePrint " ▪ TMC Cluster Group"                "$TDH_CLUSTER_GROUP"
      messagePrint " ▪ TMC Template"                     "$TMC_TKGWC_TEMPLATE"
      messagePrint " ▪ TMC Provisioner Name"             "$TDH_PROVISIONER_NAME"
      messagePrint " ▪ TKG Management Cluster"           "$TDH_MANAGEMENT_CLUSTER"
      messagePrint " ▪ TMC Account Name"                 "$TMC_ACCOUNT_NAME"
      messagePrint " ▪ Kubernetes Version"               "$TDH_TKGWC_KUBERNETES"
      messagePrint " ▪ Kubernetes Image"                 "$KUBERNETES_VERSION"

      TDH_TKGWC_WORKERNODES_MACHINE=$(nodeType $TDH_TKGWC_WORKERNODES_TYPE)
      TDH_TKGWC_CONTROPLANE_MACHINE=$(nodeType $TDH_TKGWC_CONTROPLANE_NODETYPE)
      messagePrint " ▪ TKG Cluster Worker Nodes"         "$TDH_TKGWC_WORKERNODES"
      messagePrint " ▪ TKG Cluster Worker Node Type"     "$TDH_TKGWC_WORKERNODES_TYPE ($TDH_TKGWC_WORKERNODES_MACHINE)"
      messagePrint " ▪ TKG Control Plane Nodes"          "$TDH_TKGWC_CONTROPLANE"
      messagePrint " ▪ TKG Control Plane Nodes Type"     "$TDH_TKGWC_CONTROPLANE_NODETYPE ($TDH_TKGWC_CONTROPLANE_MACHINE)"

      if [ -f $HOME/.tanzu-demo-hub/KeyPair-Azure.pub ]; then 
        read ssh_key < $HOME/.tanzu-demo-hub/KeyPair-Azure.pub
      else
        echo "ERROR: ssh key $HOME/.tanzu-demo-hub/KeyPair-Azure.pub does not exist"
        exit
      fi

      TMC_CONFIG=/tmp/tmc_config.yaml
      echo ""                                                                                                                        >  $TMC_CONFIG
      echo "fullName:"                                                                                                               >> $TMC_CONFIG
      echo "  managementClusterName: $TDH_MANAGEMENT_CLUSTER"                                                                        >> $TMC_CONFIG
      echo "  name: $TDH_CLUSTER_NAME"                                                                                               >> $TMC_CONFIG
      echo "  provisionerName: default"                                                                                              >> $TMC_CONFIG
      echo "meta:"                                                                                                                   >> $TMC_CONFIG
      echo "spec:"                                                                                                                   >> $TMC_CONFIG
      echo "  clusterGroupName: $TDH_CLUSTER_GROUP"                                                                                  >> $TMC_CONFIG
      echo "  tkgAzure:"                                                                                                             >> $TMC_CONFIG
      echo "    distribution:"                                                                                                       >> $TMC_CONFIG
      echo "      osName: $KUBERNETES_OSTYP"                                                                                         >> $TMC_CONFIG
      echo "      osVersion: \"$KUBERNETES_OSVER\""                                                                                  >> $TMC_CONFIG
      echo "      region: $AZURE_LOCATION"                                                                                           >> $TMC_CONFIG
      echo "      resourceGroup: ${TDH_CLUSTER_NAME}-rg"                                                                             >> $TMC_CONFIG
      echo "      version: $KUBERNETES_VERSION"                                                                                      >> $TMC_CONFIG
      echo "    settings:"                                                                                                           >> $TMC_CONFIG
      echo "      network:"                                                                                                          >> $TMC_CONFIG
      echo "        cluster:"                                                                                                        >> $TMC_CONFIG
      echo "          pods:"                                                                                                         >> $TMC_CONFIG
      echo "            cidrBlocks:"                                                                                                 >> $TMC_CONFIG
      echo "            - 100.96.0.0/11"                                                                                             >> $TMC_CONFIG
      echo "          services:"                                                                                                     >> $TMC_CONFIG
      echo "            cidrBlocks:"                                                                                                 >> $TMC_CONFIG
      echo "            - 100.64.0.0/13"                                                                                             >> $TMC_CONFIG
      echo "        provider:"                                                                                                       >> $TMC_CONFIG
      echo "          controlPlaneSubnet:"                                                                                           >> $TMC_CONFIG
      echo "            cidrBlock: 10.0.0.0/24"                                                                                      >> $TMC_CONFIG
      echo "            name: sacha-controlplane-subnet"                                                                             >> $TMC_CONFIG
      echo "          vnet:"                                                                                                         >> $TMC_CONFIG
      echo "            cidrBlock: 10.0.0.0/16"                                                                                      >> $TMC_CONFIG
      echo "            spec:"                                                                                                       >> $TMC_CONFIG
      echo "              name: tkg-vnet"                                                                                            >> $TMC_CONFIG
      echo "              resourceGroup: ${TDH_CLUSTER_NAME}-rg"                                                                     >> $TMC_CONFIG
      echo "          workerNodeSubnet:"                                                                                             >> $TMC_CONFIG
      echo "            cidrBlock: 10.0.1.0/24"                                                                                      >> $TMC_CONFIG
      echo "            name: tkg-node-subnet"                                                                                       >> $TMC_CONFIG
      echo "      security:"                                                                                                         >> $TMC_CONFIG
      echo "        sshKeyValue: $ssh_key"                                                                                           >> $TMC_CONFIG
      echo "        subscriptionId: $AZURE_SUBSCRIPTION_ID"                                                                          >> $TMC_CONFIG
      echo "    topology:"                                                                                                           >> $TMC_CONFIG
      echo "      controlPlane:"                                                                                                     >> $TMC_CONFIG
      echo "        vmSize: $TDH_TKGWC_CONTROPLANE_MACHINE"                                                                          >> $TMC_CONFIG
      echo "      nodePools:"                                                                                                        >> $TMC_CONFIG
      echo "       - spec:"                                                                                                          >> $TMC_CONFIG
      echo "           workerNodeCount: $TDH_TKGWC_WORKERNODES"                                                                      >> $TMC_CONFIG
      echo "           tkgAzure:"                                                                                                    >> $TMC_CONFIG
      echo "             vmSize: $TDH_TKGWC_WORKERNODES_MACHINE"                                                                     >> $TMC_CONFIG
      echo "         info:"                                                                                                          >> $TMC_CONFIG
      echo "           name: md-0"                                                                                                   >> $TMC_CONFIG
      echo "type:"                                                                                                                   >> $TMC_CONFIG
      echo "  kind: Cluster"                                                                                                         >> $TMC_CONFIG
      echo "  package: vmware.tanzu.manage.v1alpha1.cluster"                                                                         >> $TMC_CONFIG
      echo "  version: v1alpha1"                                                                                                     >> $TMC_CONFIG

      tmc cluster create -f $TMC_CONFIG > /tmp/error 2>&1; ret=$?
      if [ $ret -ne 0 ]; then 
        logMessages /tmp/error.log
        echo "ERROR: failed to build TKG Cluster with TMC on Azure"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ tmc cluster create -f $TMC_CONFIG"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => tmc cluster create -f $TMC_CONFIG"
        fi
        exit 1
      fi

      messagePrint " ▪ Cluster Status"         "creating ...."
      stt=""
      while [ "$stt" != "READY" ]; do
        stt=$(tmc cluster list -m $TDH_MANAGEMENT_CLUSTER --name $TDH_CLUSTER_NAME -o json 2>/dev/null | \
              jq -r '.clusters[].status.phase' 2>/dev/null | head -1)
  
        sleep 10
      done

      sleep 300
    fi
  fi

  if [ "$TDH_DEPLOYMENT_CLOUD" == "AWS" ]; then 
    stt=$(tmc cluster list -m $TDH_MANAGEMENT_CLUSTER -p $TDH_PROVISIONER_NAME --name $TDH_CLUSTER_NAME -o json 2>/dev/null | \
          jq -r '.clusters[].status.phase' 2>/dev/null) 

    if [ "$stt" == "CREATING" ]; then                                         
       echo "ERROR: Cluster is currently beeing crated, please try again later"; exit 1
    fi

    if [ "$stt" == "DELETING" ]; then                                         
       echo "ERROR: Cluster is currently on deleting, please try again later"; exit 1
    fi

    if [ "$stt" == "" -o "$stt" != "READY" ]; then
      KUBERNETES_VERSION=$(for n in $TMC_K8S_VERSIONS; do echo v$n; done | egrep "$TDH_TKGWC_KUBERNETES" | tail -1 | sed 's/^v//g') 

      [ "$TMC_KUBERNETES_VERSION" != "" ] && KUBERNETES_VERSION=$TMC_KUBERNETES_VERSION
      [ "$TDH_TKGWC_CONTROPLANE" == "3" ] && TMC_TKGWC_TEMPLATE="aws-ha" || TMC_TKGWC_TEMPLATE="default"

      # --- SEARCH KUBERNETES VERSIONS ---
      KUBERNETES_VERSION=$(tmcGetKubernetesImage $TDH_TKGWC_KUBERNETES) 

      if [ "$KUBERNETES_VERSION" == "" ]; then 
        echo "ERROR: No supported image found for Kubernetes version: $TDH_TKGWC_KUBERNETES"
        exit
      fi

      messageTitle "TMC Create Workload Cluster"     
      messagePrint " ▪ TMC Cluster Name"                 "$TDH_CLUSTER_NAME"
      messagePrint " ▪ TMC Cluster Group"                "$TDH_CLUSTER_GROUP"
      messagePrint " ▪ TMC Template"                     "$TMC_TKGWC_TEMPLATE"
      messagePrint " ▪ TMC Provisioner Name"             "$TDH_PROVISIONER_NAME"
      messagePrint " ▪ TKG Management Cluster"           "$TDH_MANAGEMENT_CLUSTER"
      messagePrint " ▪ TMC Account Name"                 "$TMC_ACCOUNT_NAME"
      messagePrint " ▪ Kubernetes Version"               "$TDH_TKGWC_KUBERNETES"
      messagePrint " ▪ Kubernetes Image"                 "$KUBERNETES_VERSION"

      TDH_TKGWC_WORKERNODES_MACHINE=$(nodeType $TDH_TKGWC_WORKERNODES_TYPE)
      TDH_TKGWC_CONTROPLANE_MACHINE=$(nodeType $TDH_TKGWC_CONTROPLANE_NODETYPE)
      messagePrint " ▪ TKG Cluster Worker Nodes"         "$TDH_TKGWC_WORKERNODES"
      messagePrint " ▪ TKG Cluster Worker Node Type"     "$TDH_TKGWC_WORKERNODES_TYPE ($TDH_TKGWC_WORKERNODES_MACHINE)"
      messagePrint " ▪ TKG Control Plane Nodes"          "$TDH_TKGWC_CONTROPLANE"
      messagePrint " ▪ TKG Control Plane Nodes Type"     "$TDH_TKGWC_CONTROPLANE_NODETYPE ($TDH_TKGWC_CONTROPLANE_MACHINE)"

      tmc cluster create -m $TDH_MANAGEMENT_CLUSTER -p $TDH_PROVISIONER_NAME \
                         --instance-type $TDH_TKGWC_WORKERNODES_MACHINE \
                         --ssh-key-name $TMC_SSH_KEY_NAME_AWS -c $TMC_ACCOUNT_NAME \
                         --worker-node-count $TDH_TKGWC_WORKERNODES \
                         -r $AWS_REGION -n $TDH_CLUSTER_NAME \
                         -g $TDH_CLUSTER_GROUP -t $TMC_TKGWC_TEMPLATE \
                         --version $KUBERNETES_VERSION --availability-zone $AWS_PRIMARY_AZ

      if [ $? -ne 0 ]; then 
        echo "ERROR: failed to build TKG Cluster on AWS"
        echo "       => tmc cluster create -m $TDH_MANAGEMENT_CLUSTER -p $TDH_PROVISIONER_NAME \\"
        echo "          --ssh-key-name $TMC_SSH_KEY_NAME_AWS -c $TMC_ACCOUNT_NAME \\"
        echo "          --worker-node-count $TDH_TKGWC_WORKERNODES \\"
        echo "          -r $AWS_REGION -n $TDH_CLUSTER_NAME -q $TMC_TKGWC_WORKERNODES \\"
        echo "          -g $TDH_CLUSTER_GROUP -t $TMC_TKGWC_TEMPLATE \\"
        echo "          --version $KUBERNETES_VERSION --availability-zone $AWS_PRIMARY_AZ"
        exit 1
      fi

      messagePrint " ▪ Cluster Status"         "creating ...."
      stt=""
      while [ "$stt" != "READY" ]; do
        stt=$(tmc cluster list -m $TDH_MANAGEMENT_CLUSTER --name $TDH_CLUSTER_NAME -o json 2>/dev/null | \
              jq -r '.clusters[].status.phase' 2>/dev/null | head -1)
  
        sleep 10
      done

      sleep 300
    fi
  fi

  if [ "$TDH_DEPLOYMENT_CLOUD" == "vSphere" ]; then 
    TDH_PROVISIONER_NAME=tanzu-demo-hub
    stt=$(tmc cluster list -m $TDH_MANAGEMENT_CLUSTER -p $TDH_PROVISIONER_NAME --name $TDH_CLUSTER_NAME -o json 2>/dev/null | \
          jq -r '.clusters[].status.phase' 2>/dev/null)

    if [ "$stt" == "DELETING" ]; then
       echo "ERROR: Cluster is currently on deleting, please try again later"; exit 1
    fi

    if [ "$stt" == "" -o "$stt" != "READY" ]; then
      messageTitle "TMC Create Workload Cluster"
      messagePrint " ▪ TMC Cluster Name"                 "$TDH_CLUSTER_NAME"
      messagePrint " ▪ TMC Cluster Group"                "$TDH_CLUSTER_GROUP"
      messagePrint " ▪ TMC Cluster Storage Class"        "$TDH_TKGWC_STORAGE_CLASS"
      messagePrint " ▪ TMC Template"                     "$TMC_TKGWC_TEMPLATE"
      messagePrint " ▪ TMC Provisioner Name"             "$TDH_PROVISIONER_NAME"
      messagePrint " ▪ TKG Management Cluster"           "$TDH_MANAGEMENT_CLUSTER"
      messagePrint " ▪ TMC Account Name"                 "$TMC_ACCOUNT_NAME"
      messagePrint " ▪ Kubernetes Version"               "$TDH_TKGWC_KUBERNETES"

      TDH_TKGWC_WORKERNODES_MACHINE=$(nodeType $TDH_TKGWC_WORKERNODES_TYPE)
      TDH_TKGWC_CONTROPLANE_MACHINE=$(nodeType $TDH_TKGWC_CONTROPLANE_NODETYPE)
      messagePrint " ▪ TKG Cluster Worker Nodes"         "$TDH_TKGWC_WORKERNODES"
      messagePrint " ▪ TKG Cluster Worker Node Type"     "$TDH_TKGWC_WORKERNODES_TYPE ($TDH_TKGWC_WORKERNODES_MACHINE)"
      messagePrint " ▪ TKG Control Plane Nodes"          "$TDH_TKGWC_CONTROPLANE"
      messagePrint " ▪ TKG Control Plane Nodes Type"     "$TDH_TKGWC_CONTROPLANE_NODETYPE ($TDH_TKGWC_CONTROPLANE_MACHINE)"


      # List of Tanzu Kubernetes Releases
      # https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-292482C2-A5FA-44B5-B26E-F887A91BB19D.html
      tmc cluster create -t tkgs -m $TDH_MANAGEMENT_CLUSTER -p $TDH_PROVISIONER_NAME \
                           --worker-instance-type $TDH_TKGWC_WORKERNODES_MACHINE \
                           --instance-type $TDH_TKGWC_CONTROPLANE_MACHINE \
                           --worker-node-count $TDH_TKGWC_WORKERNODES \
                           --storage-class $TDH_TKGWC_STORAGE_CLASS \
                           --default-storage-class $TDH_TKGWC_STORAGE_CLASS \
                           -g $TDH_CLUSTER_GROUP --version $TDH_TKGWC_KUBERNETES -n $TDH_CLUSTER_NAME > /tmp/error.log 2>&1; ret=$?

      if [ $ret -ne 0 ]; then
        logMessages /tmp/error.log
        echo "ERROR: failed to build Cluster on vSphere"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ tmc cluster create -t tkgs -m $TDH_MANAGEMENT_CLUSTER -p $TDH_PROVISIONER_NAME \\"
          echo "                          --worker-instance-type $TDH_TKGWC_WORKERNODES_MACHINE \\"
          echo "                          --instance-type $INSTANCE_TYPE \\"
          echo "                          --default-storage-class $TDH_TKGWC_STORAGE_CLASS \\"
          echo "                          --worker-node-count $TDH_TKGWC_WORKERNODES \\"
          echo "                          --storage-class $TDH_TKGWC_STORAGE_CLASS \\"
          echo "                          -g $TDH_CLUSTER_GROUP --version $KUBERNETES_VERSION -n $TDH_CLUSTER_NAME"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => tmc cluster create -t tkgs -m $TDH_MANAGEMENT_CLUSTER -p $TDH_PROVISIONER_NAME \\"
          echo "                          --worker-instance-type $TDH_TKGWC_WORKERNODES_MACHINE \\"
          echo "                          --instance-type $INSTANCE_TYPE \\"
          echo "                          --default-storage-class $TDH_TKGWC_STORAGE_CLASS \\"
          echo "                          --worker-node-count $TDH_TKGWC_WORKERNODES \\"
          echo "                          --storage-class $TDH_TKGWC_STORAGE_CLASS \\"
          echo "              -g $TDH_CLUSTER_GROUP --version $KUBERNETES_VERSION -n $TDH_CLUSTER_NAM"
        fi
  
        exit 1
      fi

      messagePrint " ▪ Cluster Status"         "creating ...."
      stt=""
      while [ "$stt" != "READY" ]; do
        stt=$(tmc cluster list -m $TDH_MANAGEMENT_CLUSTER --name $TDH_CLUSTER_NAME -o json 2>/dev/null | \
              jq -r '.clusters[].status.phase' 2>/dev/null | head -1)
  
        sleep 10
      done
  
      sleep 300

    fi
  fi

  messageTitle "TMC Verify Cluster"     
  messagePrint " ▪ Cluster Name"           "$TDH_CLUSTER_NAME"
  messagePrint " ▪ TKG Management Cluster" "$TDH_MANAGEMENT_CLUSTER"
  messagePrint " ▪ Cluster Status"         "$stt"
  messagePrint " ▪ Provisioner Name"       "$TDH_PROVISIONER_NAME"
  messagePrint " ▪ Account Name"           "$TMC_ACCOUNT_NAME"
  echo "----------------------------------------------------------------------------------------"
  echo " TO-DELETE-THE-CLUSTER"
  echo " => tmc cluster delete -m $TDH_MANAGEMENT_CLUSTER -p $TDH_PROVISIONER_NAME $TDH_CLUSTER_NAME"
  echo "----------------------------------------------------------------------------------------"
}

alignStr() {
  printf "%-25s" "$1"
}

getTDHClusterCredentials() {
  cnt=$(kubectl config get-clusters | egrep -c "^$TDH_CLUSTER_NAME$")
  if [ $cnt -eq 0 ]; then 
    messageTitle "Request TKG Cluster Credentials from TMC"
    messagePrint " ▪ Cluster Name"           "$TDH_CLUSTER_NAME"
    messagePrint " ▪ Kubeconfig File"        "$HOME/.tanzu-demo-hub/config/${TDH_CLUSTER_NAME}.kubeconfig"
    messagePrint " ▪ Kubernetes Context"     "${TDH_CLUSTER_NAME}-admin@$TDH_CLUSTER_NAME"

    ret=1; cnt=0
    while [ $ret -ne 0 -a $cnt -lt 10 ]; do
      #tmc cluster auth kubeconfig get $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $TDH_MANAGEMENT_CLUSTER > /dev/null 2>&1; ret=$?
      tmc cluster auth admin-kubeconfig get $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $TDH_MANAGEMENT_CLUSTER > /dev/null 2>&1; ret=$?
      [ $ret -eq 0 ] && break
      sleep 60
      let cnt=cnt+1
    done
  
    if [ $ret -ne 0 ]; then 
      echo "ERROR: Failled to retrieve cluster credentials, aborting"
      echo "       => tmc cluster auth admin-kubeconfig get $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $TDH_MANAGEMENT_CLUSTER"
      exit 1
    fi
  
    rm -f /tmp/${TDH_CLUSTER_NAME}.kubeconfig; touch /tmp/${TDH_CLUSTER_NAME}.kubeconfig
    ret=1; cnt=0
    while [ ! -s /tmp/${TDH_CLUSTER_NAME}.kubeconfig -a $ret -ne 0 -a $cnt -lt 5 ]; do
      tmc cluster auth admin-kubeconfig get $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME \
          -m $TDH_MANAGEMENT_CLUSTER > /tmp/${TDH_CLUSTER_NAME}.kubeconfig 2>/dev/null; ret=$?
      [ $ret -eq 0 ] && break
      sleep 60
      let cnt=cnt+1
    done
  
    if [ $ret -ne 0 ]; then
      echo "ERROR: Failled to retrieve cluster credentials, aborting"
      echo "       => tmc cluster auth admin-kubeconfig get $TDH_CLUSTER_NAME -p $TDH_PROVISIONER_NAME -m $TDH_MANAGEMENT_CLUSTER"
      exit 1
    fi
 
    chmod 600 /tmp/${TDH_CLUSTER_NAME}.kubeconfig
    cp /tmp/${TDH_CLUSTER_NAME}.kubeconfig $HOME/.tanzu-demo-hub/config/${TDH_CLUSTER_NAME}.kubeconfig
    export KUBECONFIG=$HOME/.kube/config:/tmp/${TDH_CLUSTER_NAME}.kubeconfig
    kubectl config view --flatten > ~/.kube/config.new
    [ -f $HOME/.kube/config ] && mv $HOME/.kube/config $HOME/.kube/config.last
    [ -f $HOME/.kube/config.new ] && mv ~/.kube/config.new ~/.kube/config
    chmod 600 ~/.kube/config
    unset KUBECONFIG

    if [ "$TDH_DEPLOYMENT_CLOUD" == "vSphere" ]; then
      export KUBECTL_VSPHERE_PASSWORD=$VSPHERE_TKGS_VCENTER_PASSWORD
      kubectl vsphere login --insecure-skip-tls-verify --server $VSPHERE_TKGS_SUPERVISOR_CLUSTER \
        -u $VSPHERE_TKGS_VCENTER_ADMIN --tanzu-kubernetes-cluster-name $TDH_CLUSTER_NAME \
        --tanzu-kubernetes-cluster-namespace tanzu-demo-hub

      kubectl get clusterrolebinding -o json > /tmp/output.json
      nam=$(jq -r --arg key "default-tkg-admin-privileged-binding" '.items[].metadata | select(.name == $key).name' /tmp/output.json)
      if [ "$nam" != "default-tkg-admin-privileged-binding" ]; then
        kubectl create clusterrolebinding default-tkg-admin-privileged-binding \
            --clusterrole=psp:vmware-system-privileged --group=system:authenticated
      fi

      # --- PATCH STORAGE CLASS
      kubectl patch storageclass tanzu -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
      kubectl config use-context ${TDH_CLUSTER_NAME}-admin@$TDH_CLUSTER_NAME
    else
      kubectl config use-context ${TDH_CLUSTER_NAME}-admin@$TDH_CLUSTER_NAME
    fi

    kubectl get clusterrolebinding -o json > /tmp/output.json
    nam=$(jq -r --arg key "tanzu-demo-hub-privileged-cluster-role-binding" '.items[].metadata | select(.name == $key).name' /tmp/output.json)
    if [ "$nam" != "tanzu-demo-hub-privileged-cluster-role-binding" ]; then
      kubectl create clusterrolebinding tanzu-demo-hub-privileged-cluster-role-binding \
              --clusterrole=vmware-system-tmc-psp-privileged --group=system:authenticated
    fi
  
    #echo "----------------------------------------------------------------------------------------"
    #echo "export KUBECONFIG=/tmp/${TDH_CLUSTER_NAME}.kubeconfig"
    #echo "kubectl config get-contexts"
    #echo "----------------------------------------------------------------------------------------"
  else
    messageTitle "Verify TKG Cluster Credentials"
    messagePrint " ▪ Cluster Name"           "$TDH_CLUSTER_NAME"
    messagePrint " ▪ Kubeconfig File"        "/tmp/${TDH_CLUSTER_NAME}.kubeconfig"
    messagePrint " ▪ Kubernetes Context"     "${TDH_CLUSTER_NAME}-admin@$TDH_CLUSTER_NAME"

    kubectl config use-context ${TDH_CLUSTER_NAME}-admin@$TDH_CLUSTER_NAME
  fi
}

verifyDockerRegistry() {
a=1
}

verifyVMwareRegistry() {
a=1
}

installTAP() {
  NAMESPACE=tap

  if [ "$TDH_SERVICE_TAP" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_TAP        "false"
    return
  fi

  NAMESPACE=tap-install
  export INSTALL_REGISTRY_USERNAME=$TDH_REGISTRY_VMWARE_USER
  export INSTALL_REGISTRY_PASSWORD=$TDH_REGISTRY_VMWARE_PASS
  export INSTALL_REGISTRY_HOSTNAME=registry.tanzu.vmware.com

  createNamespace $NAMESPACE > /dev/null 2>&1
  cnt=$(cmdLoop kubectl get secrets -n $NAMESPACE | egrep -c "^tap-registry") 

  # --- CREATE A REGISTRY SECRET ---
  tanzu secret registry add tap-registry \
  --username ${INSTALL_REGISTRY_USERNAME} --password ${INSTALL_REGISTRY_PASSWORD} \
  --server ${INSTALL_REGISTRY_HOSTNAME} \
  --export-to-all-namespaces --yes --namespace $NAMESPACE > /tmp/error.log 2>&1; ret=$? 

      if [ $ret -ne 0 ]; then
        logMessages /tmp/error.log
        echo "ERROR: failed to create registry secret"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ tanzu secret registry add tap-registry \\"
          echo "                          --username ${INSTALL_REGISTRY_USERNAME} --password ${INSTALL_REGISTRY_PASSWORD} \\"
          echo "                          --server ${INSTALL_REGISTRY_HOSTNAME} \\"
          echo "                          --export-to-all-namespaces --yes --namespace $NAMESPACE"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => tanzu secret registry add tap-registry \\"
          echo "                --username ${INSTALL_REGISTRY_USERNAME} --password ${INSTALL_REGISTRY_PASSWORD} \\"
          echo "                --server ${INSTALL_REGISTRY_HOSTNAME} \\"
          echo "                --export-to-all-namespaces --yes --namespace $NAMESPACE"
        fi

        exit 1
      fi

exit

  if [ 2 -eq 1 ]; then 

    # --- TANZU BUILD SERVICE CLI (kp) BINARY ---
    TDH_BNDL_NAM="tanzu-application-platform"
    TDH_BNDL_VER=$TDH_SERVICE_TAP_VERSION
    TDH_FILE_TAG="kp"
    TDH_FILE_STR="${TDH_FILE_TAG}-linux"
    TDH_FILE_VER=$TDH_SERVICE_BUILD_SERVICE_KP
    TDH_FILE_NAM="$TDH_FILE_TAG-linux-$TDH_FILE_VER"
  
    if [ ! -e $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM ]; then
      downloadFromPivnet $TDH_BNDL_NAM $TDH_BNDL_VER "$TDH_FILE_VER" "${TDH_FILE_STR}"
      if [ -f /tmp/$TDH_FILE_NAM ]; then
        [ ! -d $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER ] && mkdir -p $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER
        mv /tmp/$TDH_FILE_NAM $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER
        cp $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM $TDH_DOCKER_FILES/$TDH_FILE_TAG
        chmod a+x $TDH_DOCKER_FILES/$TDH_FILE_TAG
      fi
  
      REBUILD_REQUIRED_SOFTWARE=1       ## REBUILD REQUIRED BECAUSE OF SOFTWARE CHANGE
    else
      cp $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM $TDH_DOCKER_FILES/$TDH_FILE_TAG
      chmod a+x $TDH_DOCKER_FILES/$TDH_FILE_TAG
    fi
  fi

  messageTitle "Tanzu Application Platform (TAP)"
  cnt=0
  if [ $cnt -eq 0 ]; then
    TDH_SERVICE_TAP_VERSION=$(getConfigMap tanzu-demo-hub TDH_SERVICE_TAP_VERSION)

    # --- VERIFY SERVICE CONFIGURATION ---
    checkKubernetesServices registry_vmware
    checkKubernetesServices registry_docker
    checkKubernetesServices github

    export TDH_REGISTRY_VMWARE_NAME=registry.tanzu.vmware.com
    messagePrint " ▪ Verify VMware Registry Access" "$TDH_REGISTRY_VMWARE_NAME"
    messagePrint "   Registry User"                 "$TDH_REGISTRY_VMWARE_USER"
    messagePrint "   Registry Password"             "$(maskPassword \"$TDH_REGISTRY_VMWARE_PASS\")"

    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      docker login $TDH_REGISTRY_VMWARE_NAME -u $TDH_REGISTRY_VMWARE_USER -p $TDH_REGISTRY_VMWARE_PASS > /dev/null 2>&1; ret=$?
      sleep 10
      let cnt=cnt+1
    done

    if [ $ret -ne 0 ]; then
      echo "ERROR: Docker login does not work 2"
      echo "       => docker login $TDH_REGISTRY_VMWARE_NAME -u $TDH_REGISTRY_VMWARE_USER -p $TDH_REGISTRY_VMWARE_PASS"; exit 1
    fi

    messagePrint " ▪ Verify Harbor Registry Access" "$TDH_HARBOR_REGISTRY_DNS_HARBOR"
    messagePrint "   Registry User"                 "admin"
    messagePrint "   Registry Password"             "$(maskPassword \"$TDH_HARBOR_REGISTRY_ADMIN_PASSWORD\")"

    DNS_DOMAIN=$(getConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_LB_DOMAIN)
    DNS_HARBOR=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR)
    TDH_HARBOR_REGISTRY_DNS_HARBOR=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR)
    TDH_HARBOR_REGISTRY_ADMIN_PASSWORD=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_ADMIN_PASSWORD)

    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 50 ]; do
      docker login $TDH_HARBOR_REGISTRY_DNS_HARBOR -u admin -p $TDH_HARBOR_REGISTRY_ADMIN_PASSWORD > /dev/null 2>&1; ret=$?
      sleep 10
      let cnt=cnt+1
    done

    if [ $ret -ne 0 ]; then
      echo "ERROR: Docker login does not work 5'"
      echo "       => docker login $TDH_HARBOR_REGISTRY_DNS_HARBOR -u admin -p $TDH_HARBOR_REGISTRY_ADMIN_PASSWORD"; exit 1
    fi

    TAP_CONFIG=/tmp/tap_config.yaml
    echo ""                                                                                                                       >  $TAP_CONFIG
    echo "profile: full"                                                                                                          >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG
    echo "buildservice:"                                                                                                          >> $TAP_CONFIG
    echo "  kp_default_repository: $DNS_HARBOR/library/tap-build-service"                                                         >> $TAP_CONFIG
    echo "  kp_default_repository_username: admin"                                                                                >> $TAP_CONFIG
    echo "  kp_default_repository_password: $TDH_HARBOR_REGISTRY_ADMIN_PASSWORD"                                                  >> $TAP_CONFIG
    echo "  tanzunet_username: $TDH_REGISTRY_VMWARE_USER"                                                                         >> $TAP_CONFIG
    echo "  tanzunet_password: $TDH_REGISTRY_VMWARE_PASS"                                                                         >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG
    echo "supply_chain: basic"                                                                                                    >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG
    echo "ootb_supply_chain_basic:"                                                                                               >> $TAP_CONFIG
    echo "  registry:"                                                                                                            >> $TAP_CONFIG
    echo "    server: $DNS_HARBOR"                                                                                                >> $TAP_CONFIG
    echo "    repository: \"tap-supply-chain\""                                                                                   >> $TAP_CONFIG
    echo "    "                                                                                                                   >> $TAP_CONFIG
    echo "ootb_supply_chain_testing:"                                                                                             >> $TAP_CONFIG
    echo " registry:"                                                                                                             >> $TAP_CONFIG
    echo "  server: \"source-lab.io\""                                                                                            >> $TAP_CONFIG
    echo "  repository: \"tap/supply-chain\""                                                                                     >> $TAP_CONFIG
    echo "  "                                                                                                                     >> $TAP_CONFIG
    echo "ootb_supply_chain_testing_scanning:"                                                                                    >> $TAP_CONFIG
    echo " registry:"                                                                                                             >> $TAP_CONFIG
    echo "  server: \"source-lab.io\""                                                                                            >> $TAP_CONFIG
    echo "  repository: \"tap/supply-chain\""                                                                                     >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG
    echo "grype:"                                                                                                                 >> $TAP_CONFIG
    echo "  targetImagePullSecret: \"supply-chain\""                                                                              >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG
    echo "contour:"                                                                                                               >> $TAP_CONFIG
    echo "  infrastructure_provider: aws"                                                                                         >> $TAP_CONFIG
    echo "  envoy:"                                                                                                               >> $TAP_CONFIG
    echo "    service:"                                                                                                           >> $TAP_CONFIG
    echo "      aws:"                                                                                                             >> $TAP_CONFIG
    echo "        LBType: nlb"                                                                                                    >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG
    echo "tap_gui:"                                                                                                               >> $TAP_CONFIG
    echo "  service_type: LoadBalancer"                                                                                           >> $TAP_CONFIG
    echo "  #ingressEnabled: \"true\""                                                                                            >> $TAP_CONFIG
    echo "  ingressDomain: tap-gui.$DNS_DOMAIN"                                                                                   >> $TAP_CONFIG
    echo "  app_config:"                                                                                                          >> $TAP_CONFIG
    echo "    organization:"                                                                                                      >> $TAP_CONFIG
    echo "      name: tanzu-demo-hub"                                                                                             >> $TAP_CONFIG
    echo "    app:"                                                                                                               >> $TAP_CONFIG
    echo "      title: Tanzu Demo Hub"                                                                                            >> $TAP_CONFIG
    echo "      baseUrl: http://xyz.$DNS_DOMAIN:7000"                                                                             >> $TAP_CONFIG
    echo "    integrations:"                                                                                                      >> $TAP_CONFIG
    echo "      github:"                                                                                                          >> $TAP_CONFIG
    echo "      - host: github.com"                                                                                               >> $TAP_CONFIG
    echo "        token: $git_token"                                                                                              >> $TAP_CONFIG
    echo "    catalog:"                                                                                                           >> $TAP_CONFIG
    echo "      locations:"                                                                                                       >> $TAP_CONFIG
    echo "        - type: url"                                                                                                    >> $TAP_CONFIG
    echo "          target: $catalog_info"                                                                                        >> $TAP_CONFIG
    echo "    backend:"                                                                                                           >> $TAP_CONFIG
    echo "        baseUrl: http://xyz.$DNS_DOMAIN:7000:7000"                                                                      >> $TAP_CONFIG
    echo "        cors:"                                                                                                          >> $TAP_CONFIG
    echo "            origin: http://xyz.$DNS_DOMAIN:7000:7000"                                                                   >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG
    echo "cnrs:"                                                                                                                  >> $TAP_CONFIG
    echo "  domain_name: $DNS_DOMAIN"                                                                                             >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG
    echo "image_policy_webhook:"                                                                                                  >> $TAP_CONFIG
    echo "   allow_unmatched_images: true"                                                                                        >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG
    echo "learningcenter:"                                                                                                        >> $TAP_CONFIG
    echo "  ingressDomain: learn.$DNS_DOMAIN"                                                                                     >> $TAP_CONFIG
    echo "  storageClass: \"default\""                                                                                            >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG
    echo "tap_gui:"                                                                                                               >> $TAP_CONFIG
    echo "  service_type: LoadBalancer"                                                                                           >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG
    echo "ceip_policy_disclosed: true"                                                                                            >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG
    echo "accelerator:"                                                                                                           >> $TAP_CONFIG
    echo "  service_type: \"LoadBalancer\""                                                                                       >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG
    echo "appliveview:"                                                                                                           >> $TAP_CONFIG
    echo "  connector_namespaces: [default]"                                                                                      >> $TAP_CONFIG
    echo "  service_type: LoadBalancer"                                                                                           >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG
    echo "metadata_store:"                                                                                                        >> $TAP_CONFIG
    echo "  app_service_type: LoadBalancer"                                                                                       >> $TAP_CONFIG
    echo ""                                                                                                                       >> $TAP_CONFIG

echo "TAP_CONFIG:$TAP_CONFIG"

  fi
} 


installSpringCloudGateway() {
  NAMESPACE=spring-cloud-gateway

  if [ "$TDH_SERVICE_SPRING_CLOUD_GATEWAY" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_SPRING_CLOUD_GATEWAY        "false"
    return
  fi

  messageTitle "Spring Cloud Gateway"
  cnt=$(kubectl get pods -A | egrep -c "^$NAMESPACE")
  if [ $cnt -eq 0 ]; then
    TBS_FILE="/tmp/spring-cloud-gateway-k8s-$TDH_SERVICE_SPRING_CLOUD_GATEWAY_VERSION.tgz"
    [ -f $TBS_FILE ] && rm -f $TBS_FILE

    # --- DOWNLOAD BUILD_SERVICE BINARIES ---
    downloadFromPivnet spring-cloud-gateway-for-kubernetes $TDH_SERVICE_SPRING_CLOUD_GATEWAY_VERSION \
        "$TDH_SERVICE_SPRING_CLOUD_GATEWAY_VERSION" "Spring Cloud Gateway for Kubernetes Installer"

    [ -d /tmp/spring-cloud-gateway ] && rm -rf /tmp/spring-cloud-gateway
    mkdir -p /tmp/spring-cloud-gateway

    tar xfz $TBS_FILE -C /tmp/spring-cloud-gateway; ret=$? 
    if [ $ret -ne 0 ]; then
      logMessages /tmp/error.log
      echo "ERROR: Failed to unpack $TBS_FILE"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ tar xfz $TBS_FILE -C /tmp/spring-cloud-gateway"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => tar xfz $TBS_FILE -C /tmp/spring-cloud-gateway"
      fi

      exit 1
    fi

    messagePrint " ▪ Verify VMware Registry Access" "$TDH_REGISTRY_VMWARE_NAME"
    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      docker login $TDH_REGISTRY_VMWARE_NAME -u $TDH_REGISTRY_VMWARE_USER -p $TDH_REGISTRY_VMWARE_PASS > /dev/null 2>&1; ret=$?
      sleep 60
      let cnt=cnt+1
    done
  
    if [ $ret -ne 0 ]; then
      echo "ERROR: Docker login does not work 3"
      echo "       => docker login $TDH_REGISTRY_VMWARE_NAME -u $TDH_REGISTRY_VMWARE_USER -p $TDH_REGISTRY_VMWARE_PASS"; exit 1
    fi

    #########################################################################################################################
    ######################################## INSTALL SPRING CLOUD GATEWAY ###################################################
    #########################################################################################################################

    if [ "$TDH_SERVICE_REGISTRY_HARBOR" == "true" ]; then
      DNS_HARBOR=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR)
      TDH_HARBOR_REGISTRY_DNS_HARBOR=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR)
      TDH_HARBOR_REGISTRY_ADMIN_PASSWORD=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_ADMIN_PASSWORD)
      messagePrint " ▪ Verify Harbor Registry Access" "$TDH_HARBOR_REGISTRY_DNS_HARBOR"
  
      cnt=0; ret=1
      while [ $ret -ne 0 -a $cnt -lt 5 ]; do
        docker login $TDH_HARBOR_REGISTRY_DNS_HARBOR -u admin -p $TDH_HARBOR_REGISTRY_ADMIN_PASSWORD > /dev/null 2>&1; ret=$?
        sleep 10
        let cnt=cnt+1
      done
  
      if [ $ret -ne 0 ]; then
        echo "ERROR: Docker login does not work 11"
        echo "       => docker login $TDH_HARBOR_REGISTRY_DNS_HARBOR -u admin -p $TDH_HARBOR_REGISTRY_ADMIN_PASSWORD"; exit 1
      fi
  
      cmdLoop kubectl get ns $NAMESPACE > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        messagePrint " ▪ Relocate the images" "$DNS_HARBOR/library/spring-cloud-gateway"
        cd /tmp/spring-cloud-gateway/spring-cloud-gateway-k8s-$TDH_SERVICE_SPRING_CLOUD_GATEWAY_VERSION

        ./scripts/relocate-images.sh $DNS_HARBOR/library/spring-cloud-gateway > /dev/null 2>&1
        if [ $? -ne 0 ]; then
          echo "ERROR: Image Relocation failed"
          echo "       => cd /tmp/spring-cloud-gateway/spring-cloud-gateway-k8s-$TDH_SERVICE_SPRING_CLOUD_GATEWAY_VERSION"
          echo "       => scripts/relocate-images.sh $DNS_HARBOR/library/spring-cloud-gateway" 
        fi

        messagePrint " ▪ Deploy Spring Cloud Gateway" "$NAMESPACE"
        nam=""; cnt=0
        while [ "$nam" != "scg-operator" -a $cnt -lt 5 ]; do
          cd /tmp/spring-cloud-gateway/spring-cloud-gateway-k8s-$TDH_SERVICE_SPRING_CLOUD_GATEWAY_VERSION
          ./scripts/install-spring-cloud-gateway.sh > /dev/null 2>&1
          if [ $? -ne 0 ]; then
            echo "ERROR: Image Relocation failed"
            echo "       => cd /tmp/spring-cloud-gateway/spring-cloud-gateway-k8s-$TDH_SERVICE_SPRING_CLOUD_GATEWAY_VERSION"
            echo "       => ./scripts/install-spring-cloud-gateway.sh"
          fi

          cmdLoop kubectl -n $NAMESPACE get deployment.apps/scg-operator -o json > /tmp/output.json
          nam=$(jq -r '.metadata.name' /tmp/output.json) 
          [ "$nam" == "scg-operator" ] && break
echo "... Deploy Spring Cloud Gateway: $nam"
     
          sleep 30
          let cnt=cnt+1
        done

        # --- WAIT FOR PODS TO GET STARTED ---
        runningPods $NAMESPACE
      fi
    else
      messageTitle "Verify Spring Cloud Gateway"
    fi
  fi
}

installArgoCD() {
  NAMESPACE=argo-cd
  HELMCHART=argo-cd

  if [ "$TDH_SERVICE_ARGOCD" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_ARGOCD        "false"
    deleteHelmChart $NAMESPACE $HELMCHART
    return
  fi

  TDH_DOMAIN=$(getConfigMap tanzu-demo-hub TDH_DOMAIN)
  TDH_ENVNAME=$(getConfigMap tanzu-demo-hub TDH_ENVNAME)
  DOMAIN=$(getConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_LB_DOMAIN)

  messageTitle "ArgoCD Continuous Delivery for Kubernetes"
  cnt=$(helm list -q -n $NAMESPACE | egrep -c "^argo-cd")
  if [ $cnt -eq 0 ]; then
    messagePrint " ▪ Add the Bintnami Repository"   "https://charts.bitnami.com/bitnami"
    helmRepoAdd bitnami https://charts.bitnami.com/bitnami

    # --- CREATE NAMESPACE ---
    messagePrint " ▪ Create Namespace" "$NAMESPACE"
    createNamespace $NAMESPACE > /dev/null 2>&1

    # --- COPY SECRET TO NAMESPACE ---
    messagePrint " ▪ Copy TLS Secret" "tanzu-demo-hub-tls"
    copySecretObject default $NAMESPACE tanzu-demo-hub-tls
    #kubectl get secret tanzu-demo-hub-tls --namespace=default  -oyaml | grep -v '^\s*namespace:\s' | \
    #kubectl apply --namespace=$NAMESPACE -f - > /dev/null 2>&1

    HELM_VALUES=/tmp/helm_argocd_values.yaml

    echo "global:"                                                                                                                >  $HELM_VALUES
    echo "  imageRegistry: docker.io"                                                                                             >> $HELM_VALUES
    echo "  imagePullSecrets:"                                                                                                    >> $HELM_VALUES
    echo "    - tdh-docker-repo"                                                                                                  >> $HELM_VALUES
    echo ""                                                                                                                       >> $HELM_VALUES
    echo "config:"                                                                                                                >> $HELM_VALUES
    echo "  secret:"                                                                                                              >> $HELM_VALUES
    echo "    argocdServerAdminPassword: admin"                                                                                   >> $HELM_VALUES
    echo ""                                                                                                                       >> $HELM_VALUES
    echo "service:"                                                                                                               >> $HELM_VALUES
    echo "  type: ClusterIP"                                                                                                      >> $HELM_VALUES
    echo ""                                                                                                                       >> $HELM_VALUES
    echo "server:"                                                                                                                >> $HELM_VALUES
    echo "  insecure: true"                                                                                                       >> $HELM_VALUES
    echo "  ingress:"                                                                                                             >> $HELM_VALUES
    echo "    enabled: true"                                                                                                      >> $HELM_VALUES
    echo "    certManager: false"                                                                                                 >> $HELM_VALUES
    echo "    annotations:"                                                                                                       >> $HELM_VALUES
    echo "      kubernetes.io/ingress.class: contour"                                                                             >> $HELM_VALUES
    echo "      ingress.kubernetes.io/force-ssl-redirect: \"true\""                                                               >> $HELM_VALUES
    echo "      ingress.kubernetes.io/ssl-passthrough: \"true\""                                                                  >> $HELM_VALUES
    echo "    path: /"                                                                                                            >> $HELM_VALUES
    echo "    hostname: argocd.$DOMAIN"                                                                                           >> $HELM_VALUES
    echo "    tls: true"                                                                                                          >> $HELM_VALUES
    echo "    extraTls:"                                                                                                          >> $HELM_VALUES
    echo "    - hosts:"                                                                                                           >> $HELM_VALUES
    echo "        - argocd.$DOMAIN"                                                                                               >> $HELM_VALUES
    echo "      secretName: tanzu-demo-hub-tls"                                                                                   >> $HELM_VALUES

    export TZ=UTC
    messagePrint " ▪ Deploy the ArgoCD Helm Chart" "argo-cd"
    helm install argo-cd --namespace $NAMESPACE bitnami/argo-cd -f $HELM_VALUES --wait --wait-for-jobs --timeout 10m > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: Failed to deploy ArgoCD Helm Chart"
      echo "       => helm install argo-cd --namespace $NAMESPACE bitnami/argo-cd -f $HELM_VALUES --wait"
      exit 1
    fi

    messagePrint " ▪ ArgoCD Dashboard deployed"        "https://argocd.$DOMAIN"

    messagePrint " ▪ Create Serviceaccount"              "argocd"
    kubectl delete serviceaccount argocd > /dev/null 2>&1
    kubectl create serviceaccount argocd > /dev/null 2>&1
  fi

  cmdLoop helm ls -n $NAMESPACE -o json > /tmp/output.json
  if [ -s /tmp/output.json ]; then
    ver=$(jq -r --arg chart "argo-cd" '.[] | select(.name == $chart).app_version' /tmp/output.json)
    crt=$(jq -r --arg chart "argo-cd" '.[] | select(.name == $chart).chart' /tmp/output.json)
    stt=$(jq -r --arg chart "argo-cd" '.[] | select(.name == $chart).status' /tmp/output.json)
    dat=$(jq -r --arg chart "argo-cd" '.[] | select(.name == $chart).updated' /tmp/output.json)
  else
    ver=""; crt=""; stt=""; dat=""
  fi

  messagePrint " ▪ Verify ArgoCD"                "$crt"
  messagePrint "   Helm Chart Name"                "argo-cd"
  messagePrint "   Helm Chart Version"             "$ver"
  messagePrint "   Helm Chart Status"              "$stt"
  messagePrint "   Helm Chart Installed/Updated"   "$dat"
  messagePrint "   ArgoCD Internal Name"         "argocd.argo-cd.svc.cluster.local"
  messagePrint "   ArgoCD Management Portal"     "https://argocd.$DOMAIN"

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_ARGOCD             "true"
}

installJenkins() {
  NAMESPACE=jenkins
  HELMCHART=jenkins

  if [ "$TDH_SERVICE_JENKINS" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_JENKINS        "false"
    deleteHelmChart $NAMESPACE $HELMCHART
    return
  fi

  TDH_DOMAIN=$(getConfigMap tanzu-demo-hub TDH_DOMAIN)
  TDH_ENVNAME=$(getConfigMap tanzu-demo-hub TDH_ENVNAME)
  DOMAIN=$(getConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_LB_DOMAIN)

  messageTitle "Jenkins Automation Server"
  cnt=$(helm list -q -n $NAMESPACE | egrep -c "^jenkins")
  if [ $cnt -eq 0 ]; then
    messagePrint " ▪ Add the Bintnami Repository"   "https://charts.bitnami.com/bitnami"
    helmRepoAdd bitnami https://charts.bitnami.com/bitnami

    # --- CREATE NAMESPACE ---
    messagePrint " ▪ Create Namespace" "$NAMESPACE"
    createNamespace $NAMESPACE > /dev/null 2>&1

    # --- COPY SECRET TO NAMESPACE ---
    messagePrint " ▪ Copy TLS Secret" "tanzu-demo-hub-tls"
    copySecretObject default $NAMESPACE tanzu-demo-hub-tls
    #kubectl get secret tanzu-demo-hub-tls --namespace=default  -oyaml | grep -v '^\s*namespace:\s' | \
    #kubectl apply --namespace=$NAMESPACE -f - > /dev/null 2>&1

    HELM_VALUES=/tmp/helm_jenkins_values.yaml

    echo "global:"                                                                                                              >  $HELM_VALUES
    echo "  imageRegistry: docker.io"                                                                                           >> $HELM_VALUES
    echo "  imagePullSecrets:"                                                                                                  >> $HELM_VALUES
    echo "    - tdh-docker-repo"                                                                                                >> $HELM_VALUES
    echo ""                                                                                                                     >> $HELM_VALUES
    echo "service:"                                                                                                             >> $HELM_VALUES
    echo "  type: ClusterIP"                                                                                                    >> $HELM_VALUES
    echo ""                                                                                                                     >> $HELM_VALUES
    echo "jenkinsUser: admin"                                                                                                   >> $HELM_VALUES
    echo "jenkinsPassword: $TDH_HARBOR_ADMIN_PASSWORD"                                                                          >> $HELM_VALUES
    echo ""                                                                                                                     >> $HELM_VALUES
    echo "ingress:"                                                                                                             >> $HELM_VALUES
    echo "  enabled: true"                                                                                                      >> $HELM_VALUES
    echo "  certManager: false"                                                                                                 >> $HELM_VALUES
    echo "  annotations:"                                                                                                       >> $HELM_VALUES
    echo "    kubernetes.io/ingress.class: contour"                                                                             >> $HELM_VALUES
    echo "    ingress.kubernetes.io/force-ssl-redirect: \"true\""                                                               >> $HELM_VALUES
    echo "  path: /"                                                                                                            >> $HELM_VALUES
    echo "  hostname: jenkins.$DOMAIN"                                                                                          >> $HELM_VALUES
    echo "  tls: true"                                                                                                          >> $HELM_VALUES
    echo "  extraTls:"                                                                                                          >> $HELM_VALUES
    echo "  - hosts:"                                                                                                           >> $HELM_VALUES
    echo "      - jenkins.$DOMAIN"                                                                                              >> $HELM_VALUES
    echo "    secretName: tanzu-demo-hub-tls"                                                                                   >> $HELM_VALUES

#gaga Jenkins
    messagePrint " ▪ Deploy the Jenkins Helm Chart" "$HELMCHART"
    cnt=0; ret=1; stt="notdeployed"
    while [ $ret -ne 0 -a $cnt -lt 7 -a "$stt" != "deployed" ]; do
      helm install $HELMCHART --namespace $NAMESPACE bitnami/$HELMCHART -f $HELM_VALUES --create-namespace --wait --wait-for-jobs --timeout 10m > /tmp/error.log 2>&1; ret=$?
      #helm install $HELMCHART --namespace $NAMESPACE bitnami/$HELMCHART -f $HELM_VALUES --create-namespace --wait --wait-for-jobs --timeout 20m; ret=$?

      # --- WAIT FOR PODS TO GET STARTED ---
      runningPods $NAMESPACE

      cmdLoop helm ls -n $NAMESPACE -o json > /tmp/output.json
      stt=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).status' /tmp/output.json)

      [ $ret -eq 0 -a "$stt" == "deployed" ] && break

      # --- DELTE FAILED HELM CHART ---
      deleteHelmChart $NAMESPACE $HELMCHART > /dev/null 2>&1
      deleteNamespace $NAMESPACE > /dev/null 2>&1
      
      let cnt=cnt+1
      sleep 60
    done

    if [ $ret -ne 0 -o "$stt" != "deployed" ]; then
      logMessages /tmp/error.log
      echo "ERROR: Failed to deploy Jenkins Helm Chart"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ helm install jenkins --namespace $NAMESPACE bitnami/jenkins -f $HELM_VALUES"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => helm install jenkins --namespace $NAMESPACE bitnami/jenkins -f $HELM_VALUES"
      fi

      exit 1
    fi

    #java -jar ~/Downloads/jenkins-cli.jar -s https://jenkins.apps-contour.local.pcfsdu.com/ -auth user:Idh5k5eZts 2>&1"
    #curl -X GET https://jenkins.apps-contour.local.pcfsdu.com/job/sacha_xxx1/config.xml -u user:Idh5k5eZts -o mylocalconfig.xml

    #java -jar ~/Downloads/jenkins-cli.jar -s https://jenkins.apps-contour.local.pcfsdu.com/ -auth user:Idh5k5eZts get-job sacha_xxx1 > template.xml
    #java -jar ~/Downloads/jenkins-cli.jar -s https://jenkins.apps-contour.local.pcfsdu.com/ -auth user:Idh5k5eZts create-job sacha_xxx2 < template.xml

    messagePrint " ▪ Create Cluster Role Binding"                 "default:jenkins-operator"
    createClusterRoleBinding jenkins-tbs build-service-user-role  default:jenkins-tbs

    #DELETEME
    #kubectl delete clusterrolebinding jenkins-tbs > /dev/null 2>&1
    #kubectl create clusterrolebinding jenkins-tbs --clusterrole=build-service-user-role --serviceaccount=default:jenkins-tbs > /dev/null 2>&1

    messagePrint " ▪ Create Serviceaccount"              "jenkins-operator"
    createServiceAccount default jenkins-tbs

    #DELETEME
    #kubectl delete serviceaccount jenkins-tbs > /dev/null 2>&1
    #kubectl create serviceaccount jenkins-tbs > /dev/null 2>&1
  fi

  cmdLoop helm ls -n $NAMESPACE -o json > /tmp/output.json
  if [ -s /tmp/output.json ]; then
    ver=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).app_version' /tmp/output.json)
    crt=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).chart' /tmp/output.json)
    stt=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).status' /tmp/output.json)
    dat=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).updated' /tmp/output.json)
  else 
    ver=""; crt=""; stt=""; dat=""
  fi

  cmdLoop kubectl get secret --namespace jenkins jenkins -o json > /tmp/output.json
  if [ -s /tmp/output.json ]; then 
    jenkins_pass=$(jq -r '.data."jenkins-password"' /tmp/output.json | base64 --decode)
  fi

  #DELETEME
  #jenkins_pass=$(kubectl get secret --namespace jenkins jenkins -o jsonpath="{.data.jenkins-password}" | base64 --decode)

  messagePrint " ▪ Verify Jenkins"                 "$crt"
  messagePrint "   Helm Chart Name"                "jenkins"
  messagePrint "   Helm Chart Version"             "$ver"
  messagePrint "   Helm Chart Status"              "$stt"
  messagePrint "   Helm Chart Installed/Updated"   "$dat"
  messagePrint "   jenkins Internal Name"          "jenkins.jenkins.svc.cluster.local"
  messagePrint "   jenkins Dashboard URL"          "https://jenkins.$DOMAIN"
  messagePrint "   jenkins Dashboard Credentials"  "admin/$jenkins_pass"

  echo "   ------------------------------------------------------------------------------------------------------------------------------------------------------------------"
  echo "   Jenkins CLI:"
  echo "   wget https://jenkins.$DOMAIN/jenkins/jnlpJars/jenkins-cli.jar -O /tmp/jenkins-cli.jar"
  echo "   java -jar /tmp/jenkins-cli.jar -s https://jenkins.$DOMAIN -auth admin:$jenkins_pass"
  echo "  ------------------------------------------------------------------------------------------------------------------------------------------------------------------"

  #java -jar ~/Downloads/jenkins-cli.jar -s https://jenkins.apps-contour.local.pcfsdu.com/ -auth admin:Password12345 list-credentials-as-xml "system::system::jenkins"
  #java -jar ~/Downloads/jenkins-cli.jar -s https://jenkins.apps-contour.local.pcfsdu.com/ -auth admin:Password12345 get-credentials-as-xml "system::system::jenkins" _ 0c60eb5e-310b-4f57-8f77-62acff64921f 

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_JENKINS             "true"
}

# ------------------------------------------------------------------------------------------
# Function Name ......: installGitea
# Function Purpose ...: Install Gitea Helm Chargt
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: None
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
# References .........: https://gitea.com/gitea/helm-chart ............. Gitea Documentation
#                     : https://docs.gitea.io/en-us/api-usage/ ............. Gitea API Guide
#                     : https://try.gitea.io/api/swagger#/user/userCreateToken ..... Swagger
# ------------------------------------------------------------------------------------------
installGitea() {
  NAMESPACE=gitea
  HELMCHART=gitea
  HELMREPO=gitea-charts
  HELMREPO_URL=https://dl.gitea.io/charts/

  if [ "$TDH_SERVICE_KUBEAPPS" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_GITEA        "false"
    deleteHelmChart $NAMESPACE $HELMCHART
    return
  fi

  TDH_DOMAIN=$(getConfigMap tanzu-demo-hub TDH_DOMAIN)
  TDH_ENVNAME=$(getConfigMap tanzu-demo-hub TDH_ENVNAME)
  DOMAIN=$(getConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_LB_DOMAIN)

  messageTitle "Self-hosted Git service (Gitea)"
  cnt=$(helm list -q -n $NAMESPACE | egrep -c "^$HELMCHART")
  if [ $cnt -eq 0 ]; then
    # --- CREATE NAMESPACE ---
    messagePrint " ▪ Create Namespace" "$HELMCHART"
    deleteNamespace $NAMESPACE > /dev/null 2>&1
    createNamespace $NAMESPACE > /dev/null 2>&1

    # --- COPY SECRET TO NAMESPACE ---
    messagePrint " ▪ Copy TLS Secret" "tanzu-demo-hub-tls"
    copySecretObject default $NAMESPACE tanzu-demo-hub-tls
    #kubectl get secret tanzu-demo-hub-tls --namespace=default -oyaml | grep -v '^\s*namespace:\s' | \
    #kubectl apply --namespace=$NAMESPACE -f - > /dev/null 2>&1

    # --- DOCKER PULL SECRET ---
    dockerPullSecret $NAMESPACE

    # --- ADDING HELM REPO ---
    helmRepoAdd $HELMREPO $HELMREPO_URL

    if [ "$TDH_SERVICE_GITEA_VERSION" == "" -o "$TDH_SERVICE_GITEA_VERSION" == "latest" ]; then
      HELM_CHART_VERSION=$(helm search repo $HELMREPO/$HELMCHART | egrep "^$HELMREPO/$HELMCHART" | tail -1 | awk '{ print $2 }')
    else
      HELM_CHART_VERSION=$TDH_SERVICE_GITEA_VERSION
    fi

    HELM_VALUES=/tmp/gitea_kubeapps_values.yaml

    echo "images:"                                                                                                              >  $HELM_VALUES
    echo "  imageRegistry: docker.io"                                                                                           >> $HELM_VALUES
    echo "  imagePullPolicy: Always"                                                                                            >> $HELM_VALUES
    echo "  imagePullSecrets: tdh-docker-repo"                                                                                  >> $HELM_VALUES
    echo ""                                                                                                                     >> $HELM_VALUES
    echo "config:"                                                                                                              >> $HELM_VALUES
    echo "  disableRegistration: true"                                                                                          >> $HELM_VALUES
    echo ""                                                                                                                     >> $HELM_VALUES
    echo "gitea:"                                                                                                               >> $HELM_VALUES
    echo "  admin:"                                                                                                             >> $HELM_VALUES
    echo "    # NOTE: Password can not be changed its stays 'r8sA8CPHD9!bt6d'"                                                  >> $HELM_VALUES
    echo "    username: gitea_admin"                                                                                            >> $HELM_VALUES
    echo "    password: r8sA8CPHD9!bt6d"                                                                                        >> $HELM_VALUES
    echo "    email: \"gi@tea.com\""                                                                                            >> $HELM_VALUES
    echo ""                                                                                                                     >> $HELM_VALUES
    echo "ingress:"                                                                                                             >> $HELM_VALUES
    echo "  enabled: true"                                                                                                      >> $HELM_VALUES
    echo "  annotations:"                                                                                                       >> $HELM_VALUES
    echo "    kubernetes.io/ingress.class: contour"                                                                             >> $HELM_VALUES
    echo "    ingress.kubernetes.io/force-ssl-redirect: \"true\""                                                               >> $HELM_VALUES
    echo "  hosts:"                                                                                                             >> $HELM_VALUES
    echo "    - host: $HELMCHART.$DOMAIN"                                                                                       >> $HELM_VALUES
    echo "      paths:"                                                                                                         >> $HELM_VALUES
    echo "      - path: /"                                                                                                      >> $HELM_VALUES
    echo "        pathType: Prefix"                                                                                             >> $HELM_VALUES
    echo "  tls:"                                                                                                               >> $HELM_VALUES
    echo "    - secretName: tanzu-demo-hub-tls"                                                                                 >> $HELM_VALUES
    echo "      hosts:"                                                                                                         >> $HELM_VALUES
    echo "        - $HELMCHART.$DOMAIN"                                                                                         >> $HELM_VALUES

#gaga gitea
    messagePrint " ▪ Deploy the $HELMCHART Helm Chart" "$HELMCHART"
    cnt=0; ret=1; stt="notdeployed"
    while [ $ret -ne 0 -a $cnt -lt 5 -a "$stt" != "deployed" ]; do
      helm install $HELMCHART --namespace $NAMESPACE $HELMREPO/$HELMCHART -f $HELM_VALUES --version $HELM_CHART_VERSION --create-namespace --wait --wait-for-jobs --timeout 10m > /tmp/error.log 2>&1; ret=$?
      #helm install $HELMCHART --namespace $NAMESPACE $HELMREPO/$HELMCHART -f $HELM_VALUES --version $HELM_CHART_VERSION --create-namespace --wait --wait-for-jobs --timeout 10m; ret=$?

      # --- WAIT FOR PODS TO GET STARTED ---
      runningPods $NAMESPACE

      cmdLoop helm ls -n $NAMESPACE -o json > /tmp/output.json
      stt=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).status' /tmp/output.json)

      [ $ret -eq 0 -a "$stt" == "deployed" ] && break

      # --- DELTE FAILED HELM CHART ---
      deleteHelmChart $NAMESPACE $HELMCHART > /dev/null 2>&1
      deleteNamespace $NAMESPACE > /dev/null 2>&1

      let cnt=cnt+1
      sleep 30
    done

    if [ $ret -ne 0 -o "$stt" != "deployed" ]; then
      logMessages /tmp/error.log
      echo "ERROR: Failed to deploy $HELMCHART Helm Chart"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ helm install $HELMCHART --namespace $NAMESPACE $HELMREPO/$HELMCHART -f $HELM_VALUES --version $HELM_CHART_VERSION"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => helm install $HELMCHART --namespace $NAMESPACE $HELMREPO/$HELMCHART -f $HELM_VALUES --version $HELM_CHART_VERSION"
      fi

      exit 1
    fi
  fi

  cmdLoop helm ls -n $NAMESPACE -o json > /tmp/output.json
  if [ -s /tmp/output.json ]; then
    ver=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).app_version' /tmp/output.json)
    crt=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).chart' /tmp/output.json)
    stt=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).status' /tmp/output.json)
    dat=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).updated' /tmp/output.json)
  else
    ver=""; crt=""; stt=""; dat=""
  fi
  
  cmdLoop kubectl -n $HELMCHART exec -it gitea-0 -- $HELMCHART admin user change-password -u gitea_admin -p $TDH_HARBOR_ADMIN_PASSWORD > /tmp/error.log 2>&1; ret=$?
  if [ $ret -ne 0 ]; then
    logMessages /tmp/error.log
    echo "ERROR: Failed to change password for Gitea"
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ kubectl -n $HELMCHART exec -it gitea-0 -- $HELMCHART admin user change-password -u gitea_admin -p $TDH_HARBOR_ADMIN_PASSWORD"
      echo "          tdh-tools:/$ exit"
    else
      echo "       =>  kubectl -n $HELMCHART exec -it gitea-0 -- $HELMCHART admin user change-password -u gitea_admin -p $TDH_HARBOR_ADMIN_PASSWORD"
    fi

    exit 1
  fi

  messagePrint " ▪ Verify Helm Chart $HELMCHART"   "$crt"
  messagePrint "   Helm Chart Name"                "$HELMCHART"
  messagePrint "   Helm Chart Version"             "$ver"
  messagePrint "   Helm Chart Status"              "$stt"
  messagePrint "   Helm Chart Installed/Updated"   "$dat"
  messagePrint "   Gitea Internal Name"            "gitea.gitea.svc.cluster.local"
  messagePrint "   Gitea Management Portal"        "https://gitea.$DOMAIN"
  messagePrint "   Gitea Admin User"               "gitea_admin"
  messagePrint "   Gitea Admin Password"           "$TDH_HARBOR_ADMIN_PASSWORD"

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_GITEA             "true"
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_USER  "gitea_admin"
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_PASS  "$TDH_HARBOR_ADMIN_PASSWORD"
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_SERVER      "gitea.$DOMAIN"
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_SERVER_URL  "https://gitea.$DOMAIN"

  # --- CREATE AMDIN TOKEN FOR USER gitea_admin ---
  createGiteaAPItoken

  TDH_SERVICE_GITEA_ADMIN_TOKEN_ID=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_TOKEN_ID)
  messagePrint "   Gitea Admin Token ID"           "$TDH_SERVICE_GITEA_ADMIN_TOKEN_ID"
}

# ------------------------------------------------------------------------------------------
# Function Name ......: installKubeapps
# Function Purpose ...: Install Kubeapps Helm Chargt
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: None
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
installKubeapps() {
  NAMESPACE=kubeapps
  HELMCHART=kubeapps

  if [ "$TDH_SERVICE_KUBEAPPS" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_KUBEAPPS        "false"
    deleteHelmChart $NAMESPACE $HELMCHART
    return
  fi

  TDH_DOMAIN=$(getConfigMap tanzu-demo-hub TDH_DOMAIN)
  TDH_ENVNAME=$(getConfigMap tanzu-demo-hub TDH_ENVNAME)
  DOMAIN=$(getConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_LB_DOMAIN)

  messageTitle "Application Dashboard for Kubernetes (Kupeapps)"
  cnt=$(helm list -q -n $NAMESPACE | egrep -c "^$HELMCHART")
  if [ $cnt -eq 0 ]; then
    messagePrint " ▪ Add the Bintnami Repository"   "https://charts.bitnami.com/bitnami"

    # --- CREATE NAMESPACE ---
    messagePrint " ▪ Create Namespace" "$HELMCHART"
    deleteNamespace $NAMESPACE > /dev/null 2>&1
    createNamespace $NAMESPACE > /dev/null 2>&1

    # --- DOCKER PULL SECRET ---
    dockerPullSecret $NAMESPACE

    # --- COPY SECRET TO NAMESPACE ---
    messagePrint " ▪ Copy TLS Secret" "tanzu-demo-hub-tls"
    copySecretObject default $NAMESPACE tanzu-demo-hub-tls
    #kubectl get secret tanzu-demo-hub-tls --namespace=default -oyaml | grep -v '^\s*namespace:\s' | \
    #kubectl apply --namespace=$NAMESPACE -f - > /dev/null 2>&1

    # --- ADD GIT REPOSITORY ---
    helmRepoAdd bitnami https://charts.bitnami.com/bitnami

    if [ "$TDH_SERVICE_KUBEAPPS_VERSION" == "" -o "$TDH_SERVICE_KUBEAPPS_VERSION" == "latest" ]; then
      HELM_KUBEAPPS_VERSION=$(helm search repo bitnami/kubeapps | egrep "^bitnami/kubeapps" | tail -1 | awk '{ print $2 }')
    else
      HELM_KUBEAPPS_VERSION=$TDH_SERVICE_KUBEAPPS_VERSION
    fi

    HELM_VALUES=/tmp/helm_kubeapps_values.yaml

    echo "global:"                                                                                                              >  $HELM_VALUES
    echo "  imageRegistry: docker.io"                                                                                           >> $HELM_VALUES
    echo "  imagePullSecrets:"                                                                                                  >> $HELM_VALUES
    echo "    - tdh-docker-repo"                                                                                                >> $HELM_VALUES
    echo ""                                                                                                                     >> $HELM_VALUES
    echo "ingress:"                                                                                                             >> $HELM_VALUES
    echo "  enabled: true"                                                                                                      >> $HELM_VALUES
    echo "  annotations:"                                                                                                       >> $HELM_VALUES
    echo "    kubernetes.io/ingress.class: contour"                                                                             >> $HELM_VALUES
    echo "    ingress.kubernetes.io/force-ssl-redirect: \"true\""                                                               >> $HELM_VALUES
    echo "  path: /"                                                                                                            >> $HELM_VALUES
    echo "  hostname: kubeapps.$DOMAIN"                                                                                         >> $HELM_VALUES
    echo "  tls: true"                                                                                                          >> $HELM_VALUES
    echo "  extraTls:"                                                                                                          >> $HELM_VALUES
    echo "  - hosts:"                                                                                                           >> $HELM_VALUES
    echo "      - kubeapps.$DOMAIN"                                                                                             >> $HELM_VALUES
    echo "    secretName: tanzu-demo-hub-tls"                                                                                   >> $HELM_VALUES

    messagePrint " ▪ Deploy the Kubeapps Helm Chart" "kubeapps"
#gaga kubeapps
    cnt=0; ret=1; stt="notdeployed"
    while [ $ret -ne 0 -a $cnt -lt 5 -a "$stt" != "deployed" ]; do
      helm install $HELMCHART --namespace $NAMESPACE bitnami/$HELMCHART -f $HELM_VALUES --version $HELM_KUBEAPPS_VERSION --create-namespace --wait --wait-for-jobs --timeout 10m > /tmp/error.log 2>&1; ret=$?
      #helm install $HELMCHART --namespace $NAMESPACE bitnami/$HELMCHART -f $HELM_VALUES --version $HELM_KUBEAPPS_VERSION --create-namespace --wait --wait-for-jobs --timeout 10m; ret=$?

      # --- WAIT FOR PODS TO GET STARTED ---
      runningPods $NAMESPACE

      cmdLoop helm ls -n $NAMESPACE -o json > /tmp/output.json
      stt=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).status' /tmp/output.json)

      [ $ret -eq 0 -a "$stt" == "deployed" ] && break

      # --- DELTE FAILED HELM CHART ---
      deleteHelmChart $NAMESPACE $HELMCHART > /dev/null 2>&1
      deleteNamespace $NAMESPACE > /dev/null 2>&1

      let cnt=cnt+1
      sleep 30
    done

    if [ $ret -ne 0 -o "$stt" != "deployed" ]; then
      logMessages /tmp/error.log
      echo "ERROR: Failed to deploy $HELMCHART Helm Chart"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ helm install $HELMCHART --namespace $NAMESPACE bitnami/$HELMCHART -f $HELM_VALUES --version $HELM_KUBEAPPS_VERSION"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => helm install $HELMCHART --namespace $NAMESPACE bitnami/$HELMCHART -f $HELM_VALUES --version $HELM_KUBEAPPS_VERSION"
      fi

      exit 1
    fi

    messagePrint " ▪ Create Cluster Role Binding"        "default:kubeapps-operator"
    createClusterRoleBinding kubeapps-operator cluster-admin default:kubeapps-operator > /dev/null 2>&1

    messagePrint " ▪ Create Serviceaccount"              "kubeapps-operator"
    createServiceAccount default kubeapps-operator > /dev/null 2>&1

    messagePrint " ▪ Kubeapps Dashboard deployed"        "https://kubeapps.$DOMAIN"
  fi

  cmdLoop helm ls -n $NAMESPACE -o json > /tmp/output.json
  if [ -s /tmp/output.json ]; then
    ver=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).app_version' /tmp/output.json)
    crt=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).chart' /tmp/output.json)
    stt=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).status' /tmp/output.json)
    dat=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).updated' /tmp/output.json)
  else  
    ver=""; crt=""; stt=""; dat=""
  fi

  secret_name=$(getServiceAccountSecret kubeapps-operator)
  if [ "$secret_name" != "" ]; then
    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      KUBEAPPS_TOKEN=$(kubectl get secret $secret_name -o jsonpath='{.data.token}' -o go-template='{{.data.token | base64decode}}' 2>/tmp/error.log); ret=$?
  
      [ $ret -eq 0 ] && break
      let cnt=cnt+1
      sleep 30
    done
  else
    echo "ERROR: Failed to get kubeapps secret"
  fi

  messagePrint " ▪ Verify Helm Chart $HELMCHART"   "$crt"
  messagePrint "   Helm Chart Name"                "$HELMCHART"
  messagePrint "   Helm Chart Version"             "$ver"
  messagePrint "   Helm Chart Status"              "$stt"
  messagePrint "   Helm Chart Installed/Updated"   "$dat"
  messagePrint "   Kubeapps Internal Name"         "kubeapps.kubeapps.svc.cluster.local"
  messagePrint "   Kubeapps Management Portal"     "https://kubeapps.$DOMAIN"

  if [ $ret -ne 0 ]; then
    logMessages /tmp/error.log
    echo "ERROR: Failed to get Kubeapps Token"
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ kubectl get secret $secret_name -o jsonpath='{.data.token}' -o go-template='{{.data.token | base64decode}}'"
      echo "          tdh-tools:/$ exit"
    else
      echo "       => kubectl get secret $secret_name -o jsonpath='{.data.token}' -o go-template='{{.data.token | base64decode}}'"
    fi

    exit 1
  fi

  messageLine
  echo "KUBEAPP TOKEN: $KUBEAPPS_TOKEN"
  messageLine

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_KUBEAPPS             "true"
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_KUNEAPPS_TOKEN       "$KUBEAPPS_TOKEN"
}

installMinio() {
  NAMESPACE=minio
  HELMCHART=minio

  if [ "$TDH_SERVICE_MINIO" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_MINIO        "false"
    kubectl delete -n $NAMESPACE > /dev/null 2>&1
    deleteHelmChart $NAMESPACE $HELMCHART
    return
  fi

  TDH_DOMAIN=$(getConfigMap tanzu-demo-hub TDH_DOMAIN)
  TDH_ENVNAME=$(getConfigMap tanzu-demo-hub TDH_ENVNAME)
  DOMAIN=$(getConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_LB_DOMAIN)

  messageTitle "Minio S3 Object Storage"
  cnt=$(helm list -A -q | egrep -c "^minio")
  if [ $cnt -eq 0 ]; then
    messagePrint " ▪ Installing Minio" "bitnami/$HELMCHART"
    createNamespace $NAMESPACE > /dev/null 2>&1

    # --- COPY SECRET TO NAMESPACE ---
    copySecretObject default $NAMESPACE tanzu-demo-hub-tls

    HELM_VALUES=/tmp/helm_values.yaml
    echo "auth:"                                                                                                                >  $HELM_VALUES
    echo "  rootUser: admin"                                                                                                    >> $HELM_VALUES
    echo "  rootPassword: $TDH_HARBOR_ADMIN_PASSWORD"                                                                           >> $HELM_VALUES
    echo "auth:"                                                                                                                >> $HELM_VALUES
    echo "ingress:"                                                                                                             >> $HELM_VALUES
    echo "  enabled: true"                                                                                                      >> $HELM_VALUES
    echo "  certManager: false"                                                                                                 >> $HELM_VALUES
    echo "  annotations:"                                                                                                       >> $HELM_VALUES
    echo "    kubernetes.io/ingress.class: contour"                                                                             >> $HELM_VALUES
    echo "    ingress.kubernetes.io/force-ssl-redirect: \"true\""                                                               >> $HELM_VALUES
    echo "  path: /"                                                                                                            >> $HELM_VALUES
    echo "  hostname: minio.$DOMAIN"                                                                                            >> $HELM_VALUES
    #echo "  hostname: minio.local"                                                                                             >> $HELM_VALUES
    echo "  tls: true"                                                                                                          >> $HELM_VALUES
    echo "  servicePort: minio-console"                                                                                         >> $HELM_VALUES
    echo "  extraTls:"                                                                                                          >> $HELM_VALUES
    echo "  - hosts:"                                                                                                           >> $HELM_VALUES
    echo "      - minio.$DOMAIN"                                                                                                >> $HELM_VALUES
    echo "    secretName: tanzu-demo-hub-tls"                                                                                   >> $HELM_VALUES
    echo "  extraHosts:"                                                                                                        >> $HELM_VALUES
    echo "    - name: tdh-postgres-backup.minio.$DOMAIN"                                                                        >> $HELM_VALUES
    echo "      path: /"                                                                                                        >> $HELM_VALUES
    echo "apiIngress:"                                                                                                          >> $HELM_VALUES
    echo "  enabled: true"                                                                                                      >> $HELM_VALUES
    echo "  certManager: false"                                                                                                 >> $HELM_VALUES
    echo "  annotations:"                                                                                                       >> $HELM_VALUES
    echo "    kubernetes.io/ingress.class: contour"                                                                             >> $HELM_VALUES
    echo "    ingress.kubernetes.io/force-ssl-redirect: \"true\""                                                               >> $HELM_VALUES
    echo "  path: /"                                                                                                            >> $HELM_VALUES
    echo "  hostname: minio-api.$DOMAIN"                                                                                        >> $HELM_VALUES
    echo "  tls: true"                                                                                                          >> $HELM_VALUES
    echo "  servicePort: minio-api"                                                                                             >> $HELM_VALUES
    echo "  extraTls:"                                                                                                          >> $HELM_VALUES
    echo "  - hosts:"                                                                                                           >> $HELM_VALUES
    echo "      - minio-api.$DOMAIN"                                                                                            >> $HELM_VALUES
    echo "    secretName: tanzu-demo-hub-tls"                                                                                   >> $HELM_VALUES
    echo "  extraHosts:"                                                                                                        >> $HELM_VALUES
    echo "    - name: tdh-postgres-backup.minio.$DOMAIN"                                                                        >> $HELM_VALUES
    echo "      path: /"                                                                                                        >> $HELM_VALUES

    # --- ADD REPOSITORY --
    helmRepoAdd bitnami https://charts.bitnami.com/bitnami

#gaga minio
    messagePrint " ▪ Deploy the Minio Helm Chart" "minio"
    cnt=0; ret=1; stt="notdeployed"
    while [ $ret -ne 0 -a $cnt -lt 5 -a "$stt" != "deployed" ]; do
      helm install $HELMCHART bitnami/$HELMCHART --namespace $NAMESPACE -f ${HELM_VALUES} --create-namespace --wait --wait-for-jobs --timeout 10m > /tmp/error.log 2>&1; ret=$?
      #helm install $HELMCHART bitnami/$HELMCHART --namespace $NAMESPACE -f ${HELM_VALUES} --create-namespace --wait --wait-for-jobs --timeout 10m; ret=$?

      # --- WAIT FOR PODS TO GET STARTED ---
      runningPods $NAMESPACE

      cmdLoop helm ls -n $NAMESPACE -o json > /tmp/output.json
      stt=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).status' /tmp/output.json)

      [ $ret -eq 0 -a "$stt" == "deployed" ] && break

      # --- DELTE FAILED HELM CHART ---
      deleteHelmChart $NAMESPACE $HELMCHART > /dev/null 2>&1
      deleteNamespace $NAMESPACE > /dev/null 2>&1

      let cnt=cnt+1
      sleep 30
    done

    if [ $ret -ne 0 -o "$stt" != "deployed" ]; then
      logMessages /tmp/error.log
      echo "ERROR: Failed to deploy $HELMCHART Helm Chart"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ helm install minio --namespace $NAMESPACE bitnami/minio -f ${HELM_VALUES}"
        echo "          tdh-tools:/$ exit"
      else
        echo "       =>  helm install minio bitnami/minio --namespace $NAMESPACE -f ${HELM_VALUES}"
      fi

      exit 1
    fi

    # --- WAIT FOR PODS TO GET STARTED ---
    runningPods $NAMESPACE
  fi

  cmdLoop helm ls -n $NAMESPACE -o json > /tmp/output.json
  if [ -s /tmp/output.json ]; then
    ver=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).app_version' /tmp/output.json)
    crt=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).chart' /tmp/output.json)
    stt=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).status' /tmp/output.json)
    dat=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).updated' /tmp/output.json)
  else
    ver=""; crt=""; stt=""; dat=""
  fi

  cmdLoop kubectl -n $NAMESPACE get secrets minio -o json > /tmp/output.json
  ROOT_USER=$(jq -r '.data."root-user"' /tmp/output.json | base64 --decode)
  ROOT_PASSWORD=$(jq -r '.data."root-password"' /tmp/output.json | base64 --decode)

  messagePrint " ▪ Verify Minio S3"                "$crt"
  messagePrint "   Helm Chart Name"                "minio"
  messagePrint "   Helm Chart Version"             "$ver"
  messagePrint "   Helm Chart Status"              "$stt"
  messagePrint "   Helm Chart Installed/Updated"   "$dat"
  messagePrint "   Minio Internal Name"            "minio.default.svc.cluster.local"
  messagePrint "   Minio Management Portal"        "https://minio.$DOMAIN"
  messagePrint "   Minio Admin User"               "$ROOT_USER"
  messagePrint "   Minio Admin Password"           "$ROOT_PASSWORD"

  echo "   -----------------------------------------------------------------------------------------------------------"
  messageTitle "   Test Minio S3 Access: (https://docs.min.io/docs/minio-client-complete-guide)"

  if [ ! -f /usr/local/bin/mc ]; then
    if [ $(uname) == "Darwin" ]; then 
      echo "   MacOS installation"
      echo "     => brew install minio/stable/mc"
      echo ""
    else
      echo "   Linux installation"
      echo "     => wget https://dl.minio.io/client/mc/release/linux-amd64/mc"
      echo "     => mv mc /usr/local/bin/mc && chmod a+x /usr/local/bin/mc"
      echo ""
    fi
  fi

  echo "     => mc alias set minio https://minio-api.$DOMAIN $ACCESS_KEY $SECRET_KEY --api S3v4"
  echo "     => mc ls minio"
  echo "   -----------------------------------------------------------------------------------------------------------"
  
  mc alias list minio > /dev/null 2>&1; ret=$?
  if [ $? -ne 0 ]; then
    # --- WAIT FOR HARBOR TO COME ONLINE ---
    ret=1; cnt=0
    while [ $ret -ne 0 -a $cnt -le 5 ]; do
      mc alias set minio https://minio-api.$DOMAIN $ACCESS_KEY $SECRET_KEY --api S3v4 > /tmp/error.log 2>&1; ret=$?
      [ $ret -eq 0 ] && break
      sleep 60
      let cnt=cnt+1
    done

    if [ $ret -ne 0 ]; then
      logMessages /tmp/error.log
      echo "ERROR: Unable to configure Minio Client (mc)"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ mc alias set minio https://minio-api.$DOMAIN $ACCESS_KEY $SECRET_KEY --api S3v4"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => mc alias set minio https://minio-api.$DOMAIN $ACCESS_KEY $SECRET_KEY --api S3v4"
      fi

      exit 1
    fi
  fi
  
  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_MINIO                "true"
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_MINIO_INTERNAL_NAME  "minio.default.svc.cluster.local:9000"
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_MINIO_ACCESS_KEY     "$ACCESS_KEY"
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_MINIO_SECRET_KEY     "$SECRET_KEY"
}

# ------------------------------------------------------------------------------------------
# Function Name ......: installGitLab
# Function Purpose ...: Installing GitLab Commnunity Edition
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: 
#          ($2) ......: 
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
# GitLab for Open Source.....: https://about.gitlab.com/install/?test=capabilities
# GitLab Quick Start Guide ..: https://docs.gitlab.com/charts/quickstart/index.html 
# GitLab API ................: https://docs.gitlab.com/ee/api/
# ------------------------------------------------------------------------------------------
installGitLab() {
  NAMESPACE=gitlab
  HELMCHART=gitlab

  if [ "$TDH_SERVICE_GITLAB" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_GITLAB        "false"
    deleteHelmChart $NAMESPACE $HELMCHART
    deleteNamespace $NAMESPACE
    return
  fi

  TDH_DOMAIN=$(getConfigMap tanzu-demo-hub TDH_DOMAIN)
  TDH_ENVNAME=$(getConfigMap tanzu-demo-hub TDH_ENVNAME)
  DOMAIN=$(getConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_LB_DOMAIN)
  DOMAIN="apps-nginx.${TDH_ENVNAME}.${TDH_DOMAIN}"

  messageTitle "GitLab Commnunity Edition"
  cnt=$(helm list -A -q | egrep -c "^gitlab")
  if [ $cnt -eq 0 ]; then
    messagePrint " ▪ Installing GitLab" "gitlab/gitlab"
    createNamespace $NAMESPACE > /dev/null 2>&1

    # --- ADD REPOSITORY --
    helmRepoAdd gitlab https://charts.gitlab.io/

    # --- COPY SECRET TO NAMESPACE ---
    copySecretObject default $NAMESPACE tanzu-demo-hub-tls

    #DELETEME
    #kubectl get secret tanzu-demo-hub-tls --namespace=default  -oyaml | grep -v '^\s*namespace:\s' | \
    #kubectl apply --namespace=$NAMESPACE -f -

    # --- DOCKER PULL SECRET ---
    dockerPullSecret $NAMESPACE

    copySecretObject default $NAMESPACE tanzu-demo-hub-tls gitlab-wildcard-tls

    #DELETEME
    #kubectl get secret tanzu-demo-hub-tls --namespace=default  -oyaml | grep -v '^\s*namespace:\s' | \
    #sed 's/name: tanzu-demo-hub-tls/name: gitlab-wildcard-tls/g' | \
    #kubectl apply --namespace=$NAMESPACE -f - 

    deleteSecret $NAMESPACE tdh-gitlab-initial-root-password
    cmdLoop kubectl create secret generic tdh-gitlab-initial-root-password \
            --from-literal=password=$TDH_HARBOR_ADMIN_PASSWORD -n $NAMESPACE

    HELM_VALUES=/tmp/gitlab_values.yaml
    echo ""                                                                                                                     >  $HELM_VALUES
    echo "global:"                                                                                                              >> $HELM_VALUES
    echo "  imageRegistry: docker.io"                                                                                           >> $HELM_VALUES
    echo "  imagePullSecrets:"                                                                                                  >> $HELM_VALUES
    echo "    - tdh-docker-repo"                                                                                                >> $HELM_VALUES
    echo "  edition: ee"                                                                                                        >> $HELM_VALUES
    echo "  hosts:"                                                                                                             >> $HELM_VALUES
    echo "    domain: $DOMAIN"                                                                                                  >> $HELM_VALUES
    echo "  initialRootPassword:"                                                                                               >> $HELM_VALUES
    echo "    secret: tdh-gitlab-initial-root-password"                                                                         >> $HELM_VALUES
    echo "    key: password"                                                                                                    >> $HELM_VALUES
    echo "  ingress:"                                                                                                           >> $HELM_VALUES
    echo "    configureCertmanager: false"                                                                                      >> $HELM_VALUES
    echo "ingress:"                                                                                                             >> $HELM_VALUES
    echo "  apiVersion: \"\""                                                                                                   >> $HELM_VALUES
    echo "  configureCertmanager: true"                                                                                         >> $HELM_VALUES
    echo "  provider: nginx"                                                                                                    >> $HELM_VALUES
    echo "  annotations:"                                                                                                       >> $HELM_VALUES
    echo "    kubernetes.io/ingress.class: nginx"                                                                               >> $HELM_VALUES
    echo "  enabled: true"                                                                                                      >> $HELM_VALUES
    echo "  tls:"                                                                                                               >> $HELM_VALUES
    echo "    enabled: true"                                                                                                    >> $HELM_VALUES
    echo "    secretName: tanzu-demo-hub-tls"                                                                                   >> $HELM_VALUES
    echo "  path: /"                                                                                                            >> $HELM_VALUES
    echo "  pathType: Prefix"                                                                                                   >> $HELM_VALUES
    echo "certmanager-issuer:"                                                                                                  >> $HELM_VALUES
    echo "  email: $$TDH_CERTMANAGER_EMAIL"                                                                                     >> $HELM_VALUES
    echo "certmanager:"                                                                                                         >> $HELM_VALUES
    echo "  installCRDs: false"                                                                                                 >> $HELM_VALUES
    echo "  install: false"                                                                                                     >> $HELM_VALUES

#gogo
    cnt=0; ret=1; stt="notdeployed"
    while [ $ret -ne 0 -a $cnt -lt 5 -a "$stt" != "deployed" ]; do
      helm install gitlab gitlab/gitlab -f ${HELM_VALUES} --namespace $NAMESPACE --wait --wait-for-jobs --timeout 10m > /tmp/error.log 2>&1; ret=$?

      cmdLoop helm ls -n $NAMESPACE -o json > /tmp/output.json
      stt=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).status' /tmp/output.json)

      [ $ret -eq 0 -a "$stt" == "deployed" ] && break

      # --- DELTE FAILED HELM CHART ---
      deleteHelmChart $NAMESPACE $HELMCHART

      let cnt=cnt+1
      sleep 30
    done

    if [ $ret -ne 0 -o "$stt" != "deployed" ]; then
      logMessages /tmp/error.log
      echo "ERROR: Failed to deploy GitLab Helm Chart"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ helm install gitlab gitlab/gitlab --namespace $NAMESPACE -f ${HELM_VALUES}"
        echo "          tdh-tools:/$ exit"
      else
        echo "       =>  helm install gitlab gitlab/gitlab --namespace $NAMESPACE -f ${HELM_VALUES}"
      fi

      exit 1
    fi

    # PATCH DOCKER PULL SECRET
    dockerPullSecret gitlab gitlab-certmanager-issuer 
    dockerPullSecret gitlab gitlab-gitlab-runner 
    dockerPullSecret gitlab gitlab-nginx-ingress
    dockerPullSecret gitlab gitlab-nginx-ingress-backend
    dockerPullSecret gitlab gitlab-prometheus-server

    cmdLoop kubectl -n gitlab rollout restart deployment gitlab-gitlab-runner > /dev/null 2>&1
    cmdLoop kubectl -n gitlab rollout restart deployment gitlab-prometheus-server > /dev/null 2>&1

    # --- WAIT FOR PODS TO GET STARTED ---
    runningPods $NAMESPACE
  fi

  cmdLoop helm ls -n $NAMESPACE -o json > /tmp/output.json
  if [ -s /tmp/output.json ]; then
    ver=$(jq -r --arg chart "gitlab" '.[] | select(.name == $chart).app_version' /tmp/output.json)
    crt=$(jq -r --arg chart "gitlab" '.[] | select(.name == $chart).chart' /tmp/output.json)
    stt=$(jq -r --arg chart "gitlab" '.[] | select(.name == $chart).status' /tmp/output.json)
    dat=$(jq -r --arg chart "gitlab" '.[] | select(.name == $chart).updated' /tmp/output.json)
  else
    ver=""; crt=""; stt=""; dat=""
  fi

  SECRET=$(cmdLoop kubectl get secret tdh-gitlab-initial-root-password  -n $NAMESPACE -o json | jq -r '.data.password' | base64 -d )

  messagePrint " ▪ Verify GitLab CE Deployment"    "$crt"
  messagePrint "   Helm Chart Name"                "minio"
  messagePrint "   Helm Chart Version"             "$ver"
  messagePrint "   Helm Chart Status"              "$stt"
  messagePrint "   Helm Chart Installed/Updated"   "$dat"
  messagePrint "   Gitlab Internal Name"           "gitlab.default.svc.cluster.local"
  messagePrint "   Gitlab Management Portal"       "https://gitlab.$DOMAIN"
  messagePrint "   Gitlab Admin User"              "root"
  messagePrint "   Gitlab Admin Passwird"          "$SECRET"
  echo "   -----------------------------------------------------------------------------------------------------------------------"
  echo "   WARNING: If the login to GitLab is not working (Error 402), thatn try to clear browser history and cookies             "     
  echo "   -----------------------------------------------------------------------------------------------------------------------"

  # --- VERIFY DNS DOMAIN ---
  verifyHostedZone ${TDH_ENVNAME}.$AWS_HOSTED_DNS_DOMAIN

  ipa=""; cnt=0
  while [ "$ipa" == "" -a $cnt -lt 15 ]; do
    ipa=$(kubectl describe svc gitlab-nginx-ingress-controller --namespace $NAMESPACE | egrep "^LoadBalancer Ingress:" | awk '{ print $NF }')
    [ "$ipa" != "" ] && break
    let cnt=cnt+1
    sleep 30
  done

  if [ "$ipa" == "" ]; then
    echo "ERROR: Failed to gather IP Adress of the ingress-contour-envoy service in namespace ingress-contour"
    echo "       Please check manually:"
    messageLine
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ kubectl get svc --namespace $NAMESPACE"
      echo "          tdh-tools:/$ exit"
    else
      echo "       => kubectl get svc --namespace $NAMESPACE"
    fi

    exit 1
  fi

  # --- SET DNS RECORD ---
  if [ "$TDH_DEPLOYMENT_CLOUD" == "minikube" -o "$TDH_DEPLOYMENT_CLOUD" == "vSphere" -o "$TDH_DEPLOYMENT_CLOUD" == "Azure" -o "$TDH_DEPLOYMENT_CLOUD" == "docker" -o \
       "$TDH_TKGMC_INFRASTRUCTURE" == "minikube" -o "$TDH_TKGMC_INFRASTRUCTURE" == "vSphere" -o "$TDH_TKGMC_INFRASTRUCTURE" == "Azure" -o "$TDH_TKGMC_INFRASTRUCTURE" == "docker" ]; then
    setDNSrecord "$ipa" "${TDH_ENVNAME}" "${AWS_HOSTED_DNS_DOMAIN}" "*.apps-nginx" "A"
  else
    setDNSalias "$ipa" "${TDH_ENVNAME}" "${AWS_HOSTED_DNS_DOMAIN}" "*.apps-nginx" "A"
  fi

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_GITLAB                "true"
}

installTanzuDataPostgres() {
  NAMESPACE=default
  HELMCHART=postgres-operator

  if [ "$TDH_SERVICE_TANZU_DATA_POSTGRES" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_TANZU_DATA_POSTGRES        "false"
    deleteHelmChart $NAMESPACE $HELMCHART
    deleteNamespace $NAMESPACE
    return
  fi

  messageTitle "VMware Tanzu Postgres"
  cnt=$(cmdLoop helm list --all-namespaces -q | egrep -c "^postgres-operator")
  if [ $cnt -eq 0 ]; then
    # --- DOWNLOAD POSTGRES BINARIES ---
    TBS_FILE="/tmp/postgres-for-kubernetes-v${TDH_SERVICE_TANZU_DATA_POSTGRES_VERSION}.tar.gz"
    TBS_TAR_FILE="/tmp/postgres-for-kubernetes-v${TDH_SERVICE_TANZU_DATA_POSTGRES_VERSION}.tar"

    rm -f $TBS_FILE $TBS_TAR_FILE

    downloadFromPivnet tanzu-sql-postgres $TDH_SERVICE_TANZU_DATA_POSTGRES_VERSION "$TDH_SERVICE_TANZU_DATA_POSTGRES_VERSION" "Postgres for VMware Tanzu v$TDH_SERVICE_TANZU_DATA_POSTGRES_VERSION"
    #https://network.pivotal.io/api/v2/products/tanzu-sql-postgres/releases/769686/product_files/819245/download

    [ -d /tmp/postgres-for-kubernetes ] && rm -rf /tmp/postgres-for-kubernetes
    [ -f $TBS_TAR_FILE ] && rm -f $TBS_TAR_FILE
    mkdir -p /tmp/postgres-for-kubernetes
    [ -f $TBS_FILE ] && gunzip $TBS_FILE
    if [ -f $TBS_TAR_FILE ]; then
      tar xf $TBS_TAR_FILE -C /tmp/postgres-for-kubernetes
    else
      echo "ERROR: can not find file $TBS_TAR_FILE"; exit 1
    fi

    #########################################################################################################################
    ########################################### INSTALL POSTGRES OPERATOR ###################################################
    #########################################################################################################################

    if [ "$TDH_SERVICE_REGISTRY_HARBOR" == "true" ]; then
      # DNS_HARBOR=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR)
      TDH_HARBOR_REGISTRY_DNS_HARBOR=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR)
      TDH_HARBOR_REGISTRY_ADMIN_PASSWORD=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_ADMIN_PASSWORD)
      messagePrint " ▪ Verify Harbor Registry Access" "$TDH_HARBOR_REGISTRY_DNS_HARBOR"
     
      cnt=0; ret=1
      while [ $ret -ne 0 -a $cnt -lt 5 ]; do
        docker login $TDH_HARBOR_REGISTRY_DNS_HARBOR -u admin -p $TDH_HARBOR_REGISTRY_ADMIN_PASSWORD > /tmp/error.log 2>&1; ret=$?
        sleep 10
        let cnt=cnt+1
      done

      if [ $ret -ne 0 ]; then
        logMessages /tmp/error.log
        echo "ERROR: Docker login does not work 1"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ docker login $TDH_HARBOR_REGISTRY_DNS_HARBOR -u admin -p $TDH_HARBOR_REGISTRY_ADMIN_PASSWORD"; exit 1
          echo "          tdh-tools:/$ exit"
        else
          echo "       => docker login $TDH_HARBOR_REGISTRY_DNS_HARBOR -u admin -p $TDH_HARBOR_REGISTRY_ADMIN_PASSWORD"; exit 1
        fi
  
        exit 1
      fi
  
      messageTitle "Tanzu Postgres Operator"
      cnt=$(cmdLoop helm list --all-namespaces -q | egrep -c "^postgres-operator") 
      if [ $cnt -eq 0 ]; then
        messagePrint " ▪ Install Postgres Operator" "$TDH_SERVICE_TANZU_DATA_POSTGRES_VERSION"
  
        # --- INSTALL POSTGRES ---
        POSTGRES_INSTALL_PATH="/tmp/postgres-for-kubernetes/postgres-for-kubernetes-v$TDH_SERVICE_TANZU_DATA_POSTGRES_VERSION"
        # REGISTRY="harbor.apps-contour.local.pcfsdu.com/library"
  
        cmdLoop docker load -i ${POSTGRES_INSTALL_PATH}/images/postgres-instance  > /dev/null 2>&1
        cmdLoop docker load -i ${POSTGRES_INSTALL_PATH}/images/postgres-operator  > /dev/null 2>&1
  
        messagePrint " ▪ Relocate Images (postgres-instance) to Harbor" "$REGISTRY/postgres-instance"
  
        INSTANCE_IMAGE_NAME="${TDH_HARBOR_REGISTRY_DNS_HARBOR}/library/postgres-instance:$(cat $POSTGRES_INSTALL_PATH/images/postgres-instance-tag)"
        cmdLoop docker tag $(cat $POSTGRES_INSTALL_PATH/images/postgres-instance-id) ${INSTANCE_IMAGE_NAME}  > /dev/null 2>&1
        cmdLoop docker push ${INSTANCE_IMAGE_NAME}  > /dev/null 2>&1
  
        messagePrint " ▪ Reocate Images (postgres-operator) to Harbor" "$TDH_HARBOR_REGISTRY_DNS_HARBOR/postgres-operator"
        OPERATOR_IMAGE_NAME="${TDH_HARBOR_REGISTRY_DNS_HARBOR}/library/postgres-operator:$(cat $POSTGRES_INSTALL_PATH/images/postgres-operator-tag)"
        cmdLoop docker tag $(cat $POSTGRES_INSTALL_PATH/images/postgres-operator-id) ${OPERATOR_IMAGE_NAME}  > /dev/null 2>&1
        cmdLoop docker push ${OPERATOR_IMAGE_NAME}  > /dev/null 2>&1
  
        # --- CLEANUP DOCKER IMAGES ---
        #for n in $(docker images | egrep "postgres-(operator|instance) " | awk '{ print $3 }'); do
        #  docker image rm $n -f > /dev/null 2>&1
        #done
  
        messagePrint " ▪ Customize Postgres Helm Chart Values" "/tmp/values-overrides.yaml"
        sed -e "s+operatorImageRepository: postgres-operator+operatorImageRepository: $TDH_HARBOR_REGISTRY_DNS_HARBOR/library/postgres-operator+g" \
            -e "s+postgresImageRepository: postgres-instance+postgresImageRepository: $TDH_HARBOR_REGISTRY_DNS_HARBOR/library/postgres-instance+g" \
            ${POSTGRES_INSTALL_PATH}/operator/values.yaml > /tmp/values-overrides.yaml
        if [ "${DEBUG}" == "1" ]; then
          echo "-----------------------------------------------------------------------------------------------------------"
          cat /tmp/values-overrides.yaml
          echo "-----------------------------------------------------------------------------------------------------------"
        fi

        #messagePrint " ▪ Create Kubernetes Namespace" "postgres-operator"
        #deleteNamespace $NAMESPACE > /dev/null 2>&1
        #createNamespace $NAMESPACE > /dev/null 2>&1

#gaga postgress
        messagePrint " ▪ Deploy the Postgres operator" "postgres-operator"
        cnt=0; ret=1; stt="notdeployed"
        while [ $ret -ne 0 -a $cnt -lt 5 -a "$stt" != "deployed" ]; do
          helm install $HELMCHART --namespace $NAMESPACE -f /tmp/values-overrides.yaml ${POSTGRES_INSTALL_PATH}/operator/ --create-namespace --wait --wait-for-jobs --timeout 10m > /tmp/error.log 2>&1; ret=$?
          #helm install $HELMCHART --namespace $NAMESPACE -f /tmp/values-overrides.yaml ${POSTGRES_INSTALL_PATH}/operator/ --create-namespace --wait --wait-for-jobs --timeout 10m; ret=$?

          # --- WAIT FOR PODS TO GET STARTED ---
          runningPods $NAMESPACE

          cmdLoop helm ls -n $NAMESPACE -o json > /tmp/output.json
          stt=$(jq -r --arg chart "$HELMCHART" '.[] | select(.name == $chart).status' /tmp/output.json)

          [ $ret -eq 0 -a "$stt" == "deployed" ] && break

          # --- DELTE FAILED HELM CHART ---
          deleteHelmChart $NAMESPACE $HELMCHART > /dev/null 2>&1
          deleteNamespace $NAMESPACE > /dev/null 2>&1

          let cnt=cnt+1
          sleep 30
        done

        if [ $ret -ne 0 -o "$stt" != "deployed" ]; then
          logMessages /tmp/error.log
          echo "ERROR: Failed to deploy $HELMCHART Helm Chart"
          if [ "$NATIVE" == "0" ]; then
            echo "       => tools/${TDH_TOOLS}.sh"
            echo "          tdh-tools:/$ helm install $HELMCHART --namespace $NAMESPACE -f /tmp/values-overrides.yaml ${POSTGRES_INSTALL_PATH}/operator/"
            echo "          tdh-tools:/$ exit"
          else
            echo "       => helm install $HELMCHART --namespace $NAMESPACE -f /tmp/values-overrides.yaml ${POSTGRES_INSTALL_PATH}/operator/"
          fi
    
          exit 1
        fi

        # patch serviceaccounts default and postgres-operator-service-account in the default namespace
        patchServiceAccount default default
        patchServiceAccount postgres-operator-service-account default

      fi
    fi
  fi

  cmdLoop helm ls -n default -o json > /tmp/output.json
  if [ -s /tmp/output.json ]; then
    ver=$(jq -r --arg chart "postgres-operator" '.[] | select(.name == $chart).app_version' /tmp/output.json)
    crt=$(jq -r --arg chart "postgres-operator" '.[] | select(.name == $chart).chart' /tmp/output.json)
    stt=$(jq -r --arg chart "postgres-operator" '.[] | select(.name == $chart).status' /tmp/output.json)
    dat=$(jq -r --arg chart "postgres-operator" '.[] | select(.name == $chart).updated' /tmp/output.json)
  else
    ver=""; crt=""; stt=""; dat=""
  fi

  messagePrint " ▪ Verify Postgres Operator" "$crt"
  messagePrint "   Helm Chart Name"                "postgres-operator"
  messagePrint "   Helm Chart Version"             "$ver"
  messagePrint "   Helm Chart Status"              "$stt"
  messagePrint "   Helm Chart Installed/Updated"   "$dat"

#  cmdLoop helm ls -n $NAMESPACE -o json > /tmp/output.json
#  if [ -s /tmp/output.json ]; then
#    ver=$(jq -r --arg chart "tdh-pgadmin" '.[] | select(.name == $chart).app_version' /tmp/output.json)
#    crt=$(jq -r --arg chart "tdh-pgadmin" '.[] | select(.name == $chart).chart' /tmp/output.json)
#    stt=$(jq -r --arg chart "tdh-pgadmin" '.[] | select(.name == $chart).status' /tmp/output.json)
#    dat=$(jq -r --arg chart "tdh-pgadmin" '.[] | select(.name == $chart).updated' /tmp/output.json)
#  else
#    ver=""; crt=""; stt=""; dat=""
#  fi
#  #ver=$(helm ls -o json | jq -r --arg chart "tdh-pgadmin" '.[] | select(.name == $chart).app_version')
#  #crt=$(helm ls -o json | jq -r --arg chart "tdh-pgadmin" '.[] | select(.name == $chart).chart')
#  #stt=$(helm ls -o json | jq -r --arg chart "tdh-pgadmin" '.[] | select(.name == $chart).status')
#  #dat=$(helm ls -o json | jq -r --arg chart "tdh-pgadmin" '.[] | select(.name == $chart).updated')
#
#  #messageTitle "Verify pgAdmin" 
#  #messagePrint " ▪ Helm Chart"                     "$crt"
#  #messagePrint " ▪ Helm Chart Name"                "tdh-pgadmin"
#  #messagePrint " ▪ Helm Chart Version"             "$ver"
#  #messagePrint " ▪ Helm Chart Status"              "$stt"
#  #messagePrint " ▪ Helm Chart Installed/Updated"   "$dat"

  CURRENT_DATE=$(date "+%Y-%m-%d %M:%H:%S")
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_TANZU_DATA_POSTGRES                "true"
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_TANZU_DATA_POSTGRES_VERSION        $TDH_SERVICE_TANZU_DATA_POSTGRES_VERSION
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_TANZU_DATA_POSTGRES_INSTALLED      "$CURRENT_DATE"
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_TANZU_DATA_POSTGRES_VALIDATED      "$CURRENT_DATE"
}

installBuildService() {
  tbs_cleanup=0
  if [ "$TDH_SERVICE_BUILD_SERVICE" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_BUILD_SERVICE        "false"
    return
  fi

  if [ "$TDH_SERVICE_BUILD_SERVICE_VERSION" == "" ]; then
    echo "ERROR: Environment Variable (TDH_SERVICE_BUILD_SERVICE_VERSION) not found in"
    echo "       in the deployment file: ${TDHPATH}/deployments/${K8S_DEPLOYMENT}"
    exit 1
  fi

  # --- GET BUILD_SERVICE INFORMATION ---
  TDH_SERVICE_BUILD_SERVICE_INSTALLED=$(getConfigMap tanzu-demo-hub TDH_SERVICE_BUILD_SERVICE_INSTALLED)
  TDH_SERVICE_BUILD_SERVICE_VERIFIED=$(getConfigMap tanzu-demo-hub TDH_SERVICE_BUILD_SERVICE_VERIFIED)
  TDH_SERVICE_REGISTRY_DOCKER_LAST=$(getConfigMap tanzu-demo-hub TDH_SERVICE_REGISTRY_DOCKER)
  TDH_SERVICE_REGISTRY_HARBOR_LAST=$(getConfigMap tanzu-demo-hub TDH_SERVICE_REGISTRY_HARBOR)

  # --- VERIFY IF BUILDSERVICE IS INSTALLED ---
  messageTitle "Tanzu Build Service"

  if [ "$TDH_SERVICE_REGISTRY_HARBOR_LAST" == "true" -a "$TDH_SERVICE_REGISTRY_DOCKER" == "true" -o \
       "$TDH_SERVICE_REGISTRY_DOCKER_LAST" == "true" -a "$TDH_SERVICE_REGISTRY_HARBOR" == "true" ]; then 

    if [ "$TDH_SERVICE_REGISTRY_HARBOR_LAST" == "true" -a "$TDH_SERVICE_REGISTRY_DOCKER" == "true" ]; then 
      messageLine
      echo "INFO: Tanzu Build Service (TBS) has been deployed on the local Harbor Registry in the previous deployment."
      echo "      but for this deployment Docker Hub has been configured. Therefore the build service will be reinstalled"
      echo "      => TDH_SERVICE_REGISTRY_DOCKER=$TDH_SERVICE_REGISTRY_DOCKER"
      messageLine
    else
      messageLine
      echo "INFO: Tanzu Build Service (TBS) has been deployed on the DockerHub Registry in the previous deployment."
      echo "      but for this deployment local Harbor Hub has been configured. Therefore the build service will be reinstalled"
      echo "      => TDH_SERVICE_REGISTRY_HARBOR=$TDH_SERVICE_REGISTRY_HARBOR"
      messageLine
    fi
    tbs_cleanup=1
  fi

  # --- VERIFY INSTALLATION ---
  kp clusterbuilder list > /dev/null 2>&1; ret=$?
  cnt=$(kp clusterbuilder list 2>/dev/null | grep -c false) 
  if [ $ret -ne 0 -o $cnt -ne 0 ]; then 
    messagePrint " ▪ Clusterbuilders not working" "cleaning up and reinstall build-service"
    tbs_cleanup=1
  fi

  # --- CLEANING UP BUILDSERVICE ---
  if [ $tbs_cleanup -eq 1 ]; then 
    # --- NOT WORKING, SO CLEAN IT UP FIRST ---
    for n in $(kp clusterbuilder list 2>/dev/null | grep -v NAME | awk '{ print $1 }'); do
      #echo "kp clusterbuilder delete $n"
      kp clusterbuilder delete $n > /dev/null 2>&1
      if [ $ret -ne 0 ]; then 
        echo "ERROR: failed to delete clusterbuilder"
        echo "       => kp clusterbuilder delete $n"
      fi
    done

    kapp delete -a tanzu-build-service -y > /dev/null 2>&1
    deleteNamespace build-service > /dev/null 2>&1
    deleteNamespace kpack > /dev/null 2>&1

    export TDH_SERVICE_BUILD_SERVICE_INSTALLED=""
  fi

  if [ "$TDH_SERVICE_BUILD_SERVICE_INSTALLED" == "" ]; then 
    messagePrint " ▪ Installing Tanzu Build Service" "$TDH_SERVICE_BUILD_SERVICE_VERSION"

    # --- DOWNLOAD BUILD_SERVICE BINARIES ---
    #downloadFromPivnet build-service $TDH_SERVICE_BUILD_SERVICE_VERSION "$TDH_SERVICE_BUILD_SERVICE_VERSION" "build-service-$TDH_SERVICE_BUILD_SERVICE_VERSION.tar"
    #TBS_FILE="/tmp/build-service-$TDH_SERVICE_BUILD_SERVICE_VERSION.tar"
    #[ -d /tmp/build-service ] && rm -rf /tmp/build-service
    #mkdir /tmp/build-service
    #tar xf $TBS_FILE -C /tmp/build-service

  
    # --- VERIFY SERVICE CONFIGURATION ---
    checkKubernetesServices registry_vmware
    checkKubernetesServices registry_docker
    checkKubernetesServices github
  
    export TDH_REGISTRY_VMWARE_NAME=registry.tanzu.vmware.com
    messagePrint " ▪ Verify VMware Registry Access" "$TDH_REGISTRY_VMWARE_NAME"
    messagePrint "   Registry User"                 "$TDH_REGISTRY_VMWARE_USER"
    messagePrint "   Registry Password"             "$(maskPassword \"$TDH_REGISTRY_VMWARE_PASS\")"
  
    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      docker login $TDH_REGISTRY_VMWARE_NAME -u $TDH_REGISTRY_VMWARE_USER -p $TDH_REGISTRY_VMWARE_PASS > /dev/null 2>&1; ret=$?
      sleep 10
      let cnt=cnt+1
    done
  
    if [ $ret -ne 0 ]; then
      echo "ERROR: Docker login does not work 2"
      echo "       => docker login $TDH_REGISTRY_VMWARE_NAME -u $TDH_REGISTRY_VMWARE_USER -p $TDH_REGISTRY_VMWARE_PASS"; exit 1
    fi

    if [ "$TDH_SERVICE_REGISTRY_DOCKER" == "true" -a "$TDH_SERVICE_REGISTRY_HARBOR" == "true" ]; then
      echo "ERROR: The Tanzu Build service can be installed with one registry only. Please set only ONE of the two variables to 'true'"
      echo "       => TDH_SERVICE_REGISTRY_DOCKER=\"<true|false>\""
      echo "       => TDH_SERVICE_REGISTRY_HARBOR=\"<true|false>\""
      exit 1
    fi
  
    #########################################################################################################################
    ################################# TANZU BUILD SERVICE (TBS) - ON DOCKER REGISTRY ########################################
    #########################################################################################################################
    # kp secret create with --registry index.docker.io generates invalid secret #155
    # https://githubmemory.com/repo/vmware-tanzu/kpack-cli/issues/155

    if [ "$TDH_SERVICE_REGISTRY_DOCKER" == "true" ]; then
      #DNS_DOCKER=$(getConfigMap tanzu-demo-hub TDH_REGISTRY_DOCKER_NAME)
      #TDH_REGISTRY_DOCKER_NAME=$(getConfigMap tanzu-demo-hub TDH_REGISTRY_DOCKER_NAME)
      #TDH_REGISTRY_DOCKER_USER=$(getConfigMap tanzu-demo-hub TDH_REGISTRY_DOCKER_USER)
      #TDH_REGISTRY_DOCKER_PASS=$(getConfigMap tanzu-demo-hub TDH_REGISTRY_DOCKER_PASS)

      TDH_REGISTRY_DOCKER_NAME=docker.io
  
      # --- TESTING REGISTRY ---
      messagePrint " ▪ Verify Docker Registry Access" "$TDH_REGISTRY_DOCKER_NAME"
  
      cnt=0; ret=1
      while [ $ret -ne 0 -a $cnt -lt 5 ]; do
        docker login -u $TDH_REGISTRY_DOCKER_USER -p $TDH_REGISTRY_DOCKER_PASS > /dev/null 2>&1; ret=$?
        sleep 10
        let cnt=cnt+1
      done
  
      if [ $ret -ne 0 ]; then
        echo "ERROR: Docker login does not work 4"
        echo "       => docker login $TDH_REGISTRY_DOCKER_NAME -u $TDH_REGISTRY_DOCKER_USER -p $TDH_REGISTRY_DOCKER_PASS"; exit 1
      fi
  
      messagePrint " ▪ Relocate images to" "$TDH_REGISTRY_DOCKER_NAME/$TDH_REGISTRY_DOCKER_USER/build-service"
      cnt=0; ret=1
      while [ $ret -ne 0 -a $cnt -lt 5 ]; do
	imgpkg copy -b "registry.pivotal.io/build-service/bundle:$TDH_SERVICE_BUILD_SERVICE_VERSION" \
		    --to-repo $TDH_REGISTRY_DOCKER_USER/build-service > /tmp/error.log 2>&1; ret=$?
        [ $ret -eq 0 ] && break
        let cnt=cnt+1
        sleep 30
      done

      if [ $ret -ne 0 ]; then
        logMessages /tmp/error.log
        echo "ERROR: Image Relocation failed"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ imgpkg copy -b registry.pivotal.io/build-service/bundle:$TDH_SERVICE_BUILD_SERVICE_VERSION \\"
	  echo "                                   --to-repo $TDH_REGISTRY_DOCKER_USER/build-service"; exit 1
          echo "          tdh-tools:/$ exit"
        else
          echo "       => imgpkg copy -b registry.pivotal.io/build-service/bundle:$TDH_SERVICE_BUILD_SERVICE_VERSION \\"
          echo "          --to-repo $TDH_REGISTRY_DOCKER_USER/build-service"; exit 1
        fi

        exit 1
      fi

      messagePrint " ▪ Pull the Tanzu Build Service bundle image locally" "$TDH_REGISTRY_VMWARE_NAME/library/build-service"
      rm -rf /tmp/bundle
      cnt=0; ret=1
      while [ $ret -ne 0 -a $cnt -lt 5 ]; do
	imgpkg pull -b "$TDH_REGISTRY_DOCKER_USER/build-service:$TDH_SERVICE_BUILD_SERVICE_VERSION" -o /tmp/bundle > /tmp/error.log 2>&1; ret=$?
        [ $ret -eq 0 ] && break
        let cnt=cnt+1
        sleep 30
      done

      if [ $ret -ne 0 ]; then
        logMessages /tmp/error.log
        echo "ERROR: Image Relocation failed"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ imgpkg pull -b \"$TDH_REGISTRY_DOCKER_USER/build-service\" -o /tmp/bundle"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => imgpkg pull -b \"$TDH_REGISTRY_DOCKER_USER/build-service\" -o /tmp/bundle"
        fi

        exit 1
      fi

      # --- DEPLOY KAPP ---
      cnt=$(kapp inspect -a tanzu-build-service 2>/dev/null | grep -c "0 resources")
      stt=$(kapp inspect -a tanzu-build-service 2>/dev/null | tail -1)
      if [ "$stt" != "Succeeded" -o $cnt -eq 1 ]; then
        kapp delete -a tanzu-build-service -y >/dev/null 2>&1
        messagePrint " ▪ Deploy Build Service (kapp)" "/tmp/bundle/values.yaml"
        echo "   -----------------------------------------------------------------------------------------------------------"
        echo "   WARNING: This process may take up to 2h. You can run 'kapp inspect -a tanzu-build-service' to see progress "
        echo "   -----------------------------------------------------------------------------------------------------------"

        ytt -f /tmp/bundle/values.yaml \
            -f /tmp/bundle/config/ \
            -v tanzunet_username="$TDH_REGISTRY_VMWARE_USER" \
            -v tanzunet_password="$TDH_REGISTRY_VMWARE_PASS" \
            -v docker_repository="$TDH_REGISTRY_DOCKER_USER" \
            -v docker_username="$TDH_REGISTRY_DOCKER_USER" \
            -v docker_password="$TDH_REGISTRY_DOCKER_PASS" 2>/dev/null \
            | kbld -f /tmp/bundle/.imgpkg/images.yml -f- 2>/dev/null \
            | kapp deploy -a tanzu-build-service -f- -y > /tmp/kapp.log 2>&1; ret=$?

        cnt=$(kapp inspect -a tanzu-build-service --json | jq -r '.Tables[].Rows[].reconcile_state' | grep -vc "ok")
        while [ $cnt -ne 0 ]; do
          cnt=$(kapp inspect -a tanzu-build-service --json | jq -r '.Tables[].Rows[].reconcile_state' | grep -vc "ok")
          sleep 360
        done
      fi
    fi
  
    #########################################################################################################################
    ################################# TANZU BUILD SERVICE (TBS) - ON HARBOR REGISTRY ########################################
    #########################################################################################################################
  
    if [ "$TDH_SERVICE_REGISTRY_HARBOR" == "true" ]; then 
      DNS_HARBOR=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR)
      TDH_HARBOR_REGISTRY_DNS_HARBOR=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR)
      TDH_HARBOR_REGISTRY_ADMIN_PASSWORD=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_ADMIN_PASSWORD)
  
      messagePrint " ▪ Verify Harbor Registry Access" "$TDH_HARBOR_REGISTRY_DNS_HARBOR"
      messagePrint "   Registry User"                 "admin"
      messagePrint "   Registry Password"             "$(maskPassword \"$TDH_HARBOR_REGISTRY_ADMIN_PASSWORD\")"
  
      cnt=0; ret=1
      while [ $ret -ne 0 -a $cnt -lt 5 ]; do
        docker login $TDH_HARBOR_REGISTRY_DNS_HARBOR -u admin -p $TDH_HARBOR_REGISTRY_ADMIN_PASSWORD > /dev/null 2>&1; ret=$?
        sleep 10
        let cnt=cnt+1
      done

      if [ $ret -ne 0 ]; then
        echo "ERROR: Docker login does not work 5'"
        echo "       => docker login $TDH_HARBOR_REGISTRY_DNS_HARBOR -u admin -p $TDH_HARBOR_REGISTRY_ADMIN_PASSWORD"; exit 1
      fi

      messagePrint " ▪ Relocate the images" "$TDH_REGISTRY_VMWARE_NAME/library/build-service"
      imgpkg copy -b "$TDH_REGISTRY_VMWARE_NAME/build-service/bundle:$TDH_SERVICE_BUILD_SERVICE_VERSION" --to-repo $DNS_HARBOR/library/build-service > /dev/null 2>&1; ret=$?
      if [ $ret -ne 0 ]; then
        echo "ERROR: Image Relocation failed"
        echo "       => imgpkg copy -b \"$TDH_REGISTRY_VMWARE_NAME/build-service/bundle:$TDH_SERVICE_BUILD_SERVICE_VERSION\" --to-repo $DNS_HARBOR/library/build-service"
      fi

      messagePrint " ▪ Pull the Tanzu Build Service bundle image locally" "$TDH_REGISTRY_VMWARE_NAME/library/build-service"
      rm -rf /tmp/bundle
      imgpkg pull -b "$DNS_HARBOR/library/build-service:$TDH_SERVICE_BUILD_SERVICE_VERSION" -o /tmp/bundle > /dev/null 2>&1; ret=$?
      if [ $ret -ne 0 ]; then
        echo "ERROR: Image Relocation failed"
        echo "       => imgpkg pull -b \"$DNS_HARBOR/library/build-service\" -o /tmp/bundle"
      fi

      # --- DEPLOY KAPP ---
      cnt=$(kapp inspect -a tanzu-build-service 2>/dev/null | grep -c "0 resources")
      stt=$(kapp inspect -a tanzu-build-service 2>/dev/null | tail -1)
      if [ "$stt" != "Succeeded" -o $cnt -eq 1 ]; then
        kapp delete -a tanzu-build-service -y >/dev/null 2>&1
        messagePrint " ▪ Deploy Build Service (kapp)" "/tmp/bundle/values.yaml"
        echo "   -----------------------------------------------------------------------------------------------------------"
        echo "   WARNING: This process may take up to 2h. You can run 'kapp inspect -a tanzu-build-service' to see progress "
        echo "   -----------------------------------------------------------------------------------------------------------"
        touch /tmp/bundle/values.yaml

        # --- 1.4.2 Tested ---
	ytt -f /tmp/bundle/values.yaml \
            -f /tmp/bundle/config/ \
            -f $HOME/.tanzu-demo-hub/certificates/ca.pem \
            -v kp_default_repository=$DNS_HARBOR/library/build-service \
            -v kp_default_repository_username=admin \
            -v kp_default_repository_password=$TDH_HARBOR_REGISTRY_ADMIN_PASSWORD \
            -v tanzunet_username="$TDH_REGISTRY_VMWARE_USER" \
            -v tanzunet_password="$TDH_REGISTRY_VMWARE_PASS" \
	    -v descriptor_name='lite' 2>/dev/null \
           | kbld -f /tmp/bundle/.imgpkg/images.yml -f- \
           | kapp deploy -a tanzu-build-service -f- -y > /tmp/kapp.log 2>&1; ret=$?

        # --- 1.2.1 Tested ---
        #ytt -f /tmp/bundle/values.yaml \
        #    -f /tmp/bundle/config/ \
        #    -f $HOME/.tanzu-demo-hub/certificates/ca.pem \
        #    -v tanzunet_username="$TDH_REGISTRY_VMWARE_USER" \
        #    -v tanzunet_password="$TDH_REGISTRY_VMWARE_PASS" \
        #    -v docker_repository="$DNS_HARBOR/library/build-service" \
        #    -v docker_username="admin" \
        #    -v docker_password="$TDH_HARBOR_REGISTRY_ADMIN_PASSWORD" 2>/dev/null \
        #    | kbld -f /tmp/bundle/.imgpkg/images.yml -f- 2>/dev/null \
        #    | kapp deploy -a tanzu-build-service -f- -y > /tmp/kapp.log 2>&1; ret=$?

        cnt=$(kapp inspect -a tanzu-build-service --json | jq -r '.Tables[].Rows[].reconcile_state' | grep -vc "ok")
        while [ $cnt -ne 0 ]; do
          cnt=$(kapp inspect -a tanzu-build-service --json | jq -r '.Tables[].Rows[].reconcile_state' | grep -vc "ok")
          sleep 360
        done
      else
        messageTitle "Verify Tanzu Build Service"
        messagePrint " ▪ Veriify Build Service (kapp)" "tanzu-build-service"
      fi

      stt=$(kubectl get TanzuNetDependencyUpdaters dependency-updater -n build-service | egrep "^dependency-updater" | awk '{ print $3 }')
      ver=$(kubectl get TanzuNetDependencyUpdaters dependency-updater -n build-service | egrep "^dependency-updater" | awk '{ print $2 }')
      messagePrint " ▪ Verify Dependancy Updater (dependency-updater)" "build-service/dependency-updater"
      messagePrint "   Dependancy Version"         "$ver"
      messagePrint "   Dependancy Statuws"         "$stt"

      messagePrint " ▪ Verify the cluster builders" "base, default, full and tiny"
      messageLine
      kp clusterbuilder list
      messageLine
    fi

    CURRENT_DATE=$(date "+%Y-%m-%d %M:%H:%S") 
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_BUILD_SERVICE            "true"
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_BUILD_SERVICE_VERSION    $TDH_SERVICE_BUILD_SERVICE_VERSION
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_BUILD_SERVICE_INSTALLED  "$CURRENT_DATE"
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_REGISTRY_DOCKER          $TDH_SERVICE_REGISTRY_DOCKER
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_REGISTRY_HARBOR          $TDH_SERVICE_REGISTRY_HARBOR
    uodateConfigMap tanzu-demo-hub TDH_REGISTRY_VMWARE_NAME             $TDH_REGISTRY_VMWARE_NAME
    uodateConfigMap tanzu-demo-hub TDH_REGISTRY_VMWARE_USER             $TDH_REGISTRY_VMWARE_USER
    uodateConfigMap tanzu-demo-hub TDH_REGISTRY_VMWARE_PASS             $TDH_REGISTRY_VMWARE_PASS
    uodateConfigMap tanzu-demo-hub TDH_REGISTRY_DOCKER_NAME             $TDH_REGISTRY_DOCKER_NAME
    uodateConfigMap tanzu-demo-hub TDH_REGISTRY_DOCKER_USER             $TDH_REGISTRY_DOCKER_USER
    uodateConfigMap tanzu-demo-hub TDH_REGISTRY_DOCKER_PASS             $TDH_REGISTRY_DOCKER_PASS
    uodateConfigMap tanzu-demo-hub TDH_GITHUB_USER                      $TDH_GITHUB_USER
    uodateConfigMap tanzu-demo-hub TDH_GITHUB_PASS                      $TDH_GITHUB_PASS
  fi

  messagePrint " ▪ Verify Tanzu Build Service" "$TDH_SERVICE_BUILD_SERVICE_VERSION"
  kp clusterbuilder list > /dev/null 2>&1
  [ $? -eq 0 ] && stt="successfuly installed" || stt="installed/failed"
  if [ $DEBUG -eq 1 ]; then
    messagePrint " ▪ Verify Dependency Descriptors" "$stt"
    messageLine
    kp clusterbuilder list | sed '$d'
    messageLine
  else
    messagePrint " ▪ Verify Dependency Descriptors" "$stt"
  fi

  CURRENT_DATE=$(date "+%Y-%m-%d %M:%H:%S")
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_BUILD_SERVICE_VALIDATED  "$CURRENT_DATE"
}

downloadFromPivnet() {
  PRODUCT_SLUG=$1
  SLUG_VER="$2"
  PROD_VER="$3"
  PROD_STR="$4"

  for pid in $(pivnetAPI $PCF_PIVNET_TOKEN GET products/$PRODUCT_SLUG/releases 2> /dev/null | \
             jq -r ".releases[] | select(.version | scan(\"^$SLUG_VER\")).id"); do

    fid=$(pivnetAPI $PCF_PIVNET_TOKEN GET products/$PRODUCT_SLUG/releases/$pid 2>/dev/null | \
          jq -r ".product_files[] | select(.name | scan(\"^$PROD_STR\")) | select(.file_type == \"Software\") | select(.file_version | scan(\"$PROD_VER\")).id")

    if [ "${fid}" != "" ]; then
      # --- ACCEPT EULA ---
      pivnetAPI $PCF_PIVNET_TOKEN POST products/$PRODUCT_SLUG/releases/$pid/eula_acceptance > /dev/null 2>&1
      break
    else
      echo "ERROR: Unable to find $PRODUCT_SLUG $2 on Pivnet"
      exit 1
    fi
  done

  if [ "$fid" == "" ]; then 
    echo "ERROR: Can not find version: $SLUG_VER from ($PRODUCT_SLUG) on PIVNET"
    exit 1
  fi

  file=$(pivnetAPI $PCF_PIVNET_TOKEN GET products/$PRODUCT_SLUG/releases/$pid 2>/dev/null | \
       jq -r ".product_files[] | select(.id | tostring | scan(\"^$fid$\")).aws_object_key" | \
       awk -F'/' '{ print $NF }')

  if [ ! -f /tmp/$file ]; then
    messagePrint " ▪ Download from PIVNET" "$PRODUCT_SLUG"
    messagePrint "   Product Version" "$SLUG_VER"
    messagePrint "   Download Location:" "/tmp/$file"
    pivnetAPIdownload $PCF_PIVNET_TOKEN $PRODUCT_SLUG $pid $fid $file
  else
    messagePrint " ▪ Verify PIVNET Download" "$PRODUCT_SLUG"
    messagePrint "   Product Version:" "$SLUG_VER"
    messagePrint "   Download Location:" "/tmp/$file"
  fi
}

getRootCA() {
  SECRET=$1
  LETSENSCRIPT_INTERMEDIATE_CERT=""
  LETSENSCRIPT_ROOT_CERT=""
  THD_TLS_CERTIFICATE=tdh-cert.pem
  THD_TLS_KEY=tdh-key.pem
  CERT_DIR=$HOME/.tanzu-demo-hub/certificates

  # --- CREATE CERTIFICATES DIRECTORY ---
  [ ! -d $CERT_DIR ] && mkdir $CERT_DIR

  if [ ! -f $CERT_DIR/ca.pem -o $CERT_DIR/$THD_TLS_CERTIFICATE -o $CERT_DIR/$THD_TLS_KEY ]; then 
    cmdLoop kubectl get secrets $SECRET -o json > /tmp/output.json
    jq -r '.data."tls.crt"' /tmp/output.json | base64 -d > $CERT_DIR/$THD_TLS_CERTIFICATE
    jq -r '.data."tls.key"' /tmp/output.json | base64 -d > $CERT_DIR/$THD_TLS_KEY

    #DELETEME
    #kubectl get secret $SECRET -o json | jq -r '.data."tls.crt"' | base64 -d > $CERT_DIR/$THD_TLS_CERTIFICATE
    #kubectl get secret $SECRET -o json | jq -r '.data."tls.key"' | base64 -d > $CERT_DIR/$THD_TLS_KEY

    if [ ! -f $CERT_DIR/$THD_TLS_CERTIFICATE ]; then 
      echo "ERROR: Failed to get tdh-cert.pem"
      echo "       => kubectl get secret $SECRET -o json | jq -r '.data."tls.crt"' | base64 -d > $CERT_DIR/$THD_TLS_CERTIFICATE"
      exit 1
    fi

    if [ ! -f $CERT_DIR/$THD_TLS_KEY ]; then 
      echo "ERROR: Failed to get $THD_TLS_KEY"
      echo "       => kubectl get secret $SECRET -o json | jq -r '.data."tls.key"' | base64 -d > $CERT_DIR/$THD_TLS_KEY"
      exit 1
    fi

    cmdLoop kubectl get secrets $SECRET -o json > /tmp/output.json
    txt=$(jq -r '.data."tls.crt"' /tmp/output.json | base64 -d | openssl x509 -noout -issuer)

    #DELETEME
    txt=$(kubectl get secret $SECRET -o json | jq -r '.data."tls.crt"' | base64 -d | openssl x509 -noout -issuer) 

    if [ $(uname) == "Darwin" ]; then 
      # issuer= /C=US/O=Let's Encrypt/CN=R3
      ca_country=$(echo $txt | awk -F'/' '{ print $2 }' | awk -F'=' '{ print $2 }') 
      ca_issuer=$(echo $txt | awk -F'/' '{ print $3 }' | awk -F'=' '{ print $2 }') 
      ca_cn=$(echo $txt | awk -F'/' '{ print $4 }' | awk -F'=' '{ print $2 }') 
    else
      # issuer=C = US, O = Let's Encrypt, CN = R3
      ca_country=$(echo $txt | awk -F',' '{ print $1 }' | awk '{ print $NF }')
      ca_issuer=$(echo $txt | awk -F',' '{ print $2 }' | sed 's/^.*= //g') 
      ca_cn=$(echo $txt | sed 's/^.*CN = //g' | awk '{ print $1 }')
    fi
  
    messagePrint " ▪ TDH Certificate Issuer:"  "$ca_issuer"
    messagePrint " ▪ TDH Certificate File:"    "$THD_TLS_CERTIFICATE"
    messagePrint " ▪ TDH Certificate CN:"      "$ca_cn"
  
    if [ "$ca_issuer" == "Let's Encrypt" ]; then 
      if [ "$ca_cn" == "R3" ]; then 
        LETSENSCRIPT_INTERMEDIATE_CERT=$ca_cn
        LETSENSCRIPT_INTERMEDIATE_CERT_FILE=lets-encrypt-r3.pem

        url=https://letsencrypt.org/certs/lets-encrypt-r3.pem
        curl $url 2>/dev/null > $CERT_DIR/$LETSENSCRIPT_INTERMEDIATE_CERT_FILE
        if [ $? -eq 0 ]; then
          messagePrint " ▪ Let's Encrypt Intermediate Certificate:"      "$CERT_DIR/$LETSENSCRIPT_INTERMEDIATE_CERT"
          messagePrint " ▪ Let's Encrypt Intermediate Certificate Url:"  "$url"
          messagePrint " ▪ Let's Encrypt Intermediate Certificate File:" "$CERT_DIR/$LETSENSCRIPT_INTERMEDIATE_CERT_FILE"
        else
          echo "ERROR: Failed to optiain the root certificate in $CERT_DIR/$LETSENSCRIPT_INTERMEDIATE_CERT"
          echo "       => curl $url > $CERT_DIR/$LETSENSCRIPT_INTERMEDIATE_CERT_FILE"
          exit 1
        fi
      fi

      if [ "$ca_cn" == "E1" ]; then 
        LETSENSCRIPT_INTERMEDIATE_CERT=$ca_cn
        LETSENSCRIPT_INTERMEDIATE_CERT_FILE=lets-encrypt-e1.pem

        url=https://letsencrypt.org/certs/lets-encrypt-e1.pem
        curl $url 2>/dev/null > $CERT_DIR/$LETSENSCRIPT_INTERMEDIATE_CERT_FILE
        if [ $? -eq 0 ]; then
          messagePrint " ▪ Let's Encrypt Intermediate Certificate:"      "$CERT_DIR/$LETSENSCRIPT_INTERMEDIATE_CERT"
          messagePrint " ▪ Let's Encrypt Intermediate Certificate Url:"  "$url"
          messagePrint " ▪ Let's Encrypt Intermediate Certificate File:" "$CERT_DIR/$LETSENSCRIPT_INTERMEDIATE_CERT_FILE"
        else
          echo "ERROR: Failed to optiain the root certidicate $CERT_DIR/$LETSENSCRIPT_INTERMEDIATE_CERT"
          echo "       => curl $url > $CERT_DIR/$LETSENSCRIPT_INTERMEDIATE_CERT_FILE"
          exit 1
        fi
      fi

      if [ "$LETSENSCRIPT_INTERMEDIATE_CERT_FILE" != "" ]; then
        txt=$(openssl x509 -noout -in $CERT_DIR/$LETSENSCRIPT_INTERMEDIATE_CERT_FILE -issuer)
        if [ $(uname) == "Darwin" ]; then 
          # issuer= /C=US/O=Let's Encrypt/CN=R3
          ca_root_country=$(echo $txt | awk -F'/' '{ print $2 }' | awk -F'=' '{ print $2 }')
          ca_root_issuer=$(echo $txt | awk -F'/' '{ print $3 }' | awk -F'=' '{ print $2 }')
          ca_root_cn=$(echo $txt | awk -F'/' '{ print $4 }' | awk -F'=' '{ print $2 }')
        else
          # issuer=C = US, O = Internet Security Research Group, CN = ISRG Root X1
          ca_root_country=$(echo $txt | awk -F',' '{ print $1 }' | awk '{ print $NF }')
          ca_root_issuer=$(echo $txt | awk -F',' '{ print $2 }' | sed 's/^.* = //g')
          ca_root_cn=$(echo $txt | awk -F',' '{ print $3 }' | sed 's/^.* = //g')
        fi

        if [ "$ca_root_cn" == "ISRG Root X1" ]; then
          LETSENSCRIPT_ROOT_CERT=$ca_cn
          LETSENSCRIPT_ROOT_CERT_FILE=isrgrootx1.pem
  
          url=https://letsencrypt.org/certs/isrgrootx1.pem
          curl $url 2>/dev/null > $CERT_DIR/$LETSENSCRIPT_ROOT_CERT_FILE
          messagePrint " ▪ Let's Encrypt Root Certificate:"      "$CERT_DIR/$LETSENSCRIPT_ROOT_CERT"
          messagePrint " ▪ Let's Encrypt Root Certificate Url:"  "$url"
          messagePrint " ▪ Let's Encrypt Root Certificate File:" "$CERT_DIR/$LETSENSCRIPT_ROOT_CERT_FILE"
        fi

        cat $CERT_DIR/$LETSENSCRIPT_INTERMEDIATE_CERT_FILE $CERT_DIR/$LETSENSCRIPT_ROOT_CERT_FILE > $CERT_DIR/ca.pem
      fi
    else
      echo "ERROR: Certificate Issuer unknown, Please place it manually to $HOME/.tanzu-demo-hub/certificates/ca.pem"
      exit 1
    fi
  fi

  # --- VERIFY ROOTCA WITH CERTIFICATE ---
  stt=$(openssl verify -CAfile $CERT_DIR/ca.pem $CERT_DIR/$THD_TLS_CERTIFICATE)
  mo1=$(openssl x509 -noout -modulus -in $CERT_DIR/$THD_TLS_CERTIFICATE | openssl md5)
  mo2=$(openssl rsa -noout -modulus -in $CERT_DIR/$THD_TLS_KEY | openssl md5) 
  if [ "$mo1" == "$mo2" ]; then vry=ok; else vry=failed; fi

  messageTitle "Verify RootCA with the Certificate ($SECRET)"
  messagePrint " ▪ Verify Cert ($THD_TLS_CERTIFICATE) with RootCA (ca.pem)"    "$stt"
  messagePrint " ▪ Verify Cert ($THD_TLS_CERTIFICATE) with CertKey ($THD_TLS_KEY)"  "$vry"
}

InstallMetallLB() {
  #if [ "$TDH_TKGMC_LOADBALANCER" != "metallb" ]; then
  #  return
  #fi

  NAMESPACE=metallb
  createNamespace $NAMESPACE > /dev/null 2>&1

  cnt=$(helm list -q -n $NAMESPACE | egrep -c "^metallb")
  if [ $cnt -eq 0 ]; then
    messageTitle "Installing MetallLB Loadbalancer"
    
    messagePrint " ▪ Add Helm Repo (metallb)" "https://metallb.github.io/metallb"
    helmRepoAdd metallb https://metallb.github.io/metallb

    messagePrint " ▪ Create Namespace" "$NAMESPACE"
    deleteNamespace $NAMESPACE 
    createNamespace $NAMESPACE > /dev/null 2>&1
    patchServiceAccount default $NAMESPACE

    # --- DOCKER PULL SECRET ---
    #dockerPullSecret $NAMESPACE

    HELM_VALUES=/tmp/metallb_values.yaml

    ipa=$(docker network inspect kind | jq -r '.[].IPAM.Config[].Subnet' | egrep "^[0-9]*\." | awk -F'.' '{ printf("%s.%s\n",$1,$2)}')

    echo "global:"                                     >  $HELM_VALUES
    echo "  imageRegistry: docker.io"                  >> $HELM_VALUES
    echo "  imagePullSecrets:"                         >> $HELM_VALUES
    echo "    - tdh-docker-repo"                       >> $HELM_VALUES
    echo "configInline:"                               >> $HELM_VALUES
    echo "  address-pools:"                            >> $HELM_VALUES
    echo "   - name: default"                          >> $HELM_VALUES
    echo "     protocol: layer2"                       >> $HELM_VALUES
    echo "     addresses:"                             >> $HELM_VALUES
    echo "     - ${ipa}.255.1-${ipa}.255.254"          >> $HELM_VALUES

    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      helm install metallb metallb/metallb -f $HELM_VALUES -n $NAMESPACE --wait --wait-for-jobs --timeout 10m > /dev/null 2>&1; ret=$?
      [ $ret -eq 0 ] && break
      let cnt=cnt+1
      sleep 30
    done
  
    if [ $ret -ne 0 ]; then
      echo "ERROR: failed to install metallb/metallb"
      echo "       => helm install metallb metallb/metallb -f $HELM_VALUES -n $NAMESPACE"
      exit 1
    fi

    cnt=0
    while [ $cnt -eq 0 ]; do
      cnt=$(kubectl get pods -n $NAMESPACE | sed 1d | grep -vc Running)
      if [ $cnt -eq 0 ]; then cnt=1; fi
      sleep 5
    done
  fi
}

dockerRateLimit() {
  TMP_NAMESPACE=$1

  cmdLoop kubectl get secrets -o json > /tmp/outpot.json
  nam=$(jq -r --arg key "docker-registry" '.items[].metadata | select(.name == $key).name' /tmp/outpot.json)
  if [ "$nam" != "docker-registry" ]; then 
    cmdLoop kubectl -n $TMP_NAMESPACE create secret docker-registry tdh-docker-repo \
            --docker-server $TDH_REGISTRY_DOCKER_NAME \
            --docker-username $TDH_REGISTRY_DOCKER_USER \
            --docker-password $TDH_REGISTRY_DOCKER_PASS
 
    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      cmdLoop kubectl patch serviceaccount default -p '{"imagePullSecrets": [{"name": "tdh-docker-repo"}]}' -n $TMP_NAMESPACE > /dev/null 2>&1; ret=$?
      [ $ret -eq 0 ] && break
      let cnt=cnt+1
      sleep 30
    done

    if [ $ret -ne 0 ]; then
      echo "ERROR: Failed to Patch Service account"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"tdh-docker-repo\"}]}' -n $TMP_NAMESPACE"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"tdh-docker-repo\"}]}' -n $TMP_NAMESPACE"
      fi

      exit 1
    fi
  fi
}

InstallContour() {
  if [ "$TDH_SERVICE_INGRESS_CONTOUR" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_ENABLED        "false"
    return
  fi

  NAMESPACE=ingress-contour

  cnt=$(kubectl get pods -A | egrep "^$NAMESPACE " | grep -c Running)
  if [ $? -ne 0 ]; then
    # --- INSTALL CONTOUR INGRESS ---
    messageTitle "Install Ingress Contour"
    messagePrint " ▪ Create Namespace" "$NAMESPACE" 
    deleteNamespace $NAMESPACE > /dev/null 2>&1
    createNamespace $NAMESPACE > /dev/null 2>&1
    helmRepoAdd bitnami https://charts.bitnami.com/bitnami

    # --- DOCKER PULL SECRET ---
    dockerPullSecret $NAMESPACE

    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      kubectl patch serviceaccount default -p '{"imagePullSecrets": [{"name": "tdh-docker-repo"}]}' -n $NAMESPACE > /dev/null 2>&1; ret=$?
      [ $ret -eq 0 ] && break
      let cnt=cnt+1
      sleep 30
    done

    if [ $ret -ne 0 ]; then
      echo "ERROR: Failed to Patch Service account"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"tdh-docker-repo\"}]}' -n $NAMESPACE"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"tdh-docker-repo\"}]}' -n $NAMESPACE"
      fi

      exit 1
    fi

    messagePrint " ▪ Install Contour Helm Chart" "bitnami/contour"
    helmRepoAdd bitnami https://charts.bitnami.com/bitnami

    if [ "$TDH_SERVICE_INGRESS_CONTOUR_VERSION" == "" -o "$TDH_SERVICE_INGRESS_CONTOUR_VERSION" == "latest" ]; then 
      HELM_CONTOUR_VERSION=$(helm search repo bitnami/contour | egrep "^bitnami/contour " | tail -1 | awk '{ print $2 }') 
    else
      HELM_CONTOUR_VERSION=$TDH_SERVICE_INGRESS_CONTOUR_VERSION
    fi
    messagePrint " ▪ Install Contour Helm Chart Version" "$HELM_CONTOUR_VERSION"

    # 5.6.0 Issues with Spring Cloud Demo and Contour 1.18.1
    # 5.5.3 Issues with Spring Cloud Demo and Contour 1.18.1
    # 5.1.0 Works

    echo "global:"                                     >  /tmp/contour_values.yaml
    echo "  imageRegistry: docker.io"                  >> /tmp/contour_values.yaml
    echo "  imagePullSecrets:"                         >> /tmp/contour_values.yaml
    echo "    - tdh-docker-repo"                       >> /tmp/contour_values.yaml

    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      helm install -f /tmp/contour_values.yaml ingress bitnami/contour -n $NAMESPACE --version $HELM_CONTOUR_VERSION --wait --wait-for-jobs --timeout 10m > /dev/null 2>&1; ret=$?
      [ $ret -eq 0 ] && break
      let cnt=cnt+1
      sleep 30
    done

    if [ $ret -ne 0 ]; then
      echo "ERROR: failed to install bitnami/contour"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ helm install -f /tmp/contour_values.yaml ingress bitnami/contour -n $NAMESPACE --version $HELM_CONTOUR_VERSION"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => helm install -f /tmp/contour_values.yaml ingress bitnami/contour -n $NAMESPACE --version $HELM_CONTOUR_VERSION"
      fi

      exit 1
    fi

    cnt=0
    while [ $cnt -eq 0 ]; do
      cnt=$(kubectl get pods -n ingress-contour | sed 1d | grep -vc Running)
      if [ $cnt -eq 0 ]; then cnt=1; fi
      sleep 5
    done
  fi

  ver=$(helm -n $NAMESPACE ls -o json | jq -r '.[].app_version')
  crt=$(helm -n $NAMESPACE ls -o json | jq -r '.[].chart')
  stt=$(helm -n $NAMESPACE ls -o json | jq -r '.[].status')
  dat=$(helm -n $NAMESPACE ls -o json | jq -r '.[].updated')

  messageTitle "Verify Ingress Contour"
  messagePrint " ▪ Ingress Contour Namespace:"         "$NAMESPACE"
  messagePrint " ▪ Ingress Contour Helm Chart:"        "$crt"
  messagePrint " ▪ Ingress Contour Version:"           "$ver"
  messagePrint " ▪ Ingress Contour Status:"            "$stt"
  messagePrint " ▪ Ingress Contour Installed/Updated:" "$dat"

  # --- VERIFY DNS DOMAIN ---
  verifyHostedZone ${TDH_ENVNAME}.$AWS_HOSTED_DNS_DOMAIN

  ipa=""; cnt=0
  while [ "$ipa" == "" -a $cnt -lt 15 ]; do
    ipa=$(kubectl describe svc ingress-contour-envoy --namespace $NAMESPACE | grep Ingress | awk '{print $3}')
    [ "$ipa" != "" ] && break
    let cnt=cnt+1
    sleep 30
  done

  if [ "$ipa" == "" ]; then
    echo "ERROR: Failed to gather IP Adress of the ingress-contour-envoy service in namespace ingress-contour"
    echo "       Please check manually:"
    echo "       => kubectl get svc --namespace $NAMESPACE"
    exit 1
  fi

  # --- SET DNS RECORD ---
  if [ "$TDH_INFRASTRUCTURE" == "minikube" -o "$TDH_INFRASTRUCTURE" == "vSphere" -o "$TDH_INFRASTRUCTURE" == "Azure" -o "$TDH_INFRASTRUCTURE" == "docker" ]; then 
    setDNSrecord "$ipa" "${TDH_ENVNAME}" "${AWS_HOSTED_DNS_DOMAIN}" "*.apps-contour" "A"
  else
    setDNSalias "$ipa" "${TDH_ENVNAME}" "${AWS_HOSTED_DNS_DOMAIN}" "*.apps-contour" "A"
  fi

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_ENABLED        "true"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_LB_IP          "$ipa"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_LB_DOMAIN      "apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_NAMESPACE      "$NAMESPACE"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_CHART_NAME     "$crt"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_CHART_VERSION  "$ver"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_CHART_STATUS   "$stt"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_CHART_UPDATE   "$dat"
}

# ------------------------------------------------------------------------------------------
# Function Name ......: cmdLoop
# Function Purpose ...: Execute Command in a Loop until its working
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Command to execure
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
# References: https://cert-manager.io/docs/faq/acme ................. Debugging Cert Manager
# ------------------------------------------------------------------------------------------
createClusterIssuer() {
  #https://cert-manager.io/docs/tutorials/acme/dns-validation/
  NAMESPACE=cert-manager

  if [ "${TDH_HARBOR_STAGING_TLS_CERT}" == "true" ]; then
    LETSENSCRIPT_SERVER="https://acme-staging-v02.api.letsencrypt.org/directory"
  else
    LETSENSCRIPT_SERVER="https://acme-v02.api.letsencrypt.org/directory"
  fi

  # --- NOT CERTIFICATE CLEANUP ALL AND START OVER ---
  cmdLoop kubectl get certificate -o json > /tmp/output.json
  stt=$(jq -r --arg key "$TDH_TLS_CERT" '.items[] | select(.metadata.name == $key).status.conditions[].type' /tmp/output.json)
  if [ "${stt}" == "" -o "${stt}" == "False" ]; then
    messageTitle "Cleaning up Let's Enscript objects"
    cmdLoop kubectl delete clusterissuer,order,challenge,certificate --all > /dev/null 2>&1
  fi

  cmdLoop kubectl get clusterissuer -o json > /tmp/output.json
  stt=$(jq -r --arg key "$TDH_TLS_ISSUER_NAME" '.items[] | select(.metadata.name == $key).status.conditions[].type' /tmp/output.json) 
  if [ "${stt}" == "" -o "${stt}" == "False" ]; then
    cmdLoop aws route53 list-hosted-zones-by-name --dns-name ${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN} > /tmp/output.json
    AWS_HOSTED_ZONE=$(jq -r ".HostedZones[] | select(.Name | scan(\"^${zone}.\")).Id" /tmp/output.json)
    cmdLoop aws route53 list-hosted-zones --output json > /tmp/output.json
    AWS_HOSTED_ZONE_ID=$(jq -r --arg key "$${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}." '.HostedZones[] | select(.Name == $key).Id' /tmp/output.json | \
                         awk -F '/' '{ print $NF }')

    messageTitle "Create LetsEnscript ClusterIssuer"
    messagePrint " ▪ LetsEnscript Issuer Name"         "$TDH_TLS_ISSUER_NAME"
    messagePrint " ▪ LetsEnscript Solver"              "route53"
    messagePrint " ▪ LetsEnscript Solver ZoneID"       "$AWS_HOSTED_ZONE_ID"
    messagePrint " ▪ LetsEnscript DNSZone"             "*.apps-contour.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"
    messagePrint " ▪ LetsEnscript DNSZone"             "*.apps-nginx.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"
    messagePrint " ▪ LetsEnscript DNSZone"             "*.gitlab.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"

    if [ "${AWS_CERT_ACCESS_KEY}" == "" -o "${AWS_CERT_SECRET_KEY}" == "" ]; then
      messagePrint " ▪ LetsEnscript using AWS Route53 AccessKey" $(maskPassword "$AWS_ACCESS_KEY")
      messagePrint " ▪ LetsEnscript using AWS Route53 SecretKey" $(maskPassword "$AWS_SECRET_KEY")
      export AWS_CERT_ACCESS_KEY=$AWS_ACCESS_KEY
      export AWS_CERT_SECRET_KEY=$AWS_SECRET_KEY
    else
      messagePrint " ▪ LetsEnscript using AWS Route53 AccessKey" $(maskPassword "$AWS_CERT_ACCESS_KEY")
      messagePrint " ▪ LetsEnscript using AWS Route53 SecretKey" $(maskPassword "$AWS_CERT_SECRET_KEY")
    fi

    TDH_TLS_ISSUER_FILE=/tmp/$TDH_TLS_ISSUER_NAME
    echo "apiVersion: cert-manager.io/v1alpha2"                                    >  $TDH_TLS_ISSUER_FILE
    echo "kind: ClusterIssuer"                                                     >> $TDH_TLS_ISSUER_FILE
    echo "metadata:"                                                               >> $TDH_TLS_ISSUER_FILE
    echo "  name: $TDH_TLS_ISSUER_NAME"                                            >> $TDH_TLS_ISSUER_FILE
    echo "spec:"                                                                   >> $TDH_TLS_ISSUER_FILE
    echo "  acme:"                                                                 >> $TDH_TLS_ISSUER_FILE
    echo "    email: $TDH_CERTMANAGER_EMAIL"                                       >> $TDH_TLS_ISSUER_FILE
    echo "    privateKeySecretRef:"                                                >> $TDH_TLS_ISSUER_FILE
    echo "      name: $TDH_TLS_ISSUER_NAME"                                        >> $TDH_TLS_ISSUER_FILE
    echo "    server: $LETSENSCRIPT_SERVER"                                        >> $TDH_TLS_ISSUER_FILE
    echo "    solvers:"                                                            >> $TDH_TLS_ISSUER_FILE
    echo "    - selector:"                                                         >> $TDH_TLS_ISSUER_FILE
    echo "        dnsZones:"                                                       >> $TDH_TLS_ISSUER_FILE
    echo "          - \"*.apps-contour.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}\""  >> $TDH_TLS_ISSUER_FILE
    echo "          - \"*.apps-nginx.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}\""    >> $TDH_TLS_ISSUER_FILE
    echo "          - \"*.gitlab.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}\""        >> $TDH_TLS_ISSUER_FILE
    echo "      dns01:"                                                            >> $TDH_TLS_ISSUER_FILE
    echo "        route53:"                                                        >> $TDH_TLS_ISSUER_FILE
    echo "          region: $AWS_REGION"                                           >> $TDH_TLS_ISSUER_FILE
    echo "          accessKeyID: $AWS_CERT_ACCESS_KEY"                             >> $TDH_TLS_ISSUER_FILE
    echo "          secretAccessKeySecretRef:"                                     >> $TDH_TLS_ISSUER_FILE
    echo "            name: route53-credentials-secret"                            >> $TDH_TLS_ISSUER_FILE
    echo "            key: aws-credentials"                                        >> $TDH_TLS_ISSUER_FILE
    echo "          # you can also assume a role with these credentials"           >> $TDH_TLS_ISSUER_FILE
    echo "          hostedZoneID: $AWS_HOSTED_ZONE_ID"                             >> $TDH_TLS_ISSUER_FILE

    # --- CREATE/RECREATE AWS ROUTE53 SECRET ---
    #deleteSecret cert-manager route53-credentials-secret
    #deleteSecret default route53-credentials-secret

    #DELETEME
    kubectl -n cert-manager delete secret route53-credentials-secret > /dev/null 2>&1
    kubectl -n default delete secret route53-credentials-secret > /dev/null 2>&1

    echo "$AWS_CERT_SECRET_KEY" > /tmp/aws-credentials
    kubectl -n cert-manager create secret generic route53-credentials-secret \
            --from-file=aws-credentials=/tmp/aws-credentials > /dev/null 2>&1

    # --- VERIFY CERTIFICATE ---
    kubectl -n cert-manager get secret route53-credentials-secret -o json | jq -r '.data."aws-credentials"' | base64 -d > /tmp/aws-credentials_secret
    diff /tmp/aws-credentials /tmp/aws-credentials_secret > /dev/null 2>&1
    if [ $? -ne 0 ]; then 
      echo "ERROR: AWS Credentials in the secret: route53-credentials-secret to not match"
      echo "       => diff /tmp/aws-credentials /tmp/aws-credentials_secret"
      exit  1
    else
      rm -f /tmp/aws-credentials /tmp/aws-credentials_secret
    fi

    # --- RECREATE ISSUER ---
    kubectl -n $NAMESPACE delete --all clusterissuer,certificate,order,challenge > /dev/null 2>&1
    sleep 120
    kubectl -n $NAMESPACE apply -f $TDH_TLS_ISSUER_FILE > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: failed to deploy letsenscript issuer"
      echo "       => kubectl -n $NAMESPACE apply -f $TDH_TLS_ISSUER_FILE"
      exit 1
    fi

    cnt=0
    while [ $cnt -eq 0 ]; do
      # --- VERIFY ISSUER ---
      issuer_reason=$(kubectl get clusterissuer $TDH_TLS_ISSUER_NAME -n kube-system -o json 2>/dev/null | \
                    jq -r '.status.conditions[].reason' 2>/dev/null)
      issuer_status=$(kubectl get clusterissuer $TDH_TLS_ISSUER_NAME -n kube-system -o json 2>/dev/null | \
                    jq -r '.status.conditions[].status' 2>/dev/null)

      if [ "$issuer_reason" == "ACMEAccountRegistered" -a "$issuer_status" == "True" ]; then cnt=1; fi
      sleep 5
    done

    messagePrint " ▪ LetsEnscript Request Reason"      "$issuer_reason"
    messagePrint " ▪ LetsEnscript Request Status"      "$issuer_status"
  else
    issuer_reason=$(kubectl get clusterissuer letsencrypt-staging -n kube-system -o json 2>/dev/null | \
                  jq -r '.status.conditions[].reason')
    issuer_status=$(kubectl get clusterissuer letsencrypt-staging -n kube-system -o json 2>/dev/null | \
                  jq -r '.status.conditions[].status')
    issuer_date=$(kubectl get clusterissuer letsencrypt-staging -n kube-system -o json 2>/dev/null | \
                  jq -r '.status.conditions[].lastTransitionTime')

    messageTitle "Verify LetsEnscript ClusterIssuer"
    messagePrint " ▪ LetsEnscript Issuer Name"         "$TDH_TLS_ISSUER_NAME"
    messagePrint " ▪ LetsEnscript DNSZone"             "*.apps-contour.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"
    messagePrint " ▪ LetsEnscript DNSZone"             "*.apps-nginx.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"
    messagePrint " ▪ LetsEnscript Request Requested"   "$issuer_date"
    messagePrint " ▪ LetsEnscript Request Reason"      "$issuer_reason"
    messagePrint " ▪ LetsEnscript Request Status"      "$issuer_status"
  fi

  # --- CREATE CERTIFICATE ---
  stt_rdy=$(kubectl get certificate $TDH_TLS_CERT -o json 2>/dev/null | \
            jq -r '.status.conditions[] | select(.type == "Ready").status')
  kubectl get secret $TDH_TLS_SECRET > /dev/null 2>&1; stt_sec=$?
  kubectl get certificate $TDH_TLS_CERT > /dev/null 2>&1; stt_crt=$?
  if [ "${stt_rdy}" == "False" -o $stt_sec -ne 0 -o $stt_crt -ne 0 ]; then
    # --- CLEANUP OLD STUFF ---
    #deleteSecret default $TDH_TLS_SECRET
    #deleteCertificate default $TDH_TLS_CERT 

    #DELETEME
    kubectl delete secret $TDH_TLS_SECRET > /dev/null 2>&1
    kubectl delete certificate $TDH_TLS_CERT > /dev/null 2>&1

    messageTitle "Create LetsEnscript Certificate"
    CERTIFICATE_CONFIG=/tmp/certificate.yaml
    echo "apiVersion: cert-manager.io/v1"                       >  $CERTIFICATE_CONFIG
    echo "kind: Certificate"                                    >> $CERTIFICATE_CONFIG
    echo "metadata:"                                            >> $CERTIFICATE_CONFIG
    echo "  name: $TDH_TLS_CERT"                                >> $CERTIFICATE_CONFIG
    echo "  namespace: default"                                 >> $CERTIFICATE_CONFIG
    echo "spec:"                                                >> $CERTIFICATE_CONFIG
    echo "  secretName: $TDH_TLS_SECRET"                        >> $CERTIFICATE_CONFIG
    echo "  issuerRef:"                                         >> $CERTIFICATE_CONFIG
    echo "    name: letsencrypt-staging"                        >> $CERTIFICATE_CONFIG
    echo "    kind: ClusterIssuer"                              >> $CERTIFICATE_CONFIG
    echo "  dnsNames:"                                          >> $CERTIFICATE_CONFIG
    #echo "  - \"$HARBOR_HOSTNAME\""                             >> $CERTIFICATE_CONFIG
    #echo "  - \"$NOTARY_HOSTNAME\""                             >> $CERTIFICATE_CONFIG
    echo "  - '*.apps-contour.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}'"     >> $CERTIFICATE_CONFIG
    echo "  - '*.apps-nginx.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}'"       >> $CERTIFICATE_CONFIG
    echo "  - '*.gitlab.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}'"       >> $CERTIFICATE_CONFIG

    messageTitle " - This may take a couple of minutes ...."
    kubectl create -f $CERTIFICATE_CONFIG > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: failed to deploy letsenscript certificate"
      echo "       => kubectl create -f $CERTIFICATE_CONFIG"
      exit 1
    fi

    sleep 30

    cnt=0

    cmdLoop kubectl get certificate $TDH_TLS_CERT -o json > /tmp/output.json
    stt=$(jq -r '.status.conditions[] | select(.type == "Ready").status' /tmp/output.json)
    #DELETEME
    #stt=$(kubectl get certificate $TDH_TLS_CERT -o json 2>/dev/null | \
    #        jq -r '.status.conditions[] | select(.type == "Ready").status')

    while [ $cnt -lt 5 -a "$stt" != "True" ]; do
      cmdLoop kubectl get certificate $TDH_TLS_CERT -o json > /tmp/output.json
      stt=$(jq -r '.status.conditions[] | select(.type == "Ready").status' /tmp/output.json)

      #DELETEME
      #stt=$(kubectl get certificate $TDH_TLS_CERT -o json 2>/dev/null | \
      #      jq -r '.status.conditions[] | select(.type == "Ready").status')
      if [ "$stt" == "True" ]; then cnt=1; fi
      sleep 30
      let cnt=cnt+1
    done

    cmdLoop kubectl get certificate $TDH_TLS_CERT -o json > /tmp/output.json
    stt=$(jq -r '.status.conditions[] | select(.type == "Ready").status' /tmp/output.json)

    #DELETEME
    #stt=$(kubectl get certificate $TDH_TLS_CERT -o json 2>/dev/null | \
    #        jq -r '.status.conditions[] | select(.type == "Ready").status')
    if [ "$stt" != "True" ]; then 
      cmdLoop kubectl get challenge $challenge_obj -o json > /tmp/output.json
      challenge_pst=$(jq -r '.status.presented' /tmp/output.json)

      #DELETEME
      #challenge_pst=$(kubectl get challenge $challenge_obj -o json | jq -r '.status.presented')
      if [ "$challenge_pst" == "false" ]; then
         echo "------------------------------------------------------------------------------------------------------------------------"
         echo "ERROR: Let's Enscript failed to verify the AWS Route53 challenge"
         cmdLoop kubectl get challenge $challenge_obj -o json > /tmp/output.json
         jq -r '.status.reason' /tmp/output.json
         echo "------------------------------------------------------------------------------------------------------------------------"
         exit 1
      fi
    fi

    if [ "$stt" == "True" ]; then
      cmdLoop kubectl get certificate $TDH_TLS_CERT -o json > /tmp/output.json
      stt=$(jq -r '.status.conditions[].status' /tmp/output.json)
      msg=$(jq -r '.status.conditions[].message' /tmp/output.json)
      tim=$(jq -r '.status.conditions[].lastTransitionTime' /tmp/output.json)

      #DELETEME
      #tim=$(kubectl get certificate $TDH_TLS_CERT -o json 2>/dev/null | jq -r '.status.conditions[].lastTransitionTime')
      #msg=$(kubectl get certificate $TDH_TLS_CERT -o json 2>/dev/null | jq -r '.status.conditions[].message')
      #tim=$(kubectl get certificate $TDH_TLS_CERT -o json 2>/dev/null | jq -r '.status.conditions[].lastTransitionTime')
      messagePrint " ▪ LetsEnscript Certificate Name"         "$TDH_TLS_CERT"
      messagePrint " ▪ LetsEnscript Certificate Issued"       "$tim"
      messagePrint " ▪ LetsEnscript Certificate Message"      "$msg"
      messagePrint " ▪ LetsEnscript Certificate Status"       "$stt"
      messagePrint " ▪ LetsEnscript Certificate Secret"       "${TDH_TLS_SECRET}"
    else
      messagePrint " ▪ LetsEnscript Certificate Status"       "$stt"
      echo "   ------------------------------------------------------------------------------------------------------------------------"
      kubectl describe certificate $TDH_TLS_CERT
      echo "   ------------------------------------------------------------------------------------------------------------------------"
      echo "=> kubectl describe clusterissuer letsencrypt-staging"
      echo "=> kubectl describe order"
      echo "=> kubectl describe challenge"
      echo "=> kubectl describe certificate"
      exit 1
    fi
  else
    cmdLoop kubectl get certificate $TDH_TLS_CERT -o json > /tmp/output.json
    stt=$(jq -r '.status.conditions[].status' /tmp/output.json)

    #DELETEME
    #stt=$(kubectl get certificate $TDH_TLS_CERT -o json 2>/dev/null | jq -r '.status.conditions[].status')
    if [ "$stt" != "True" ]; then
      messagePrint " ▪ LetsEnscript Certificate Status"       "$stt"
      echo "   ------------------------------------------------------------------------------------------------------------------------"
      kubectl describe certificate $TDH_TLS_CERT
      echo "   ------------------------------------------------------------------------------------------------------------------------"
      echo "=> kubectl describe clusterissuer letsencrypt-staging"
      echo "=> kubectl describe order"
      echo "=> kubectl describe challenge"
      echo "=> kubectl describe certificate"
      exit 1
    fi
  fi

  messageTitle "Verify LetsEnscript Certificate"
  messagePrint " ▪ LetsEnscript Certificate Name"       "$TDH_TLS_CERT"
  messagePrint " ▪ LetsEnscript Certificate Secret"     "$TDH_TLS_SECRET"

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_CERTIFICATE_NAME   "$TDH_TLS_CERT"
  uodateConfigMap tanzu-demo-hub TDH_CERTIFICATE_SECRET "$TDH_TLS_SECRET"

  echo "------------------------------------------------------------------------------------------------------------------------"
  kubectl get secret tanzu-demo-hub-tls -o json | jq -r '.data."tls.crt"' | base64 -d | \
          openssl x509 -inform pem -noout -text | grep $AWS_HOSTED_DNS_DOMAIN
  echo "------------------------------------------------------------------------------------------------------------------------"
}

InstallCertManager() {
  NAMESPACE=cert-manager

  cnt=$(kubectl get pods -A | egrep "^$NAMESPACE " | grep -c Running)
  if [ $cnt -eq 0 ]; then
    # --- INSTALL CONTOUR INGRESS ---
    messageTitle "Cert Manager"

    deleteNamespace $NAMESPACE > /dev/null 2>&1
    createNamespace $NAMESPACE > /dev/null 2>&1
    helmRepoAdd jetstack https://charts.jetstack.io

    #DELETEME
    #helm repo add jetstack https://charts.jetstack.io -n $NAMESPACE > /dev/null 2>&1
    #helm install cert-manager jetstack/cert-manager --namespace cert-manager \
    #   --version v1.0.2 --set installCRDs=true > /dev/null 2>&1

    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      helm install cert-manager jetstack/cert-manager -n cert-manager --version v1.1.0 --set installCRDs=true --wait --wait-for-jobs --timeout 10m > /dev/null 2>&1; ret=$?
      [ $ret -eq 0 ] && break
      let cnt=cnt+1
      sleep 30
    done

    if [ $? -ne 0 ]; then
      echo "ERROR: failed to install jetstack/cert-manager "
      echo "       => helm install cert-manager jetstack/cert-manager --namespace cert-manager \\"
      echo "          --version v1.1.0 --set installCRDs=true"
      exit 1
    fi

    cnt=0
    while [ $cnt -eq 0 ]; do
      cnt=$(kubectl get pods -n $NAMESPACE | sed 1d | grep -vc Running)
      if [ $cnt -eq 0 ]; then cnt=1; fi

      sleep 5
    done
  fi

  ver=$(helm -n cert-manager ls -o json | jq -r '.[].app_version')
  crt=$(helm -n cert-manager ls -o json | jq -r '.[].chart')
  stt=$(helm -n cert-manager ls -o json | jq -r '.[].status')
  dat=$(helm -n cert-manager ls -o json | jq -r '.[].updated')

  messageTitle "Verify Cert Manager"
  messagePrint " ▪ Cert Manager Namespace:"         "$NAMESPACE"
  messagePrint " ▪ Cert Manager Helm Chart:"        "$crt"
  messagePrint " ▪ Cert Manager Version:"           "$ver"
  messagePrint " ▪ Cert Manager Status:"            "$stt"
  messagePrint " ▪ Cert Manager Installed/Updated:" "$dat"

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_ENABLED        "true"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_NAMESPACE      "$NAMESPACE"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_CHART_NAME     "$crt"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_CHART_VERSION  "$ver"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_CHART_STATUS   "$stt"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_CHART_UPDATE   "$dat"
}

InstallHarborRegistry() {
  NAMESPACE=registry-harbor
  HELMCHART=harbor

  if [ "$TDH_SERVICE_REGISTRY_HARBOR" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_REGISTRY_HARBOR        "false"
    return
  fi

  HARBOR_TLS_SECRET=$TDH_TLS_SECRET
  NOTARY_TLS_SECRET=$TDH_TLS_SECRET
  HARBOR_HOSTNAME="harbor.apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"
  NOTARY_HOSTNAME="notary.apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"

  messageTitle "Harbor Registry"
  cnt=$(kubectl get pods -A | egrep -c "^$NAMESPACE") 
  if [ $cnt -eq 0 ]; then
    # --- VERIFY SERVICE CONFIGURATION ---
    checkKubernetesServices harbor

    messagePrint " ▪ Install Harbor Registry" "bitnami/harbor"
    messagePrint "   Harbor Registry Server"               "harbor.apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"
    messagePrint "   Harbor Notary Server "                "notary.apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"
    messagePrint "   Harbor Admin Password"                $(maskPassword "$TDH_HARBOR_ADMIN_PASSWORD")
    messagePrint "   Harbor TLS Staging Cert"              "$TDH_HARBOR_STAGING_TLS_CERT"

    # --- INSTALL CONTOUR INGRESS ---
    messagePrint " ▪ Create Namespace" "$NAMESPACE"
    deleteNamespace $NAMESPACE > /dev/null 2>&1
    createNamespace $NAMESPACE > /dev/null 2>&1
    deleteHelmChart $NAMESPACE $HELMCHART
    patchServiceAccount default $NAMESPACE

    # --- DOCKER PULL SECRET ---
    dockerPullSecret $NAMESPACE

    # --- COPY SECRET TO NAMESPACE ---
    cmdLoop kubectl get secret tanzu-demo-hub-tls --namespace=default  -oyaml | grep -v '^\s*namespace:\s' | \
    cmdLoop kubectl apply --namespace=$NAMESPACE -f - > /dev/null 2>&1

    HARBOR_VALUES=/tmp/harbor_values.yaml
    echo "harborAdminPassword: $TDH_HARBOR_ADMIN_PASSWORD"                                                                      >  $HARBOR_VALUES

    echo ""                                                                                                                     >> $HARBOR_VALUES
    echo "global:"                                                                                                              >> $HARBOR_VALUES
    echo "  imageRegistry: docker.io"                                                                                           >> $HARBOR_VALUES
    echo "  imagePullSecrets:"                                                                                                  >> $HARBOR_VALUES
    echo "    - tdh-docker-repo"                                                                                                >> $HARBOR_VALUES

    if [ "${TDH_DEPLOYMENT_CLOUD}" == "vSphere" -a "${TDH_TKGMC_TKG_TYPE}" == "tkgs" ]; then
      echo "  storageClass: $VSPHERE_TKGS_STORAGE_CLASS"                                                                       >> $HARBOR_VALUES
    fi

    echo ""                                                                                                                     >> $HARBOR_VALUES
    echo "service:"                                                                                                             >> $HARBOR_VALUES
    echo "  type: ClusterIP"                                                                                                    >> $HARBOR_VALUES
    echo "  tls:"                                                                                                               >> $HARBOR_VALUES
    echo "    enabled: true"                                                                                                    >> $HARBOR_VALUES
    echo "    existingSecret: $HARBOR_TLS_SECRET"                                                                               >> $HARBOR_VALUES
    echo "    notaryExistingSecret: $NOTARY_TLS_SECRET"                                                                         >> $HARBOR_VALUES
    echo ""                                                                                                                     >> $HARBOR_VALUES

    # Not required anymore because of HTTPProxy
    #echo "ingress:"                                                                                                             >> $HARBOR_VALUES
    #echo "  enabled: true"                                                                                                      >> $HARBOR_VALUES
    #echo "  hosts:"                                                                                                             >> $HARBOR_VALUES
    #echo "    core: $HARBOR_HOSTNAME"                                                                                           >> $HARBOR_VALUES
    #echo "    notary: $NOTARY_HOSTNAME"                                                                                         >> $HARBOR_VALUES
    #echo "  annotations:"                                                                                                       >> $HARBOR_VALUES
    #echo "    ingress.kubernetes.io/force-ssl-redirect: \"true\"   # force https, even if http is requested"                    >> $HARBOR_VALUES
    #echo "    kubernetes.io/ingress.class: contour                 # using Contour for ingress"                                 >> $HARBOR_VALUES

    # https://github.com/goharbor/harbor-helm/blob/master/values.yaml
    echo "externalURL: https://harbor.apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"                                         >> $HARBOR_VALUES
    echo ""                                                                                                                     >> $HARBOR_VALUES
    echo "portal:"                                                                                                              >> $HARBOR_VALUES
    echo "  tls:"                                                                                                               >> $HARBOR_VALUES
    echo "    existingSecret: $HARBOR_TLS_SECRET"                                                                               >> $HARBOR_VALUES
    echo ""                                                                                                                     >> $HARBOR_VALUES
    echo "persistence:"                                                                                                         >> $HARBOR_VALUES
    echo "  enabled: true"                                                                                                      >> $HARBOR_VALUES
    echo "  resourcePolicy: 'keep'"                                                                                             >> $HARBOR_VALUES
    echo "  persistentVolumeClaim:"                                                                                             >> $HARBOR_VALUES
    echo "    registry:"                                                                                                        >> $HARBOR_VALUES
    echo "      accessMode: ReadWriteOnce"                                                                                      >> $HARBOR_VALUES
    echo "      size: 50Gi"                                                                                                     >> $HARBOR_VALUES
    echo "    chartmuseum:"                                                                                                     >> $HARBOR_VALUES
    echo "      accessMode: ReadWriteOnce"                                                                                      >> $HARBOR_VALUES
    echo "      size: 5Gi"                                                                                                      >> $HARBOR_VALUES
    echo "    jobservice:"                                                                                                      >> $HARBOR_VALUES
    echo "      accessMode: ReadWriteOnce"                                                                                      >> $HARBOR_VALUES
    echo "      size: 5Gi"                                                                                                      >> $HARBOR_VALUES
    echo "    database:"                                                                                                        >> $HARBOR_VALUES
    echo "      accessMode: ReadWriteOnce"                                                                                      >> $HARBOR_VALUES
    echo "      size: 50Gi"                                                                                                     >> $HARBOR_VALUES
    echo "    redis:"                                                                                                           >> $HARBOR_VALUES
    echo "      accessMode: ReadWriteOnce"                                                                                      >> $HARBOR_VALUES
    echo "      size: 50Gi"                                                                                                     >> $HARBOR_VALUES
    echo "    trivy:"                                                                                                           >> $HARBOR_VALUES
    echo "      accessMode: ReadWriteOnce"                                                                                      >> $HARBOR_VALUES
    echo "      size: 10Gi"                                                                                                     >> $HARBOR_VALUES

    messagePrint " ▪ Add the Bintnami Repository"   "https://charts.bitnami.com/bitnami"
    helmRepoAdd bitnami https://charts.bitnami.com/bitnami
    HELM_HARBOR_VERSION=$(helm search repo bitnami/harbor | tail -1 | awk '{ print $2 }')

    messagePrint " ▪ Install Harbor Helm Chart"           "bitnami/contour"
    messagePrint " ▪ Install Harbor Helm Chart Version"   "$HELM_HARBOR_VERSION"

    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 10 ]; do
      helm install harbor bitnami/harbor -f $HARBOR_VALUES --version $HELM_HARBOR_VERSION -n $NAMESPACE --wait --wait-for-jobs > /dev/null 2>&1; ret=$?
      #helm install harbor bitnami/harbor -f $HARBOR_VALUES --version $HELM_HARBOR_VERSION -n $NAMESPACE --wait --wait-for-jobs --timeout 10m; ret=$?
      [ $ret -eq 0 ] && break
      let cnt=cnt+1
      sleep 30
    done

    if [ $ret -ne 0 ]; then
      echo "ERROR: failed to install bitnami/harbor"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ helm install harbor bitnami/harbor -f $HARBOR_VALUES --version $HELM_HARBOR_VERSION -n $NAMESPACE"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => helm install harbor bitnami/harbor -f $HARBOR_VALUES --version $HELM_HARBOR_VERSION -n $NAMESPACE"
      fi

      exit 1
    fi

    cnt=0
    while [ $cnt -eq 0 ]; do
      cnt=$(kubectl get pods -n registry-harbor | sed 1d | grep -vc Running)
      if [ $cnt -eq 0 ]; then cnt=1; fi
      sleep 5
    done

    # --- ADD HTTPProxy ---
    HARBOR_VALUES=/tmp/harbor_httpproxy.yaml
    echo "apiVersion: projectcontour.io/v1"                                       >  $HARBOR_VALUES
    echo "kind: HTTPProxy"                                                        >> $HARBOR_VALUES
    echo "metadata:"                                                              >> $HARBOR_VALUES
    echo "  name: harbor-httpproxy"                                               >> $HARBOR_VALUES
    echo "  namespace: $NAMESPACE"                                                >> $HARBOR_VALUES
    echo "  annotations:"                                                         >> $HARBOR_VALUES
    echo "    kubernetes.io/ingress.class: contour"                               >> $HARBOR_VALUES
    echo "spec:"                                                                  >> $HARBOR_VALUES
    echo "  virtualhost:"                                                         >> $HARBOR_VALUES
    echo "    fqdn: harbor.apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"      >> $HARBOR_VALUES
    echo "    tls:"                                                               >> $HARBOR_VALUES
    echo "      secretName: $HARBOR_TLS_SECRET"                                   >> $HARBOR_VALUES
    echo "  routes:"                                                              >> $HARBOR_VALUES
    echo "    - conditions: "                                                     >> $HARBOR_VALUES
    echo "      - prefix: /"                                                      >> $HARBOR_VALUES
    echo "      services:"                                                        >> $HARBOR_VALUES
    echo "        - name: harbor-portal"                                          >> $HARBOR_VALUES
    echo "          port: 80"                                                     >> $HARBOR_VALUES
    echo "      timeoutPolicy:"                                                   >> $HARBOR_VALUES
    echo "        response: 600s"                                                 >> $HARBOR_VALUES
    echo "    - conditions:"                                                      >> $HARBOR_VALUES
    echo "      - prefix: /api/"                                                  >> $HARBOR_VALUES
    echo "      services:"                                                        >> $HARBOR_VALUES
    echo "        - name: harbor-core"                                            >> $HARBOR_VALUES
    echo "          port: 80"                                                     >> $HARBOR_VALUES
    echo "      timeoutPolicy:"                                                   >> $HARBOR_VALUES
    echo "        response: 600s"                                                 >> $HARBOR_VALUES
    echo "    - conditions:"                                                      >> $HARBOR_VALUES
    echo "      - prefix: /service/"                                              >> $HARBOR_VALUES
    echo "      services:"                                                        >> $HARBOR_VALUES
    echo "        - name: harbor-core"                                            >> $HARBOR_VALUES
    echo "          port: 80"                                                     >> $HARBOR_VALUES
    echo "      timeoutPolicy:"                                                   >> $HARBOR_VALUES
    echo "        response: 600s"                                                 >> $HARBOR_VALUES
    echo "    - conditions:"                                                      >> $HARBOR_VALUES
    echo "      - prefix: /v2/"                                                   >> $HARBOR_VALUES
    echo "      services:"                                                        >> $HARBOR_VALUES
    echo "        - name: harbor-core"                                            >> $HARBOR_VALUES
    echo "          port: 80"                                                     >> $HARBOR_VALUES
    echo "      timeoutPolicy:"                                                   >> $HARBOR_VALUES
    echo "        response: 600s"                                                 >> $HARBOR_VALUES
    echo "    - conditions:"                                                      >> $HARBOR_VALUES
    echo "      - prefix: /chartrepo/"                                            >> $HARBOR_VALUES
    echo "      services:"                                                        >> $HARBOR_VALUES
    echo "        - name: harbor-core"                                            >> $HARBOR_VALUES
    echo "          port: 80"                                                     >> $HARBOR_VALUES
    echo "      timeoutPolicy:"                                                   >> $HARBOR_VALUES
    echo "        response: 600s"                                                 >> $HARBOR_VALUES
    echo "    - conditions:"                                                      >> $HARBOR_VALUES
    echo "      - prefix: /c/"                                                    >> $HARBOR_VALUES
    echo "      services:"                                                        >> $HARBOR_VALUES
    echo "        - name: harbor-core"                                            >> $HARBOR_VALUES
    echo "          port: 80"                                                     >> $HARBOR_VALUES
    echo "      timeoutPolicy:"                                                   >> $HARBOR_VALUES
    echo "        response: 600s"                                                 >> $HARBOR_VALUES

    messagePrint " ▪ Deploy HTTPProxy for Harbor" "$HARBOR_VALUES"
    kubectl apply -f $HARBOR_VALUES; ret=$?
    if [ $ret -ne 0 ]; then
      echo "ERROR: failed to deploy HTTPProxy"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ kubectl apply -f $HARBOR_VALUES"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => kubectl apply -f $HARBOR_VALUES"
      fi

      exit 1
    fi


    # --- WAIT FOR HARBOR TO COME ONLINE ---
    messagePrint " ▪ Verify Harbor Registry Access" "$HARBOR_HOSTNAME"
    ret=1; cnt=0
    while [ $ret -ne 0 -a $cnt -le 9 ]; do
      curl https://harbor.apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN > /dev/null 2>&1; ret=$?
      [ $ret -eq 0 ] && break
      sleep 60
      let cnt=cnt+1
    done

    ret=1; cnt=0
    while [ $ret -ne 0 -a $cnt -le 9 ]; do
      docker login $HARBOR_HOSTNAME -u admin -p $TDH_HARBOR_ADMIN_PASSWORD > /dev/null 2>&1; ret=$?
      [ $ret -eq 0 ] && break
      sleep 60
      let cnt=cnt+1
    done

    if [ $ret -ne 0 ]; then
      echo "ERROR: failed to login to registry"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ docker login $HARBOR_HOSTNAME -u admin -p $TDH_HARBOR_ADMIN_PASSWORD"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => docker login $HARBOR_HOSTNAME -u admin -p $TDH_HARBOR_ADMIN_PASSWORD"
      fi

      exit 1
    fi
  fi

  ver=$(helm -n $NAMESPACE ls -o json | jq -r '.[].app_version')
  crt=$(helm -n $NAMESPACE ls -o json | jq -r '.[].chart')
  stt=$(helm -n $NAMESPACE ls -o json | jq -r '.[].status')
  dat=$(helm -n $NAMESPACE ls -o json | jq -r '.[].updated')

  messagePrint " ▪ Verify Harbor Registry"             "bitnami/harbor"
  messagePrint "   Harbor Registry Namespace:"         "$NAMESPACE"
  messagePrint "   Harbor Registry Helm Chart:"        "$crt"
  messagePrint "   Harbor Registry Version:"           "$ver"
  messagePrint "   Harbor Registry Status:"            "$stt"
  messagePrint "   Harbor Registry Installed/Updated:" "$dat"

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_ENABLED        "true"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_ADMIN_PASSWORD "$TDH_HARBOR_ADMIN_PASSWORD"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR     "$HARBOR_HOSTNAME"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_NOTARY     "$NOTARY_HOSTNAME"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_CHART_NAME     "$crt"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_CHART_VERSION  "$ver"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_CHART_STATUS   "$stt"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_CHART_UPDATE   "$dat"
}

tdh_verifyTKGcluster() {
  messageTitle "TKG Cluster Information"
  messagePrint " ▪ Cluster Name"           "$TDH_CLUSTER_NAME"
  messagePrint " ▪ TKG Management Cluster" "$TDH_MANAGEMENT_CLUSTER"
  messagePrint " ▪ Provisioner Name"       "$TDH_PROVISIONER_NAME"
  messagePrint " ▪ Account Name"           "$TDH_MISSION_CONTROL_ACCOUNT_NAME"
  echo 

  echo -n "Do you want detailed infos of the TKG cluster setup setup ($TDH_CLUSTER_NAME) ? (y/n): "; read cluster_info
  answer_provided="n"
  while [ "${answer_provided}" == "n" ]; do
    if [ "${cluster_info}" == "y" -o "${cluster_info}" == "Y" ]; then break; fi
    if [ "${cluster_info}" == "n" -o "${cluster_info}" == "N" ]; then break; fi
    echo -n "Do you want detailed infos of the TKG cluster setup setup ($TDH_CLUSTER_NAME) ? (y/n): "; read cluster_info
  done
  echo 

  if [ "${cluster_info}" == "y" -o "${cluster_info}" == "Y" ]; then
    prtHead "Show TDK Cluster infos ($TDH_CLUSTER_NAME)"
    execCmd "tmc cluster list $TDH_CLUSTER_NAME -m $TDH_MANAGEMENT_CLUSTER -p $TDH_PROVISIONER_NAME"
    execCmd "tmc cluster get $TDH_CLUSTER_NAME -m $TDH_MANAGEMENT_CLUSTER -p $TDH_PROVISIONER_NAME"
  fi
}

cleanAWSenv() {
  # --- DELETE HOSTED ZONE ---
  domain="$ENV_NAME.$AWS_HOSTED_DNS_DOMAIN"
  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${domain} | \
            jq -r ".HostedZones[] | select(.Name | scan(\"^$domain.\")).Id")

  if [ "${ZONE_ID}" != "" ]; then
    messagePrint " ▪ Deleting DNS Hosted Zone:" "$ZONE_ID"
    route53deleteHostedZone $ZONE_ID
  fi

  # --- DELETE INSTANCES ---
  SECGRP=/tmp/$$_aws_routing_table.json
  aws --region $AWS_LOCATION ec2 describe-instances > $SECGRP
  tot=$(grep -c "InstanceId" $SECGRP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    gid=$(jq -r ".Reservations[$i].Instances[].InstanceId" 2>/dev/null $SECGRP)

    cnt=$(jq -r ".Reservations[$i].Instances[].Tags[].Value" 2>/dev/null $SECGRP | egrep -c "^${ENV_NAME}|p-bosh")
    if [ $cnt -gt 0 ]; then
      cnt=$(jq -r ".Reservations[$i].Instances[].State.Name" 2>/dev/null $SECGRP | egrep -c "terminated")
      if [ $cnt -eq 0 ]; then
        messagePrint " ▪ Terminate Instance:" "$gid"
        aws --region $AWS_LOCATION ec2 terminate-instances --instance-ids $gid > /dev/null 2>&1
        if [ $? -ne 0 ]; then
          echo "ERROR: failled to Terminage Instance: $gid"
          echo "aws --region $AWS_LOCATION ec2 terminate-instances  --instance-ids $gid"
          exit 1
        fi

        sleep 30
      fi
    fi

    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $SECGRP

  # --- IAM DELETE ROLES ---
  TMPAWS=/tmp/$$_aws.json
  aws --region $AWS_LOCATION iam list-instance-profiles > $TMPAWS
  tot=$(grep -c "InstanceProfileId" $TMPAWS); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    arn=$(jq -r ".InstanceProfiles[$i].InstanceProfileId" 2>/dev/null $TMPAWS)
    nam=$(jq -r ".InstanceProfiles[$i].InstanceProfileName" 2>/dev/null $TMPAWS)

    cnt=$(echo $nam | egrep -c "^${ENV_NAME}_")
    if [ $cnt -gt 0 ]; then
      messagePrint " ▪ Delete IAM Instance Profile:" "$nam"
      for rol in $(jq -r ".InstanceProfiles[$i].Roles[].RoleName" 2>/dev/null $TMPAWS); do
        aws --region $AWS_LOCATION iam remove-role-from-instance-profile \
           --instance-profile-name $nam --role-name $rol
      done

      aws --region $AWS_LOCATION iam delete-instance-profile --instance-profile-name $nam > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to delete IAM Instance Profile: $nam"
        echo "aws --region $AWS_LOCATION iam delete-instance-profile --instance-profile-name $nam"
        exit 1
      fi
    fi
    let i=i+1
  done

  # --- DELETE KMS ALIASES ---
  aws --region eu-central-1 kms list-aliases --query "Aliases[?contains(to_string(AliasName),'$ENV_NAME')]" > $TMPAWS
  tot=$(grep -c "PolicyName" $TMPAWS); i=0
  while [ $i -lt $tot ]; do
    tid=$(jq -r ".[$i].TargetKeyId" 2>/dev/null $TMPAWS)
    arn=$(jq -r ".[$i].AliasArn" 2>/dev/null $TMPAWS)
    nam=$(jq -r ".[$i].AliasName" 2>/dev/null $TMPAWS)

    messagePrint " ▪ Delete KMS Alias:" "$nam"
  done

  # --- IAM DELETE ROLES ---
  TMPAWS=/tmp/$$_aws.json
  aws --region $AWS_LOCATION iam list-roles --query "Roles[?starts_with(to_string(RoleName), '$ENV_NAME')]" > $TMPAWS

  tot=$(grep -c "RoleName" $TMPAWS); i=0
  while [ $i -lt $tot ]; do
    rid=$(jq -r ".[$i].RoleId" 2>/dev/null $TMPAWS)
    arn=$(jq -r ".[$i].Arn" 2>/dev/null $TMPAWS)
    nam=$(jq -r ".[$i].RoleName" 2>/dev/null $TMPAWS)

    messagePrint " ▪ Delete IAM Role:" "$nam"

    for tmp in $(aws --region $AWS_LOCATION  iam list-policies | jq -r '.Policies[].Arn' | grep $ENV_NAME); do
      usr=$(aws --region $AWS_LOCATION iam list-entities-for-policy --policy-arn $tmp | jq -r '.PolicyUsers[].UserName')
      grp=$(aws --region $AWS_LOCATION iam list-entities-for-policy --policy-arn $tmp | jq -r '.PolicyGroups[].GroupName')
      rol=$(aws --region $AWS_LOCATION iam list-entities-for-policy --policy-arn $tmp | jq -r '.PolicyRoles[].RoleName')
      if [ "${usr}" != "" ]; then
        aws --region $AWS_LOCATION iam detach-user-policy --policy-arn $tmp --user-name $usr
      fi

      if [ "${rol}" != "" ]; then
        aws --region $AWS_LOCATION iam detach-role-policy --policy-arn $tmp --role-name $rol
      fi

      if [ "${grp}" != "" ]; then
        aws --region $AWS_LOCATION iam detach-user-policy --policy-arn $tmp --group-name $usr
      fi

      aws iam delete-policy --policy-arn $tmp
    done

    for pol in $(aws --region $AWS_LOCATION iam list-attached-role-policies --role-name $nam | \
        jq -r ".AttachedPolicies[].PolicyArn"); do

      aws --region $AWS_LOCATION iam detach-role-policy --role-name $nam --policy-arn $pol
      aws --region $AWS_LOCATION iam delete-role-policy --role-name $nam --policy-name $pol
    done

    # --- DELETE ROLE POLICY ---
    for pol in $(aws --region $AWS_LOCATION iam list-role-policies --role-name $nam | \
                 jq -r '.PolicyNames[]'); do

      aws --region $AWS_LOCATION iam delete-role-policy --role-name $nam --policy-name $pol
    done

    aws --region $AWS_LOCATION iam delete-role --role-name $nam > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: failled to delete IAM Role: $nam"
      echo "       aws --region $AWS_LOCATION iam delete-role --role-name $nam"
      exit 1
    fi
    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $TMPAWS

  # --- IAM DELETE USERS ---
  for arn in $(aws --region $AWS_LOCATION iam list-users | jq -r '.Users[].UserName' 2>/dev/null); do
    cnt=$(echo $arn | egrep -c "^${ENV_NAME}_")
    if [ $cnt -gt 0 ]; then
      messagePrint " ▪ Cleaning up iam User:" "$arn"

      # --- DETACH POLICY ---
      for pol in $(aws --region $AWS_LOCATION iam list-attached-user-policies --user-name $arn | \
          jq -r ".AttachedPolicies[].PolicyArn"); do

        aws --region $AWS_LOCATION iam detach-user-policy --user-name $arn --policy-arn $pol
      done

      # --- DELETE POLICY ---
      for pol in $(aws --region $AWS_LOCATION iam list-attached-user-policies --user-name $arn | \
          jq -r ".AttachedPolicies[].PolicyName"); do

        aws --region $AWS_LOCATION iam delete-user-policy --user-name $arn --policy-name $pol
      done

      # --- DELETE ACCESS-KEYS ---
      for key in $(aws --region $AWS_LOCATION iam list-access-keys --user-name $arn | \
          jq -r ".AccessKeyMetadata[].AccessKeyId"); do

        aws --region $AWS_LOCATION iam delete-access-key --access-key-id $key --user-name $arn
      done

      aws --region $AWS_LOCATION iam delete-user --user-name $arn
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to delete User"
        echo "aws --region $AWS_LOCATION iam delete-user --user-name $arn"
        exit 1
      fi
    fi
  done

  # --- DELETE LOAD BALANCERS ---
  TMPAWS=/tmp/$$_aws.json
  for nam in $(aws --region $AWS_LOCATION elb describe-load-balancers | jq -r '.LoadBalancerDescriptions[].LoadBalancerName' 2>/dev/null); do
    aws --region $AWS_LOCATION elb describe-load-balancers --load-balancer-names $nam >/dev/null 2>&1
    if [ $? -eq 0 ]; then
      for tag in $(aws --region $AWS_LOCATION elb describe-tags --load-balancer-name $nam 2>/dev/null | \
        jq -r '.TagDescriptions[].Tags[].Value'); do

        # --- DELETE LOAD-BALANCERS WITH TAG ENV_NAME ---
        enm=$(aws --region $AWS_LOCATION elb describe-tags --load-balancer-name $nam | \
            jq -r '.TagDescriptions[].Tags[].Value' | grep -c "${ENV_NAME}")

        # --- CLEANUOP NGINX-INGRESS-LOADBALANCER ---
        cnt=$(echo $tag | egrep -c "nginx-ingress-controller")
        if [ ${cnt} -gt 0 -o ${enm} -gt 0 ]; then
          messagePrint " ▪ Cleaning up elp Load-Balancers:" "$nam"
          aws --region $AWS_LOCATION elb delete-load-balancer --load-balancer-name $nam >/dev/null 2>&1
          if [ $? -ne 0 ]; then
            echo "ERROR: failled to delete load-balancer: $nam"
            echo "aws --region $AWS_LOCATION elb delete-load-balancer --load-balancer-name $nam"
            exit 1
          fi
        fi
      done
    fi
  done

  rm -f $TMPAWS

  # --- DELETE LOAD BALANCERS ---
  for arn in $(aws --region $AWS_LOCATION elbv2 describe-load-balancers | jq -r '.LoadBalancers[].LoadBalancerArn' 2>/dev/null); do
    nam=$(aws --region $AWS_LOCATION elbv2 describe-load-balancers --load-balancer-arns $arn | \
        jq -r '.LoadBalancers[].LoadBalancerName')

    cnt=$(echo $nam | egrep -c "^${ENV_NAME}-")
    if [ $cnt -gt 0 ]; then
      aws --region $AWS_LOCATION elbv2 delete-load-balancer --load-balancer-arn $arn
      messagePrint " ▪ Cleaning up elpv2 LoadBalancers:" "$nam"
    fi
  done

  # --- DELETE TARGET GROUPS ---
  SECGRP=/tmp/$$_aws_tgp.json
  aws --region $AWS_LOCATION elbv2 describe-target-groups > $SECGRP
  tot=$(grep -c "TargetGroupArn" $SECGRP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    sid=$(jq -r ".TargetGroups[$i].TargetGroupArn" 2>/dev/null $SECGRP)
    snm=$(jq -r ".TargetGroups[$i].TargetGroupName" 2>/dev/null $SECGRP)

    cnt=$(echo "$snm" | egrep -c "^${ENV_NAME}-")
    if [ $cnt -gt 0 ]; then
      messagePrint " ▪ Delete Target Group:" "$snm"
      aws --region $AWS_LOCATION elbv2 delete-target-group --target-group-arn $sid > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to delete Target group"
        echo "aws --region $AWS_LOCATION elbv2 delete-target-group --target-group-arn $sid"
        exit 1
      fi
    fi
    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $SECGRP

  for key in $(aws --region $AWS_LOCATION ec2 describe-key-pairs | jq -r '.KeyPairs[].KeyName' 2>/dev/null); do
    cnt=$(echo $key | egrep -c "^${ENV_NAME}-")
    if [ $cnt -gt 0 ]; then
      messagePrint " ▪ Delete KeyPair:" "$key"
      aws --region $AWS_LOCATION ec2 delete-key-pair --key-name $key >/dev/null 2>&1
      if [ $? -ne 0 ]; then
        aws --region $AWS_LOCATION ec2 delete-key-pair --key-name $key
        exit 1
      fi
    fi
  done

  # --- DELETE VPC's ---
  TMPVCP=/tmp/$$_aws_vpc.json
  aws --region $AWS_LOCATION ec2 describe-vpcs > $TMPVCP
  tot=$(grep -c "VpcId" $TMPVCP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    vpc=$(jq -r ".Vpcs[$i].VpcId" 2>/dev/null $TMPVCP)
    cnt=$(jq -r ".Vpcs[$i].Tags[].Value" 2>/dev/null $TMPVCP | egrep -c "^${ENV_NAME}|pcfjump")
    if [ $cnt -eq 0 ]; then let i=i+1; continue; fi
    messagePrint " VPC Ressources:" "$vpc"

    # --- DELETE NAT GATEWAY ---
    for id in $(aws --region $AWS_LOCATION ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$vpc" | \
                jq -r ".NatGateways[].NatGatewayId" 2>/dev/null $TMPVAL); do

      messagePrint "  ▪ 1Delete NAT Gateway:" "$id"
      aws --region $AWS_LOCATION ec2 delete-nat-gateway --nat-gateway-id $id > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to NAT Gateway: $id"
        echo "aws --region $AWS_LOCATION ec2 delete-nat-gateway --nat-gateway-id $id"
        #exit 1
      fi
    done

    sleep 10

    # --- CLEANUP ---
    rm -f $TMPAWS

    # --- DELETE LOAD BALANCERS ---
    for nam in $(aws --region $AWS_LOCATION elb describe-load-balancers | \
         jq -r '.LoadBalancerDescriptions[].LoadBalancerName' 2>/dev/null); do
      tvpc=$(aws --region $AWS_LOCATION elb describe-load-balancers --load-balancer-name $nam | \
          jq -r '.LoadBalancerDescriptions[].VPCId')
      if [ "$tvpc" == "${vpc}" ]; then
        aws --region $AWS_LOCATION elb delete-load-balancer --load-balancer-name $nam
        messagePrint "  ▪ Cleaning up elp LoadBalancers:" "$nam"
        sleep 30
      fi
    done

    # --- DELETE INTERNET GATEWAYS ---
    for id in $(aws --region $AWS_LOCATION ec2 describe-internet-gateways --filter "Name=attachment.vpc-id,Values=$vpc" | \
                jq -r ".InternetGateways[].InternetGatewayId" 2>/dev/null); do

      sleep 10
      messagePrint "  ▪ Detach Internet Gateway:" "$id"
      aws --region $AWS_LOCATION ec2 detach-internet-gateway --internet-gateway-id $id --vpc-id $vpc > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to detach internet gateway: $id"
        echo "aws --region $AWS_LOCATION ec2 detach-internet-gateway --internet-gateway-id $id --vpc-id $vpc"
        exit 1
      fi

      sleep 10
      messagePrint "  ▪ Delete Internet Gateway:" "$id"
      aws --region $AWS_LOCATION ec2 delete-internet-gateway --internet-gateway-id $id > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to delete internet gateway: $id"
        echo "aws --region $AWS_LOCATION ec2 delete-internet-gateway --internet-gateway-id $id"
        exit 1
      fi
    done

    # --- DELETE SUBNETS ---
    for id in $(aws --region $AWS_LOCATION ec2 describe-subnets --filter "Name=vpc-id,Values=$vpc" | \
                jq -r ".Subnets[].SubnetId" 2>/dev/null); do

      messagePrint "  ▪ Delete Subnet:" "$id"
      aws --region $AWS_LOCATION ec2 delete-subnet --subnet-id $id > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to deleting subnet: $id"
        echo "aws --region $AWS_LOCATION ec2 delete-subnet --subnet-id $id"
        exit 1
      fi
    done

    SECGRP=/tmp/$$_aws_security_groups.json
    aws --region $AWS_LOCATION ec2 describe-security-groups --filter "Name=vpc-id,Values=$vpc" > $SECGRP
    tot=$(grep -c "GroupName" $SECGRP); i=0; let tot=tot-1
    while [ $i -le $tot ]; do
      gnm=$(jq -r ".SecurityGroups[$i].GroupName" 2>/dev/null $SECGRP)
      gid=$(jq -r ".SecurityGroups[$i].GroupId" 2>/dev/null $SECGRP)

      cnt=$(jq -r ".SecurityGroups[$i].Tags[].Value" 2>/dev/null $SECGRP | egrep -c "^${ENV_NAME}")
      if [ $cnt -gt 0 ]; then
        messagePrint "  ▪ Delete Security Group:" "$gnm"
        aws --region $AWS_LOCATION ec2 delete-security-group --group-id $gid > /dev/null 2>&1
        if [ $? -ne 0 ]; then
          aws --region $AWS_LOCATION ec2 delete-security-group --group-name $gnm > /dev/null 2>&1
          if [ $? -ne 0 ]; then
            echo "ERROR: failled to delete security group"
            echo "aws --region $AWS_LOCATION ec2 delete-security-group --group-id $gid"
            echo "aws --region $AWS_LOCATION ec2 delete-security-group --group-name $gnm"
            exit 1
          fi
        fi
      fi
      let i=i+1
    done

   # --- DELETE ALL SCURITY GROUPS ATTACHED TO THIS VPC ---
    for gid in $(aws --region $AWS_LOCATION ec2 describe-security-groups | \
         jq -r '.SecurityGroups[].GroupId' 2>/dev/null); do
      sgnm=$(aws --region $AWS_LOCATION ec2 describe-security-groups --group-ids  $gid | \
          jq -r '.SecurityGroups[].GroupName')
      tvpc=$(aws --region $AWS_LOCATION ec2 describe-security-groups --group-ids  $gid | \
          jq -r '.SecurityGroups[].VpcId')
      if [ "$tvpc" == "${vpc}" -a "${sgnm}" != "default" ]; then
        messagePrint "  ▪ Cleaning up SecurityGroup:" "$gid"
        aws --region $AWS_LOCATION ec2 delete-security-group --group-id $gid > /dev/null 2>&1
        if [ $? -ne 0 ]; then
          echo "ERROR: failled to delete security group"
          echo "aws --region $AWS_LOCATION ec2 delete-security-group --group-id $gid"
          exit 1
        fi
      fi

      sleep 30
    done

    # --- CLEANUP ---
    rm -f $SECGRP

    # --- DELETE ROUTING TABLES ---
    for id in $(aws --region $AWS_LOCATION ec2 describe-route-tables --filter "Name=vpc-id,Values=$vpc" | \
                jq -r ".RouteTables[].RouteTableId" 2>/dev/null); do

      messagePrint "  ▪ Delete Routing Table:" "$id"
      aws --region $AWS_LOCATION ec2 delete-route-table --route-table-id $id> /dev/null 2>&1
      #if [ $? -ne 0 ]; then
      #  echo "ERROR: failled to delete routing table: $id"
      #  echo "aws --region $AWS_LOCATION ec2 delete-route-table --route-table-id $id"
      #  #exit 1
      #fi
    done

    # --- DELETE VPC ---
    messagePrint "  ▪ Delete VPC:" "$vpc"
    aws --region $AWS_LOCATION ec2 delete-vpc --vpc-id $vpc > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: failled to delete VPC: $vpc"
      echo "aws --region $AWS_LOCATION ec2 delete-vpc --vpc-id $vpc"
      exit 1
    fi

    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $TMPVCP

  # --- DELETE ADDRESSES ---
  SECGRP=/tmp/$$_aws_routing_table.json
  aws --region $AWS_LOCATION ec2 describe-addresses > $SECGRP
  tot=$(grep -c "PublicIp" $SECGRP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    pid=$(jq -r ".Addresses[$i].PublicIp" 2>/dev/null $SECGRP)
    aid=$(jq -r ".Addresses[$i].AllocationId" 2>/dev/null $SECGRP)

    cnt=$(jq -r ".Addresses[$i].Tags[].Value" 2>/dev/null $SECGRP | egrep -c "^${ENV_NAME}")
    if [ $cnt -gt 0 ]; then
      messagePrint " ▪ Release PublicIP:" "$pid"
      aws --region $AWS_LOCATION ec2 release-address --allocation-id $aid > /dev/null 2>&1; re1=$?
      aws --region $AWS_LOCATION ec2 release-address --public-ip $pid > /dev/null 2>&1; re2=$?
      if [ $re1 -ne 0 -a $re2 -ne 0 ]; then
#=> Delete NatGateway first (awspks-nat) 1660 llllllll
        echo "ERROR: 1failed to release IP: $pid"
        echo "aws --region $AWS_LOCATION ec2 release-address --allocation-id $aid"
        echo "aws --region $AWS_LOCATION ec2 release-address --public-ip $pid"
        exit 1
      fi

      sleep 20
    fi

    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $TMPAWS

  SECGRP=/tmp/$$_aws_routing_table.json
  aws --region $AWS_LOCATION ec2 describe-route-tables > $SECGRP
  tot=$(grep -c "RouteTableId" $SECGRP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    gid=$(jq -r ".RouteTables[$i].RouteTableId" 2>/dev/null $SECGRP)

    cnt=$(jq -r ".RouteTables[$i].Tags[].Value" 2>/dev/null $SECGRP | egrep -c "^${ENV_NAME}")
    if [ $cnt -gt 0 ]; then
      messagePrint " ▪ Delete Routing Table:" "$gid"
      aws --region $AWS_LOCATION ec2 delete-route-table --group-id $gid > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to delete Routing Table: $gid"
        echo "aws --region $AWS_LOCATION ec2 delete-route-table --route-table-id $gid"
        exit 1
      fi
    fi
    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $SECGRP

  SECGRP=/tmp/$$_aws_nat_gateways.json
  aws --region $AWS_LOCATION ec2 describe-nat-gateways > $SECGRP
  tot=$(grep -c "GroupName" $SECGRP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    gid=$(jq -r ".NatGateways[$i].NatGatewayId" 2>/dev/null $SECGRP)

    cnt=$(jq -r ".NatGateways[$i].Tags[].Value" 2>/dev/null $SECGRP | egrep -c "^${ENV_NAME}")
    if [ $cnt -gt 0 ]; then
      messagePrint " ▪ 2Delete NAT Gateway:" "$gid"
      aws --region $AWS_LOCATION ec2 delete-nat-gateway --group-id $gid > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        aws --region $AWS_LOCATION ec2 delete-nat-gateway --nat-gateway-id $gid > /dev/null 2>&1
        if [ $? -ne 0 ]; then
          echo "ERROR: failled to delete NAT Gateway: $gid"
          echo "aws --region $AWS_LOCATION ec2 delete-nat-gateway --group-id $gid"
          exit 1
        fi
      fi
    fi
    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $SECGRP

  SECGRP=/tmp/$$_aws_security_groups.json
  aws --region $AWS_LOCATION ec2 describe-security-groups > $SECGRP
  tot=$(grep -c "GroupName" $SECGRP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    gnm=$(jq -r ".SecurityGroups[$i].GroupName" 2>/dev/null $SECGRP)
    gid=$(jq -r ".SecurityGroups[$i].GroupId" 2>/dev/null $SECGRP)

    cnt=$(jq -r ".SecurityGroups[$i].Tags[].Value" 2>/dev/null $SECGRP | egrep -c "^${ENV_NAME}")
    if [ $cnt -gt 0 ]; then
      messagePrint " ▪ Delete Sevcurity Group:" "$gnm"
      aws --region $AWS_LOCATION ec2 delete-security-group --group-id $gid > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        aws --region $AWS_LOCATION ec2 delete-security-group --group-name $gnm > /dev/null 2>&1
        if [ $? -ne 0 ]; then
          echo "ERROR: failled to delete security group"
          echo "aws --region $AWS_LOCATION ec2 delete-security-group --group-id $gid"
          echo "aws --region $AWS_LOCATION ec2 delete-security-group --group-name $gnm"
          exit 1
        fi
      fi
    fi
    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $SECGRP

  SECGRP=/tmp/$$_aws_subnets.json
  aws --region $AWS_LOCATION ec2 describe-subnets > $SECGRP
  tot=$(grep -c "SubnetId" $SECGRP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    sid=$(jq -r ".Subnets[$i].SubnetId" 2>/dev/null $SECGRP)

    cnt=$(jq -r ".Subnets[$i].Tags[].Value" 2>/dev/null $SECGRP | egrep -c "^${ENV_NAME}")
    if [ $cnt -gt 0 ]; then
      messagePrint " ▪ Delete Subnet:" "$sid"
      aws --region $AWS_LOCATION ec2 delete-subnet --subnet-id $sid > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to delete security group"
        echo "aws --region $AWS_LOCATION ec2 delete-subnet --subnet-id $sid"
        exit 1
      fi
    fi
    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $SECGRP

  # --- DELETE ADDRESSES ---
  SECGRP=/tmp/$$_aws_routing_table.json
  aws --region $AWS_LOCATION ec2 describe-addresses > $SECGRP
  tot=$(grep -c "PublicIp" $SECGRP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    pid=$(jq -r ".Addresses[$i].PublicIp" 2>/dev/null $SECGRP)
    aid=$(jq -r ".Addresses[$i].AllocationId" 2>/dev/null $SECGRP)

    cnt=$(jq -r ".Addresses[$i].Tags[].Value" 2>/dev/null $SECGRP | egrep -c "^${ENV_NAME}")
    if [ $cnt -gt 0 ]; then
      messagePrint " ▪ Release PublicIP:" "$pid"
      aws --region $AWS_LOCATION ec2 release-address --allocation-id $aid > /dev/null 2>&1; re1=$?
      aws --region $AWS_LOCATION ec2 release-address --public-ip $pid > /dev/null 2>&1; re2=$?
      if [ $re1 -ne 0 -a $re2 -ne 0 ]; then
#=> Delete NatGateway first (awspks-nat)
        echo "ERROR: 2failed to release IP: $pid"
        echo "aws --region $AWS_LOCATION ec2 release-address --allocation-id $aid"
        echo "aws --region $AWS_LOCATION ec2 release-address --public-ip $pid"
        exit 1
      fi

      sleep 20
    fi

    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $TMPAWS

  # --- IAM DELETE POLICIES ---
  TMPAWS=/tmp/$$_aws.json
  aws --region $AWS_LOCATION iam list-policies > $TMPAWS
  tot=$(grep -c "PolicyName" $TMPAWS); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    arn=$(jq -r ".Policies[$i].Arn" 2>/dev/null $TMPAWS)
    nam=$(jq -r ".Policies[$i].PolicyName" 2>/dev/null $TMPAWS)

    cnt=$(echo $nam | egrep -c "^${ENV_NAME}_")
    if [ $cnt -gt 0 ]; then
      messagePrint " ▪ Delete IAM Policy:" "$nam"
      rol=$(aws iam list-entities-for-policy --policy-arn $arn | jq -r '.PolicyRoles[].RoleName')
      if [ "$rol" != "" ]; then
        aws --region $AWS_LOCATION iam detach-role-policy --role-name $rol --policy-arn $arn > /dev/null 2>&1
      fi

      aws --region $AWS_LOCATION iam delete-policy --policy-arn $arn > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to delete IAM Policy: $nam"
        echo "aws --region $AWS_LOCATION iam delete-policy --policy-arn $arn"
        exit 1
      fi
    fi
    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $SECGRP

  for alias in $(aws --region $AWS_LOCATION kms list-aliases | jq -r '.Aliases[].AliasName'); do
    cnt=$(echo $alias | egrep -c "alias/${ENV_NAME}")
    if [ $cnt -gt 0 ]; then
      messagePrint " ▪ Delete KMS Alias:" "$alias"
      aws --region $AWS_LOCATION kms delete-alias --alias-name $alias
    fi
  done

  # --- DELETE S3 BUCKETS ---
  for bucket in $(aws s3api --region=$AWS_LOCATION list-buckets | jq -r '.Buckets[].Name' | \
                egrep "^${ENV_NAME}-"); do
    messagePrint " ▪ Delete S3 Bucket:" "$bucket"
    aws s3api --region=$AWS_LOCATION delete-bucket --bucket $bucket > /dev/null 2>&1
    #if [ $? -ne 0 ]; then
    #  echo "ERROR: failled to delete S3 Bucket: $nam"
    #  echo "aws s3api --region $AWS_LOCATION delete-bucket --bucket $bucket"
    #  exit 1
    #fi
  done

  # --- DELETE EBS VOLUMES
  TMPAWS=/tmp/$$_vol_aws.json
  aws --region $AWS_LOCATION ec2 describe-volumes > $TMPAWS
  i=0; tot=$(jq -r ".Volumes[$i].VolumeId" 2>/dev/null $TMPAWS | egrep -c "^vol-")

  while [ $i -lt $tot ]; do
    stt=$(jq -r ".Volumes[$i].State" 2>/dev/null $TMPAWS)
    vid=$(jq -r ".Volumes[$i].VolumeId" 2>/dev/null $TMPAWS)
    cnt=$(jq -r ".Volumes[$i].Tags[].Value" $TMPAWS 2>/dev/null| egrep -c "p-bosh|bosh-init")

    if [ $cnt -gt 0 ]; then
      if [ "$stt" == "available" ]; then
        messagePrint " ▪ Delete Volume:" "$vid"
        aws --region $AWS_LOCATION ec2 delete-volume --volume-id $vid > /dev/null 2>&1
        if [ $? -ne 0 ]; then
          echo "ERROR: failled to delete volume: $vid"
          echo "aws --region $AWS_LOCATION ec2 delete-volume --volume-id $vid"
          exit 1
        fi
      else
        messagePrint " ▪ Detaching Volume:" "$vid"
        #aws ec2 --region $AWS_LOCATION detach-volume --volume-id $vid > /dev/null 2>&1
        #aws ec2 --region $AWS_LOCATION ec2 delete-volume --volume-id $vid > /dev/null 2>&1
      fi
    fi

    let i=i+1
  done
  rm -f $SECGRP
}

cleanKubeconfig() {
  if [ "$1" == "" ]; then 
    CONFIG="$HOME/.kube/config"
  else
    CONFIG="$1"
  fi

  messageTitle "Cleaning Up kubectl config ($CONFIG)"
  for cluster in $(kubectl --kubeconfig=$CONFIG config get-clusters | sed '1d' | egrep "^tdh|^tkgmc|^tcemc^tdh-tce|^tcemc"); do
    ctx=$(kubectl --kubeconfig=$CONFIG config view -o json | jq -r --arg key $cluster '.contexts[] | select(.context.cluster == $key).name' 2>/dev/null)
    usr=$(kubectl --kubeconfig=$CONFIG config view -o json | jq -r --arg key $cluster '.contexts[] | select(.context.cluster == $key).context.user' 2>/dev/null)
    cnt=$(echo $cluster | grep -c minikube)
    if [ $cnt -gt 0 ]; then 
      if [ ! -d ~/.minikube/profiles/${cluster} ]; then
        messagePrint " ▪ Cleaning up k8s cluster ($cluster)" "Context: $ctx, User: $usr"
        kubectl --kubeconfig=$CONFIG config unset clusters.${cluster} > /dev/null 2>&1
        kubectl --kubeconfig=$CONFIG config unset contexts.${cluster} > /dev/null 2>&1
        kubectl --kubeconfig=$CONFIG config unset users.${cluster} > /dev/null 2>&1
      fi
    else
      if [ "$ctx" != "" ]; then
        kubectl config use-context $ctx > /dev/null 2>&1
        kubectl config use-context $ctx > /dev/null 2>&1
        kubectl get ns > /dev/null 2>&1
        if [ $? -gt 0 ]; then
          messagePrint " ▪ Cleaning up k8s cluster ($cluster)" "Context: $ctx, User: $usr"

          kubectl --kubeconfig=$CONFIG config unset clusters.${cluster} > /dev/null 2>&1
          kubectl --kubeconfig=$CONFIG config unset contexts.${cluster} > /dev/null 2>&1
          kubectl --kubeconfig=$CONFIG config unset users.${cluster} > /dev/null 2>&1
        fi
      else
        messagePrint " ▪ Cleaning up k8s cluster ($cluster)" "Context: $ctx, User: $usr"
        kubectl --kubeconfig=$CONFIG config unset clusters.${cluster} > /dev/null 2>&1
        kubectl --kubeconfig=$CONFIG config unset contexts.${cluster} > /dev/null 2>&1
        kubectl --kubeconfig=$CONFIG config unset users.${cluster} > /dev/null 2>&1
      fi
    fi
  done

  # --- CLEANUP CONTEXTS WITHOUT CLUSTERS ---
  for ctx in $(kubectl --kubeconfig=$CONFIG config view -o json | jq -r '.contexts[].name' 2>/dev/null); do
    cls=$(kubectl --kubeconfig=$CONFIG config view -o json | jq -r --arg key $ctx '.contexts[] | select(.name == $key).context.cluster' 2>/dev/null)
    usr=$(kubectl --kubeconfig=$CONFIG config view -o json | jq -r --arg key $ctx '.contexts[] | select(.name == $key).context.user' 2>/dev/null)

    ret=$(kubectl --kubeconfig=$CONFIG config view -o json | jq -r --arg key $cls '.clusters[] | select(.name == $key).name' 2>/dev/null)
    if [ "$ret" == "" ]; then 
      messagePrint " ▪ Cleaning up unused k8s Context" "$ctx, User: $usr"
      kubectl --kubeconfig=$CONFIG config unset contexts.${ctx} > /dev/null 2>&1
      kubectl --kubeconfig=$CONFIG config unset users.${usr} > /dev/null 2>&1
    fi

  done
}

deleteManagementCluster() {
  MGMT_CLUSTER=$1

  #tmc managementcluster delete $MGMT_CLUSTER -f > /dev/null 2>&1
  tmc managementcluster delete $MGMT_CLUSTER -f 

  cnt=1; loop=0
  while [ $cnt -ne 0 -a $loop -lt 10 ]; do
    cnt=$(tmc managementcluster list | grep -c $MGMT_CLUSTER)

    let loop=loop+1
    sleep 30
  done

  cnt=$(tanzu management-cluster get 2>/dev/null | grep -c " $MGMT_CLUSTER ")
  if [ $cnt -gt 0 ]; then

    ret=1; cnt=0
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      tanzu management-cluster delete $MGMT_CLUSTER -y --force > /tmp/error.log 2>&1; ret=$?
      [ $ret -eq 0 ] && break

      sleep 300
      let cnt=cnt+1
    done

    if [ $ret -ne 0 ]; then
      logMessages /tmp/error.log
      echo "ERROR: failed to delete TKG Management Cluster: ${MGMT_CLUSTER}"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ tmc managementcluster delete $MGMT_CLUSTER -f --force -t 30m"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => tmc managementcluster delete $MGMT_CLUSTER -f --force -t 30m"
      fi
    fi
  fi
}

cleanManagementCluster() {
  messageTitle "Cleaning up old kubeconfig definitions"

  for n in $(tanzu config server list | grep " tkgmc" | awk '{ print $1 }'); do
    cfg=$(tanzu config server list -o json | jq -r --arg cluster "$n" '.[] | select(.name == $cluster).path')
    ctx=$(tanzu config server list -o json | jq -r --arg cluster "$n" '.[] | select(.name == $cluster).context')

    if [ "$cfg" != "" -a -f "$cfg" ]; then 
      kubectl cluster-info --kubeconfig="$cfg" > /dev/null 2>&1; ret=$?
      if [ $ret -ne 0 ]; then 
        messagePrint " ▪ Cleaning up config of Cluster" "$n"
        tanzu config server delete $n -y
        messagePrint " ▪ Remofind Kubeconfig" "$cfg"
        rm -f $cfg
        continue
      fi
    fi

    cnt=$(kubectl config get-clusters | grep -c "$n")
    if [ $cnt -eq 0 ]; then 
      messagePrint " ▪ Cleaning up config of Cluster" "$n"
      tanzu config server delete $n -y
    fi
  done
}

installSnap() {
  PKG=$1
  OPT=$2

  snap refresh > /dev/null 2>&1
  snap list $PKG > /dev/null 2>&1
  if [ $? -ne 0 ]; then
    messagePrint " ▪ Install Package ($PKG)" "installing"
    cnt=0
    snap install $PKG $OPT > /dev/null 2>&1; ret=$?
    while [ $ret -ne 0 -a $cnt -lt 3 ]; do
      snap install $PKG $OPT> /dev/null 2>&1; ret=$?
      sleep 30
      let cnt=cnt+1
    done

    if [ $ret -ne 0 ]; then
      echo "ERROR: failed to install package $PKG"
      echo "       => snap install $PKG"
      exit 1
    fi
  else
    ver=$(snap info $PKG | grep "installed:" | awk '{ print $2 }')
    messagePrint " ▪ Install Package ($PKG)" "$ver"
  fi
}

installPackage() {
  PKG=$1

  dpkg -s $PKG > /dev/null 2>&1
  if [ $? -ne 0 ]; then
    messagePrint " ▪ Install Package ($PKG)" "installing"

    ret=1; cnt=0
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      apt install $PKG -y > /dev/null 2>&1; ret=$?
      [ $ret -eq 0 ] && break
      let cnt=cnt+1
      sleep 30
    done

    if [ $ret -ne 0 ]; then
      echo "ERROR: failed to install package $PKG"
      echo "       => apt install $PKG -y"
      exit 1
    fi
  else
    ver=$(dpkg -s $PKG | grep "Version" | grep -v "Config-Version" | sed 's/^Version: //g') 
    messagePrint " ▪ Verify Package ($PKG)" "$ver"
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: messageCount
# Function Category ..: messageCount 
# Function Purpose ...: Print message text as counter on the same line
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Test Message Text (ie. "Deploy a Database") ............... (string)
# Argument ($2) ......: Total count of Tests (ie. 20) ................................ (int)
# Return Value .......: return 0 ..................................................... (int)
# ------------------------------------------------------------------------------------------
messageCount_old() {
  _msg="$1"
  _stt="$2"
  _cnt=$(echo "${_msg}" | wc -c | sed 's/ //g')
  _dat=$(date "+%Y%m%d-%H:%M:%S")

  ttt=$(echo "$_msg" | egrep -c "^   ")
  if [ $ttt -eq 1 ]; then MAX=62; else MAX=64; fi

  _str=""
  while [ $_cnt -lt $MAX ]; do
    _str="${_str}."
    let _cnt=_cnt+1
  done

  # --- INIZIALIZE VALUES IF EMPTY --- ---
  [ "$DEBUG" == "" ] && DEBUG=0

  [ $(uname) == "Linux" ] && HST="jump-host" || HST="localhost"

  if [ "${PCF_DEPLOYMENT_DEBUG}" == "true" -o "${DEBUG}" == "1" ]; then
    if [ "$HST" == "localhost" ]; then
      printf "\e[37m[${_dat}-$HST] ${_msg} ${_str}: ${_stt}\e[0m\n"
      #echo -e "[${_dat}-$HST] ${_msg} ${_str}: ${_stt}"
    else
      echo -e "\e[33m[${_dat}-$HST]\e[0m \e[37m${_msg} ${_str}: ${_stt}\e[0m"
    fi
    #echo "[${_dat}-$HST] ${_msg} ${_str}: ${_stt}"
  else
    printf "\e[37m${_msg} ${_str}: ${_stt}\e[0m\n"
    #echo "${_msg} ${_str}: ${_stt}"
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: messageCountFine
# Function Category ..: messageCount - Tanzu Demo Hub automated testing facility for Demos
# Function Purpose ...: Finalize Counting and print the test Name
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Test Message Text (ie. "Deploy a Database") ............... (string)
# Argument ($2) ......: Total count of Tests (ie. 20) ................................ (int)
# Return Value .......: return 0 ..................................................... (int)
# ------------------------------------------------------------------------------------------
messageCountFine() {
  printf "\b\b\b\b\b\b\b\e[37m$SELF_TEST_STATUS\e[0m\n"
}

# ------------------------------------------------------------------------------------------
# Function Name ......: messageCountInit
# Function Category ..: messageCount - Tanzu Demo Hub automated testing facility for Demos
# Function Purpose ...: Initialize SelfTest Counting and print the test Name
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Test Message Text (ie. "Deploy a Database") ............... (string)
# Argument ($2) ......: Total count of Tests (ie. 20) ................................ (int)
# Return Value .......: return 0 ..................................................... (int)
# ------------------------------------------------------------------------------------------
messageCountInit() {
  _stp=1
  _tot=$2
  _cnt=$3
  _msg="$1"
  _cnt=$(echo "${_msg}" | wc -c | sed 's/ //g')
  _dat=$(date "+%Y%m%d-%H:%M:%S")
  _num="       "
  first=1

  SELF_TEST_STATUS="completed"
  MAX=64
  [ "$first" == "" ] && first=1

  if [ $first -eq 0 ]; then
    #printf "\b\b\b\b\b\b\bcompleted\n"
    #printf "\b\b\b\b\b\b\b\e[37mcompleted\e[0m\n"
    printf "\b\b\b\b\b\e[37mcompleted1\e[0m\n"
  else
    first=0
  fi

  _str=""
  while [ $_cnt -lt $MAX ]; do
    _str="${_str}."
    let _cnt=_cnt+1
  done

  [ $(uname) == "Linux" ] && HST="jump-host" || HST="localhost"

  if [ "${PCF_DEPLOYMENT_DEBUG}" == "true" -o "${DEBUG}" == "1" ]; then
    printf "\e[37m[${_dat}-$HST] - ${_msg} ${_str}: $_num\e[0m"
  else
    printf "\e[37m${_msg} ${_str}: [%02d/%02d]\e[0m" $_cnt $_tot
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: messageCount
# Function Category ..: selfTest - Tanzu Demo Hub automated testing facility for Demos
# Function Purpose ...: Execute test and print result (sucessed / failed)
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Command to execute (ie. kubectl apply -f /tmp/<file>") .... (string)
# Return Value .......: abort if ABORT_ON_FAILURE=1 otherwise only report the failed test
# ------------------------------------------------------------------------------------------
messageCount() {
  num=$1
  [ "$NATIVE" == "" ] && NATIVE=0
  _msg="$SELF_TEST_NAME"
  _cnt=$(echo "${_msg}" | wc -c | sed 's/ //g')
  _dat=$(date "+%Y%m%d-%H:%M:%S")

  _str=""
  while [ $_cnt -lt 100 ]; do
    _str="${_str}."
    let _cnt=_cnt+1
  done

  _num=$(printf "[%02d/%02d]\n" $num $_tot)

  printf "\b\b\b\b\b\b\b\e[37m${_num}\e[0m"
  #printf "\b\b\b\b\e[37m${_num}\e[0m"

  let _stp=_stp+1

}



# ------------------------------------------------------------------------------------------
# Function Name ......: selfTestInit   
# Function Category ..: selfTest - Tanzu Demo Hub automated testing facility for Demos
# Function Purpose ...: Initialize SelfTest Counting and print the test Name
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Test Message Text (ie. "Deploy a Database") ............... (string)
# Argument ($2) ......: Total count of Tests (ie. 20) ................................ (int)
# Return Value .......: return 0 ..................................................... (int)
# ------------------------------------------------------------------------------------------
selfTestInit() {
  _stp=1
  _tot=$2
  _msg="$1"
  _cnt=$(echo "${_msg}" | wc -c | sed 's/ //g')
  _dat=$(date "+%Y%m%d-%H:%M:%S")
  _num="       "
  
  SELF_TEST_STATUS="completed"

  if [ $first -eq 0 ]; then
    #printf "\b\b\b\b\b\b\bcompleted\n"
    #printf "\b\b\b\b\b\b\b\e[37mcompleted\e[0m\n"
    printf "\b\b\b\b\b\e[37mcompleted1\e[0m\n"
  else
    first=0
  fi

  _str=""
  while [ $_cnt -lt 93 ]; do
    _str="${_str}."
    let _cnt=_cnt+1
  done

  [ $(uname) == "Linux" ] && HST="jump-host" || HST="localhost"

  if [ "${PCF_DEPLOYMENT_DEBUG}" == "true" -o "${DEBUG}" == "1" ]; then
    printf "\e[37m[${_dat}-$HST] - ${_msg} ${_str}: $_num\e[0m"
  else
    printf " \e[37m- ${_msg} ${_str}: $_num\e[0m"
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: selfTestStep
# Function Category ..: selfTest - Tanzu Demo Hub automated testing facility for Demos
# Function Purpose ...: Execute test and print result (sucessed / failed)
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Command to execute (ie. kubectl apply -f /tmp/<file>") .... (string)
# Return Value .......: abort if ABORT_ON_FAILURE=1 otherwise only report the failed test
# ------------------------------------------------------------------------------------------
selfTestStep() {
  CMD="$1"
  [ "$NATIVE" == "" ] && NATIVE=0
  _msg="$SELF_TEST_NAME"
  _cnt=$(echo "${_msg}" | wc -c | sed 's/ //g')
  _dat=$(date "+%Y%m%d-%H:%M:%S")

  _str=""
  while [ $_cnt -lt 78 ]; do
    _str="${_str}."
    let _cnt=_cnt+1
  done

  #[ $(uname) == "Linux" ] &&

  if [ "$NATIVE" -eq 0 ]; then
    echo "$CMD" > /tmp/exec && chmod 777 /tmp/exec
    tools/${TDH_TOOLS}.sh --silent --cmd "bash /tmp/exec" >> /tmp/selffTest.log 2>&1; ret=$?
  else
    eval $CMD >> /tmp/selffTest.log 2>&1; ret=$?
  fi

  _num=$(printf "[%02d/%02d]\n" $_stp $_tot)

  if [ $ret -eq 0 ]; then
    printf "\b\b\b\b\b\b\b\e[37m${_num}\e[0m"

    let _stp=_stp+1
  else
    if [ "$TDHDEMO_ABORT_ON_FAILURE" -eq 0 ]; then
      SELF_TEST_STATUS="failed"
      #printf "\b\b\b\b\b\b\bfailed\n"
    else
      printf "\n\n"
      echo "=> $CMD"
      echo "------------------------------------------------------------------------------"
      cat /tmp/selffTest.log
      echo "------------------------------------------------------------------------------"
      exit
    fi
  fi

}

selfTestFine() {
  printf "\b\b\b\b\b\b\b\e[37m$SELF_TEST_STATUS\e[0m\n"
}

verify_docker() {
  docker ps > /dev/null 2>&1
  if [ $? -ne 0 ]; then 
    echo "ERROR: docker is not running, start docker desktop"
    exit 1
  fi
}

tdh_tools_build() {
  MODE=$1
  TOOLS_VERSION=$2
  REBUILD_REQUIRED=0
  REBUILD_REQUIRED_SOFTWARE=0
  verify_docker
  checkTDHenvironment

  # 05-03-2022 sdubois no, we always do amd64, as arm has some libraries missing
  #[ "$(uname -p)" == "arm" ] && ARCH=arm64 || ARCH=amd64
  ARCH=amd64

  if [ "$MODE" == "tce" ]; then 
    BUILD_MODE=tce
    MACHINE=$(uname -m)
    TDH_TOOLS_TAG=$(echo $TOOLS_VERSION | sed 's/^v//g') 
    TDH_TOOLS_IMAGE=tdh-tools-${BUILD_MODE}-$TDH_TOOLS_TAG
    TDH_TOOLS_DOCKERFILE=Dockerfile-${BUILD_MODE}-$TDH_TOOLS_TAG
    [ -f $TDHPATH/files/tdh-tools/tdh-tools-${BUILD_MODE}-${TDH_TOOLS_TAG}.cfg ] && . $TDHPATH/files/tdh-tools/tdh-tools-${BUILD_MODE}-${TDH_TOOLS_TAG}.cfg

    TCE_DOWNLOAD_URL=https://github.com/vmware-tanzu/community-edition/releases/download/${TKG_COMNUNITY_EDITION_VERSION}
    TCE_LINUX_PACKAGE=tce-linux-amd64-${TKG_COMNUNITY_EDITION_VERSION}
    TCE_DARWIN_PACKAGE=tce-darwin-amd64-${TKG_COMNUNITY_EDITION_VERSION}
    TDH_DOCKER_FILES=/tmp/tdh-tools-tce-$TDH_TOOLS_TAG
    TDH_DOCKER_CACHE=$HOME/.tanzu-demo-hub/cache
    [ ! -d $TDH_DOCKER_CACHE ] && mkdir -p $TDH_DOCKER_CACHE
  fi

  if [ "$MODE" == "tkg" ]; then 
    BUILD_MODE=tkg
    MACHINE=$(uname -m)
    TDH_TOOLS_TAG=$TOOLS_VERSION
    TDH_TOOLS_IMAGE=tdh-tools-${BUILD_MODE}-$TDH_TOOLS_TAG
    TDH_TOOLS_DOCKERFILE=Dockerfile-${BUILD_MODE}-$TDH_TOOLS_TAG
    [ -f $TDHPATH/files/tdh-tools/tdh-tools-${BUILD_MODE}-${TDH_TOOLS_TAG}.cfg ] && . $TDHPATH/files/tdh-tools/tdh-tools-${BUILD_MODE}-${TDH_TOOLS_TAG}.cfg
    TDH_DOCKER_FILES=/tmp/tdh-tools-${BUILD_MODE}-$TDH_TOOLS_TAG
    TDH_DOCKER_CACHE=$HOME/.tanzu-demo-hub/cache
    [ ! -d $TDH_DOCKER_CACHE ] && mkdir -p $TDH_DOCKER_CACHE
  fi

  mkdir -p $TDH_DOCKER_FILES
  cp $TDHPATH/files/tdh-tools/$TDH_TOOLS_DOCKERFILE       $TDH_DOCKER_FILES/Dockerfile
  cp $TDHPATH/files/tdh-tools/wrapdocker                  $TDH_DOCKER_FILES
  cp $TDHPATH/files/tdh-tools/jenkins-cli.jar             $TDH_DOCKER_FILES
  cp $TDHPATH/files/tdh-tools/vmware-ovftool.tar          $TDH_DOCKER_FILES
  DOCKER_CHKSUM=$(sum $TDHPATH/files/tdh-tools/$TDH_TOOLS_DOCKERFILE | awk '{ print $1 }')
  DOCKER_IMAGE_LATEST_ID=$(docker images | egrep "^$TDH_TOOLS_IMAGE " | grep "latest" | awk '{ print $3 }')
  DOCKER_IMAGE_ID=$(docker images | egrep "^$TDH_TOOLS_IMAGE " | grep "CHKSUM" | grep "$DOCKER_IMAGE_LATEST_ID" | awk '{ print $3 }')
  DOCKER_IMAGE_CHKSUM=$(docker images | egrep "^$TDH_TOOLS_IMAGE " | grep "CHKSUM" | grep "$DOCKER_IMAGE_LATEST_ID" | awk '{ print $2 }' | sed 's/CHKSUM-//g')

  # --- SET DEFAULT VALUES ---
  [ "$TDH_SERVICE_BUILD_SERVICE_VERSION" == "" ] && TDH_SERVICE_BUILD_SERVICE_VERSION=1.2.1
  [ "$TDH_SERVICE_BUILD_SERVICE_KP" == "" ] && TDH_SERVICE_BUILD_SERVICE_KP=0.3.0
  [ "$TDH_SERVICE_BUILD_SERVICE_DEPENDANCIES" == "" ] && TDH_SERVICE_BUILD_SERVICE_DEPENDANCIES=0.3.0
  [ "$TKG_TANZU_KUBERNETES_GRID" == "" ] && TKG_TANZU_KUBERNETES_GRID=1.4.0

  messageTitle "TDH Tools Configureation ($TDHPATH/files/tdh-tools/${TDH_TOOLS}.cfg)"
  messagePrint " ▪ Tanzu Mission Control CLI"                                  "$TMC_CLI"
  messagePrint " ▪ Build Service Version"                                      "$TDH_SERVICE_BUILD_SERVICE_VERSION"
  messagePrint " ▪ Build Service Tools (kp)"                                   "$TDH_SERVICE_BUILD_SERVICE_KP"
  #messagePrint " ▪ Build Service Dependancies"                                 "$TDH_SERVICE_BUILD_SERVICE_DEPENDANCIES"
  if [ "$MODE" == "tkg" ]; then
    messagePrint " ▪ Tantzu CLI Bundle"                                        "$TKG_TANZU_KUBERNETES_GRID"
  else
    messagePrint " ▪ Tantzu Comnunity Edition CLI Bundle"                      "$(echo $TKG_COMNUNITY_EDITION_VERSION | sed 's/^v//g')"
  fi
  messagePrint " ▪ Carvel Tools: - Handle multiple k8s Ressources (kapp)"      "$TDH_CARVEL_KAPP"
  messagePrint " ▪ Carvel Tools: - Build/Reference container images (kbld)"    "$TDH_CARVEL_KBLD"
  messagePrint " ▪ Carvel Tools: - Template Overlay for k8s config (ytt)"      "$TDH_CARVEL_YTT"
  messagePrint " ▪ Carvel Tools: - Bundle and Relocate App Config (imgpkg)"    "$TDH_CARVEL_IMGPKG"
  messagePrint " ▪ Hashicorp Terraform"                                        "$TDH_TERRAFORM"
  messagePrint " ▪ Local Kubernetes clusters (Kind)"                           "$TDH_KIND"
  messagePrint " ▪ Kubernetes Cluster API (clusterctl)"                        "$TDH_CLUSTERCTL"
  messagePrint " ▪ vSphere CLI (GOVC)"                                         "$TDH_GOVC"
  messagePrint " ▪ Microsft Azure CLI (az)"                                    "latest"
  messagePrint " ▪ Amazon AWS CLI (aws)"                                       "latest"
  messageTitle "TDH Tools Docker Container ($TDH_TOOLS_IMAGE)"

  #################################################################################
  ################# DOWNLOAD TANZU-CLI-BUNDLE FROM MYVMWARE.COM ###################
  #################################################################################
  VMWCLI=/tmp/vmw-cli
  if [ ! -s $VMWCLI ]; then
    messagePrint " ▪ Install Package (vmw-cli)" "installing"
    docker run apnex/vmw-cli shell > $VMWCLI 2>/dev/null
    if [ $? -ne 0 ]; then
      echo "ERROR: faileed to run vmw-cli docker container"
      echo "       => docker run apnex/vmw-cli shell > /tmp/vmw-cli"
      exit 1
    fi

    chmod 755 $VMWCLI
  fi

  #export VMWUSER="$TDH_MYVMWARE_USER"
  #export VMWPASS="$TDH_MYVMWARE_PASS"
  #$VMWCLI ls vmware_tanzu_kubernetes_grid > /dev/null 2>&1

  if [ "$TDH_SERVICE_TAP_VERSION" != "" ]; then 
    [ ! -d $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION ] && mkdir -p $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION

    # --- DOWNLOAD TANZU FRAMEWORK ---
    TAC_PRODUCT_SLUG="tanzu-application-platform"
    TAC_PRODUCT_FILE="tanzu-framework-linux-amd64"
    TAC_PRODUCT_VERS="$TDH_TAP_TANZU_FRAMEWORK"
    TAC_PRODUCT_PNID="$TDH_TAP_TANZU_FRAMEWORK_PIVNET_ID"
    if [ ! -e $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION/${TAC_PRODUCT_FILE}-${TAC_PRODUCT_VERS}.tar ]; then
      pid=$(pivnetAPI $PCF_PIVNET_TOKEN GET products/$TAC_PRODUCT_SLUG/releases 2> /dev/null | \
               jq -r --arg key "$TDH_SERVICE_TAP_VERSION" '.releases[] | select(.version == $key).id')

      pivnetAPIdownload $PCF_PIVNET_TOKEN $TAC_PRODUCT_SLUG $pid $TAC_PRODUCT_PNID ${TAC_PRODUCT_FILE}.tar > /tmp/error.log 2>&1
      if [ ! -e /tmp/${TAC_PRODUCT_FILE}.tar ]; then 
        logMessages /tmp/error.log
        echo "ERROR: failed to download file ${TAC_PRODUCT_FILE}.tar"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ pivnetAPIdownload $PCF_PIVNET_TOKEN $TAC_PRODUCT_SLUG $pid $TDH_TANZU_FRAMEWORK_PIVNET_ID ${TAC_PRODUCT_FILE}.tar"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => pivnetAPIdownload $PCF_PIVNET_TOKEN $TAC_PRODUCT_SLUG $pid $TDH_TANZU_FRAMEWORK_PIVNET_ID ${PRODUCT_FILE}.tar"
        fi

        exit 1
      else
        mv /tmp/${TAC_PRODUCT_FILE}.tar $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION/${TAC_PRODUCT_FILE}-${TAC_PRODUCT_VERS}.tar
        rm -f $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION/${TAC_PRODUCT_FILE}.tar
        ln -s ${TAC_PRODUCT_FILE}-${TAC_PRODUCT_VERS}.tar $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION/${TAC_PRODUCT_FILE}.tar
      fi
    fi

    (cd $TDH_DOCKER_FILES && tar xf $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION/${TAC_PRODUCT_FILE}.tar) 
    rm -rf $TDH_DOCKER_FILES/clitap
    mv $TDH_DOCKER_FILES/cli $TDH_DOCKER_FILES/clitap
    find $TDH_DOCKER_FILES/clitap -type d -exec chmod 777 {} \; 2> /dev/null
    chmod -R a+r $$TDH_DOCKER_FILES/clitap 2>/dev/null

    # --- DOWNLOAD TANZU FRAMEWORK ---
    TAC_PRODUCT_SLUG="tanzu-application-platform"
    TAC_PRODUCT_FILE="tap-gui-blank-catalog"
    TAC_PRODUCT_VERS="$TDH_TAP_GUI_BLANK_CATALOG"
    TAC_PRODUCT_PNID="$TDH_TAP_GUI_BLANK_CATALOG_PIVNET_ID"
    if [ ! -e $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION/${TAC_PRODUCT_FILE}-${TAC_PRODUCT_VERS}.tgz ]; then
      pid=$(pivnetAPI $PCF_PIVNET_TOKEN GET products/$TAC_PRODUCT_SLUG/releases 2> /dev/null | \
               jq -r --arg key "$TDH_SERVICE_TAP_VERSION" '.releases[] | select(.version == $key).id')

      pivnetAPIdownload $PCF_PIVNET_TOKEN $TAC_PRODUCT_SLUG $pid $TAC_PRODUCT_PNID ${TAC_PRODUCT_FILE}.tgz > /tmp/error.log 2>&1
      if [ ! -s /tmp/${TAC_PRODUCT_FILE}.tgz ]; then
        logMessages /tmp/error.log
        echo "ERROR: failed to download file ${TAC_PRODUCT_FILE}.tgz"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ pivnetAPIdownload $PCF_PIVNET_TOKEN $TAC_PRODUCT_SLUG $pid $TDH_TANZU_FRAMEWORK_PIVNET_ID ${TAC_PRODUCT_FILE}.tgz"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => pivnetAPIdownload $PCF_PIVNET_TOKEN $TAC_PRODUCT_SLUG $pid $TDH_TANZU_FRAMEWORK_PIVNET_ID ${PRODUCT_FILE}.tgz"
        fi

        exit 1
      else
        mv /tmp/${TAC_PRODUCT_FILE}.tgz $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION/${TAC_PRODUCT_FILE}-${TAC_PRODUCT_VERS}.tgz
        rm -f $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION/${TAC_PRODUCT_FILE}.tgz
        ln -s ${TAC_PRODUCT_FILE}-${TAC_PRODUCT_VERS}.tgz $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION/${TAC_PRODUCT_FILE}.tgz
      fi
    fi

    (cd $TDH_DOCKER_FILES && tar xf $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION/${TAC_PRODUCT_FILE}.tgz)
    find $TDH_DOCKER_FILES/blank -type d -exec chmod 777 {} \; 2>/dev/null
    chmod -R a+r $$TDH_DOCKER_FILES/blank 2>/dev/null

    # --- DOWNLOAD TANZU FRAMEWORK ---
    TAC_PRODUCT_SLUG="tanzu-application-platform"
    TAC_PRODUCT_FILE="tap-gui-yelb-catalog"
    TAC_PRODUCT_VERS="$TDH_TAP_GUI_YELB_CATALOG"
    TAC_PRODUCT_PNID="$TDH_TAP_GUI_YELB_CATALOG_PIVNET_ID"
    if [ ! -e $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION/${TAC_PRODUCT_FILE}-${TAC_PRODUCT_VERS}.tgz ]; then
      pid=$(pivnetAPI $PCF_PIVNET_TOKEN GET products/$TAC_PRODUCT_SLUG/releases 2> /dev/null | \
               jq -r --arg key "$TDH_SERVICE_TAP_VERSION" '.releases[] | select(.version == $key).id')

      pivnetAPIdownload $PCF_PIVNET_TOKEN $TAC_PRODUCT_SLUG $pid $TAC_PRODUCT_PNID ${TAC_PRODUCT_FILE}.tgz > /tmp/error.log 2>&1
      if [ ! -s /tmp/${TAC_PRODUCT_FILE}.tgz ]; then
        logMessages /tmp/error.log
        echo "ERROR: failed to download file ${TAC_PRODUCT_FILE}.tgz"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ pivnetAPIdownload $PCF_PIVNET_TOKEN $TAC_PRODUCT_SLUG $pid $TDH_TANZU_FRAMEWORK_PIVNET_ID ${TAC_PRODUCT_FILE}.tgz"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => pivnetAPIdownload $PCF_PIVNET_TOKEN $TAC_PRODUCT_SLUG $pid $TDH_TANZU_FRAMEWORK_PIVNET_ID ${PRODUCT_FILE}.tgz"
        fi

        exit 1
      else
        mv /tmp/${TAC_PRODUCT_FILE}.tgz $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION/${TAC_PRODUCT_FILE}-${TAC_PRODUCT_VERS}.tgz
        rm -f $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION/${TAC_PRODUCT_FILE}.tgz
        ln -s ${TAC_PRODUCT_FILE}-${TAC_PRODUCT_VERS}.tgz $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION/${TAC_PRODUCT_FILE}.tgz
      fi
    fi

    (cd $TDH_DOCKER_FILES && tar xf $TDH_DOCKER_CACHE/tap-$TDH_SERVICE_TAP_VERSION/${TAC_PRODUCT_FILE}.tgz)
    find $TDH_DOCKER_FILES/yelb-catalog -type d -exec chmod 777 {} \; 2>/dev/null
    chmod -R a+r $$TDH_DOCKER_FILES/yelb-catalog 2>/dev/null
  fi

  # --- TANZU MISSION CONTROL BINARY ---
  if [ ! -s $TDH_DOCKER_CACHE/tmc-$TMC_CLI/tmc-linux-$TMC_CLI ]; then 
    TDH_TANZU_MISSION_CONTROL_REGISTRATION=true
    tmcCheckLogin --noverify

    # --- DOWNLOAD TMC CLI ---
    API_TOKEN=$(tmcAPI_getToken $TMC_SERVICE_TOKEN)
    download_url=$(tmcAPI_getCliBinary $API_TOKEN $TMC_SERVER_URL linuxX64)
    filename=$(echo $download_url | awk -F'/' '{ printf("tmc-linux-%s\n", $(NF-3))}')
    [ ! -d $TDH_DOCKER_CACHE/tmc-$TMC_CLI ] && mkdir -p $TDH_DOCKER_CACHE/tmc-$TMC_CLI

    if [ ! -s $TDH_DOCKER_CACHE/tmc-$TMC_CLI/$filename ]; then 
      messagePrint " ▪ Downloading TMC CLI BINARY"                                "$filename"
      wget $download_url -O $TDH_DOCKER_CACHE/tmc-$TMC_CLI/$filename > /tmp/error.log 2>&1; ret=$?
      [ -L $TDH_DOCKER_CACHE/tmc-$TMC_CLI/tmc-linux-$TMC_CLI ] && rm -f $TDH_DOCKER_CACHE/tmc-$TMC_CLI/tmc-linux-$TMC_CLI

      if [ ! -s $TDH_DOCKER_CACHE/tmc-$TMC_CLI/$filename ]; then
        logMessages /tmp/error.log
        echo "ERROR: failed to download $file"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ wget $download_url -O $TDH_DOCKER_CACHE/tmc-$TMC_CLI/$filename"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => wget $download_url -O $TDH_DOCKER_CACHE/tmc-$TMC_CLI/$filename"
        fi
  
        exit 1
      else
        ln -s $filename $TDH_DOCKER_CACHE/tmc-$TMC_CLI/tmc-linux-$TMC_CLI
        cp $TDH_DOCKER_CACHE/tmc-$TMC_CLI/tmc-linux-$TMC_CLI  $TDH_DOCKER_FILES/tmc
        chmod a+x $TDH_DOCKER_FILES/tmc
      fi

      REBUILD_REQUIRED_SOFTWARE=1       ## REBUILD REQUIRED BECAUSE OF SOFTWARE CHANGE
    fi
  else
    cp $TDH_DOCKER_CACHE/tmc-$TMC_CLI/tmc-linux-$TMC_CLI  $TDH_DOCKER_FILES/tmc
    chmod a+x $TDH_DOCKER_FILES/tmc
  fi

  if [ ! -e $TDH_DOCKER_CACHE/clusterctl/clusterctl-linux-amd64-$TDH_CLUSTERCTL ]; then 
    curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/$TDH_CLUSTERCTL/clusterctl-linux-amd64 \
         -o /tmp/clusterctl-linux-amd64-$TDH_CLUSTERCTL > /tmp/error.log 2>&1
    if [ ! -s /tmp/clusterctl-linux-amd64-$TDH_CLUSTERCTL ]; then
      logMessages /tmp/error.log
      echo "ERROR: failed to download clusterctl binary"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/$TDH_CLUSTERCTL/clusterctl-linux-amd64"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/$TDH_CLUSTERCTL/clusterctl-linux-amd64"
      fi

      exit 1
    else
      [ ! -d $TDH_DOCKER_CACHE/clusterctl ] && mkdir -p $TDH_DOCKER_CACHE/clusterctl
      mv /tmp/clusterctl-linux-amd64-$TDH_CLUSTERCTL $TDH_DOCKER_CACHE/clusterctl/clusterctl-linux-amd64-$TDH_CLUSTERCTL
      cp $TDH_DOCKER_CACHE/clusterctl/clusterctl-linux-amd64-$TDH_CLUSTERCTL $TDH_DOCKER_FILES/clusterctl
      chmod a+x $TDH_DOCKER_FILES/clusterctl
    fi

    REBUILD_REQUIRED_SOFTWARE=1       ## REBUILD REQUIRED BECAUSE OF SOFTWARE CHANGE
  else
    cp $TDH_DOCKER_CACHE/clusterctl/clusterctl-linux-amd64-$TDH_CLUSTERCTL $TDH_DOCKER_FILES/clusterctl
    chmod a+x $TDH_DOCKER_FILES/clusterctl
  fi

  cp $TDH_DOCKER_CACHE/tmc-$TMC_CLI/tmc-linux-$TMC_CLI $TDH_DOCKER_FILES/tmc
  chmod a+x $TDH_DOCKER_FILES/tmc

  # https://customerconnect.vmware.com/downloads/info/slug/datacenter_cloud_infrastructure/vmware_vsphere/7_0

  # --- DOWNLOAD PIVNET ---
  if [ ! -e $TDH_DOCKER_CACHE/pivnet/pivnet-linux-amd64-$TDH_PIVNET ]; then
    curl -q https://github.com/pivotal-cf/pivnet-cli/releases/download/v${TDH_PIVNET}/pivnet-linux-amd64-$TDH_PIVNET \
         -o /tmp/pivnet-linux-amd64-$TDH_PIVNET > /tmp/error.log 2>&1
    if [ ! -s /tmp/pivnet-linux-amd64-$TDH_PIVNET ]; then
      logMessages /tmp/error.log
      echo "ERROR: failed to download pivnet binary"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ curl -q https://github.com/pivotal-cf/pivnet-cli/releases/download/v${TDH_PIVNET}/pivnet-linux-amd64-$TDH_PIVNET"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => curl -q https://github.com/pivotal-cf/pivnet-cli/releases/download/v${TDH_PIVNET}/pivnet-linux-amd64-$TDH_PIVNET"
      fi

      exit 1
    else
      [ ! -d $TDH_DOCKER_CACHE/pivnet ] && mkdir -p $TDH_DOCKER_CACHE/pivnet
      mv /tmp/pivnet-linux-amd64-$TDH_PIVNET $TDH_DOCKER_CACHE/pivnet/pivnet-linux-amd64-$TDH_PIVNET
      cp $TDH_DOCKER_CACHE/pivnet/pivnet-linux-amd64-$TDH_PIVNET $TDH_DOCKER_FILES/pivnet
      chmod a+x $TDH_DOCKER_FILES/pivnet
    fi

    REBUILD_REQUIRED_SOFTWARE=1       ## REBUILD REQUIRED BECAUSE OF SOFTWARE CHANGE
  else
    cp $TDH_DOCKER_CACHE/pivnet/pivnet-linux-amd64-$TDH_PIVNET $TDH_DOCKER_FILES/pivnet
    chmod a+x $TDH_DOCKER_FILES/pivnet
  fi

  # --- TANZU BUILD SERVICE CLI (kp) BINARY ---
  TDH_BNDL_NAM="build-service"
  TDH_BNDL_VER=$TDH_SERVICE_BUILD_SERVICE_VERSION
  TDH_FILE_TAG="kp"
  TDH_FILE_STR="${TDH_FILE_TAG}-linux"
  TDH_FILE_VER=$TDH_SERVICE_BUILD_SERVICE_KP
  TDH_FILE_NAM="$TDH_FILE_TAG-linux-$TDH_FILE_VER"

  if [ ! -s $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM ]; then
    downloadFromPivnet $TDH_BNDL_NAM $TDH_BNDL_VER "$TDH_FILE_VER" "${TDH_FILE_STR}"
    if [ -f /tmp/$TDH_FILE_NAM ]; then
      [ ! -d $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER ] && mkdir -p $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER
      mv /tmp/$TDH_FILE_NAM $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER
      cp $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM $TDH_DOCKER_FILES/$TDH_FILE_TAG
      chmod a+x $TDH_DOCKER_FILES/$TDH_FILE_TAG
    fi

    REBUILD_REQUIRED_SOFTWARE=1       ## REBUILD REQUIRED BECAUSE OF SOFTWARE CHANGE
  else
    cp $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM $TDH_DOCKER_FILES/$TDH_FILE_TAG
    chmod a+x $TDH_DOCKER_FILES/$TDH_FILE_TAG
  fi

  if [ "$BUILD_MODE" == "$BUILD_MODE" ]; then
    # --- TANZU CARVEL TOOLS (imgpkg) BINARY ---
    TDH_BNDL_NAM="imgpkg"
    TDH_BNDL_VER=$TDH_CARVEL_IMGPKG
    TDH_FILE_TAG="$TDH_BNDL_NAM"
    TDH_FILE_VER=$TDH_CARVEL_IMGPKG
    #TDH_FILE_STR="${TDH_FILE_TAG} - Linux"
    #TDH_FILE_NAM="$TDH_FILE_TAG-linux-amd64-$TDH_FILE_VER"
  
    MACHINE=x86_64 ## sadbois 21.02.2022 - HACK as there is not yet an arm package on pivnet
    if [ "$MACHINE" == "arm" ]; then
      TDH_FILE_STR="$TDH_FILE_TAG-linux-arm64-$TDH_FILE_VER"
      TDH_FILE_NAM="$TDH_FILE_TAG-linux-arm64-$TDH_FILE_VER"
      TDH_FILE_ALT="$TDH_FILE_TAG-linux-arm64"
    else
      TDH_FILE_STR="${TDH_FILE_TAG} - Linux"
      TDH_FILE_NAM="$TDH_FILE_TAG-linux-amd64-$TDH_FILE_VER"
      TDH_FILE_ALT="$TDH_FILE_TAG-linux-amd64"
    fi
  
    if [ ! -s $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM ]; then
      downloadFromPivnet $TDH_BNDL_NAM $TDH_BNDL_VER "$TDH_FILE_VER" "${TDH_FILE_STR}"
      [ -s /tmp/$TDH_FILE_ALT ] && mv /tmp/$TDH_FILE_ALT /tmp/$TDH_FILE_NAM
      if [ -s /tmp/$TDH_FILE_NAM ]; then
        [ ! -d $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER ] && mkdir -p $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER
        mv /tmp/$TDH_FILE_NAM $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER
        cp $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM $TDH_DOCKER_FILES/$TDH_FILE_TAG
        chmod a+x $TDH_DOCKER_FILES/$TDH_FILE_TAG
      fi
  
      REBUILD_REQUIRED_SOFTWARE=1       ## REBUILD REQUIRED BECAUSE OF SOFTWARE CHANGE
    else
        cp $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM $TDH_DOCKER_FILES/$TDH_FILE_TAG
      chmod a+x $TDH_DOCKER_FILES/$TDH_FILE_TAG
    fi
  
    # --- TANZU CARVEL TOOLS (kbld) BINARY ---
    TDH_BNDL_NAM="kbld"
    TDH_BNDL_VER=$TDH_CARVEL_KBLD
    TDH_FILE_TAG="$TDH_BNDL_NAM"
    TDH_FILE_VER=$TDH_BNDL_VER
  
    if [ "$MACHINE" == "arm" ]; then
      TDH_FILE_STR="$TDH_FILE_TAG-linux-arm64-$TDH_FILE_VER"
      TDH_FILE_NAM="$TDH_FILE_TAG-linux-arm64-$TDH_FILE_VER"
      TDH_FILE_ALT="$TDH_FILE_TAG-linux-arm64"
    else
      TDH_FILE_STR="${TDH_FILE_TAG} - Linux"
      TDH_FILE_NAM="$TDH_FILE_TAG-linux-amd64-$TDH_FILE_VER"
      TDH_FILE_ALT="$TDH_FILE_TAG-linux-amd64"
    fi
  
    if [ ! -s $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM ]; then
      downloadFromPivnet $TDH_BNDL_NAM $TDH_BNDL_VER "$TDH_FILE_VER" "${TDH_FILE_STR}"
      [ -s /tmp/$TDH_FILE_ALT ] && mv /tmp/$TDH_FILE_ALT /tmp/$TDH_FILE_NAM
      if [ -s /tmp/$TDH_FILE_NAM ]; then
        [ ! -d $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER ] && mkdir -p $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER
        mv /tmp/$TDH_FILE_NAM $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER
        cp $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM $TDH_DOCKER_FILES/$TDH_FILE_TAG
        chmod a+x $TDH_DOCKER_FILES/$TDH_FILE_TAG
      fi
  
      REBUILD_REQUIRED_SOFTWARE=1       ## REBUILD REQUIRED BECAUSE OF SOFTWARE CHANGE
    else
      cp $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM $TDH_DOCKER_FILES/$TDH_FILE_TAG
      chmod a+x $TDH_DOCKER_FILES/$TDH_FILE_TAG
    fi

    # --- TANZU CARVEL TOOLS (kbld) BINARY ---
    TDH_BNDL_NAM="kapp"
    TDH_BNDL_VER=$TDH_CARVEL_KAPP
    TDH_FILE_TAG="$TDH_BNDL_NAM"
    TDH_FILE_VER=$TDH_BNDL_VER
  
    if [ "$MACHINE" == "arm" ]; then
      TDH_FILE_STR="$TDH_FILE_TAG-linux-arm64-$TDH_FILE_VER"
      TDH_FILE_NAM="$TDH_FILE_TAG-linux-arm64-$TDH_FILE_VER"
      TDH_FILE_ALT="$TDH_FILE_TAG-linux-arm64"
    else
      TDH_FILE_STR="${TDH_FILE_TAG} - Linux"
      TDH_FILE_NAM="$TDH_FILE_TAG-linux-amd64-$TDH_FILE_VER"
      TDH_FILE_ALT="$TDH_FILE_TAG-linux-amd64"
    fi
  
    if [ ! -s $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM ]; then
      downloadFromPivnet $TDH_BNDL_NAM $TDH_BNDL_VER "$TDH_FILE_VER" "${TDH_FILE_STR}"
      [ -s /tmp/$TDH_FILE_ALT ] && mv /tmp/$TDH_FILE_ALT /tmp/$TDH_FILE_NAM
      if [ -s /tmp/$TDH_FILE_NAM ]; then
        [ ! -d $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER ] && mkdir -p $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER
        mv /tmp/$TDH_FILE_NAM $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER
        cp $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM $TDH_DOCKER_FILES/$TDH_FILE_TAG
        chmod a+x $TDH_DOCKER_FILES/$TDH_FILE_TAG
      fi
  
      REBUILD_REQUIRED_SOFTWARE=1       ## REBUILD REQUIRED BECAUSE OF SOFTWARE CHANGE
    else
      cp $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM $TDH_DOCKER_FILES/$TDH_FILE_TAG
      chmod a+x $TDH_DOCKER_FILES/$TDH_FILE_TAG
    fi
  
    # --- TANZU CARVEL TOOLS (kbld) BINARY ---
    TDH_BNDL_NAM="ytt"
    TDH_BNDL_VER=$TDH_CARVEL_YTT
    TDH_FILE_TAG="$TDH_BNDL_NAM"
    TDH_FILE_VER=$TDH_BNDL_VER

    if [ "$MACHINE" == "arm" ]; then
      TDH_FILE_STR="$TDH_FILE_TAG-linux-arm64-$TDH_FILE_VER"
      TDH_FILE_NAM="$TDH_FILE_TAG-linux-arm64-$TDH_FILE_VER"
      TDH_FILE_ALT="$TDH_FILE_TAG-linux-arm64"
    else
      TDH_FILE_STR="${TDH_FILE_TAG} - Linux"
      TDH_FILE_NAM="$TDH_FILE_TAG-linux-amd64-$TDH_FILE_VER"
      TDH_FILE_ALT="$TDH_FILE_TAG-linux-amd64"
    fi
  
    if [ ! -s $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM ]; then
      downloadFromPivnet $TDH_BNDL_NAM $TDH_BNDL_VER "$TDH_FILE_VER" "${TDH_FILE_STR}"
      [ -s /tmp/$TDH_FILE_ALT ] && mv /tmp/$TDH_FILE_ALT /tmp/$TDH_FILE_NAM
      if [ -s /tmp/$TDH_FILE_NAM ]; then
        [ ! -d $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER ] && mkdir -p $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER
        mv /tmp/$TDH_FILE_NAM $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER
        cp $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM $TDH_DOCKER_FILES/$TDH_FILE_TAG
        chmod a+x $TDH_DOCKER_FILES/$TDH_FILE_TAG
      fi
  
      REBUILD_REQUIRED_SOFTWARE=1       ## REBUILD REQUIRED BECAUSE OF SOFTWARE CHANGE
    else
      cp $TDH_DOCKER_CACHE/$TDH_FILE_TAG-$TDH_FILE_VER/$TDH_FILE_NAM $TDH_DOCKER_FILES/$TDH_FILE_TAG
      chmod a+x $TDH_DOCKER_FILES/$TDH_FILE_TAG
    fi
  fi

  if [ ! -s $TDH_DOCKER_FILES/kubectl-vsphere ]; then
    cp $TDHPATH/software/vsphere-plugin-Linux.zip /tmp
    unzip -d /tmp /tmp/vsphere-plugin-Linux.zip > /dev/null 2>&1
    chmod a+x /tmp/bin/kubectl-vsphere /tmp/bin/kubectl
    mv /tmp/bin/kubectl-vsphere $TDH_DOCKER_FILES
    mv /tmp/bin/kubectl $TDH_DOCKER_FILES

    REBUILD_REQUIRED_SOFTWARE=1       ## REBUILD REQUIRED BECAUSE OF SOFTWARE CHANGE 
  fi

  mkdir -p $TDH_DOCKER_FILES
  (cd $TDH_DOCKER_FILES; tar xf vmware-ovftool.tar)

  if [ ! -s $TDH_DOCKER_FILES/argocd ]; then
    #wget https://github.com/argoproj/argo-cd/releases/latest/download/argocd-util-linux-amd64  2>/dev/null 1>&2
    wget https://github.com/argoproj/argo-cd/releases/download/v2.1.2/argocd-linux-amd64 2>/dev/null 1>&2
    [ -f argocd-linux-amd64 ] && mv argocd-linux-amd64 $TDH_DOCKER_FILES/argocd && chmod +x $TDH_DOCKER_FILES/argocd
  fi

  # --- TERRAFORM BINARY ---
  if [ ! -s $TDH_DOCKER_CACHE/terraform/terraform-$TDH_TERRAFORM ]; then
    messagePrint " ▪ Downloading Terraform binary"                                "terraform_${TDH_TERRAFORM}_linux_amd64.zip"
    [ ! -s $TDH_DOCKER_CACHE/terraform ] && mkdir -p $TDH_DOCKER_CACHE/terraform
    curl -q https://releases.hashicorp.com/terraform/$TDH_TERRAFORM/terraform_${TDH_TERRAFORM}_linux_amd64.zip \
         -o /tmp/terraform_${TDH_TERRAFORM}_linux_amd64.zip > /tmp/error.log 2>&1
    if [ ! -s /tmp/terraform_${TDH_TERRAFORM}_linux_amd64.zip ]; then
      logMessages /tmp/error.log
      echo "ERROR: failed to download $file"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ wget https://releases.hashicorp.com/terraform/$TDH_TERRAFORM/terraform_${TDH_TERRAFORM}_linux_amd64.zip"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => wget https://releases.hashicorp.com/terraform/$TDH_TERRAFORM/terraform_${TDH_TERRAFORM}_linux_amd64.zip"
      fi

      exit 1
    else
      unzip -o -d /tmp /tmp/terraform_${TDH_TERRAFORM}_linux_amd64.zip 2>/dev/null 1>&2
      mv /tmp/terraform $TDH_DOCKER_CACHE/terraform/terraform-$TDH_TERRAFORM
      cp $TDH_DOCKER_CACHE/terraform/terraform-$TDH_TERRAFORM $TDH_DOCKER_FILES/terraform-bin
      chmod +x $TDH_DOCKER_FILES/terraform-bin
    fi

    REBUILD_REQUIRED_SOFTWARE=1       ## REBUILD REQUIRED BECAUSE OF SOFTWARE CHANGE
  else
    cp $TDH_DOCKER_CACHE/terraform/terraform-$TDH_TERRAFORM $TDH_DOCKER_FILES/terraform-bin
    chmod +x $TDH_DOCKER_FILES/terraform-bin
  fi

  # --- KUBERNETES KIND BINARY ---
  if [ ! -s $TDH_DOCKER_CACHE/kind/kind-linux-amd64-$TDH_KIND ]; then
    messagePrint " ▪ Downloading Kind binary"                                "kind-${TDH_KIND}-linux-amd64"
    [ ! -s $TDH_DOCKER_CACHE/kind ] && mkdir -p $TDH_DOCKER_CACHE/kind

    wget https://kind.sigs.k8s.io/dl/v${TDH_KIND}/kind-linux-amd64 \
         -O /tmp/kind-linux-amd64-$TDH_KIND > /tmp/error.log 2>&1
    if [ ! -f /tmp/kind-linux-amd64-${TDH_KIND} ]; then
      logMessages /tmp/error.log
      echo "ERROR: failed to download $file"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ wget https://kind.sigs.k8s.io/dl/v${TDH_KIND}/kind-linux-amd64"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => wget https://kind.sigs.k8s.io/dl/v${TDH_KIND}/kind-linux-amd64"
      fi

      exit 1
    else
      mv /tmp/kind-linux-amd64-${TDH_KIND} $TDH_DOCKER_CACHE/kind
      cp $TDH_DOCKER_CACHE/kind/kind-linux-amd64-$TDH_KIND $TDH_DOCKER_FILES/kind
      chmod +x $TDH_DOCKER_FILES/kind
    fi

    REBUILD_REQUIRED_SOFTWARE=1       ## REBUILD REQUIRED BECAUSE OF SOFTWARE CHANGE
  else
    cp $TDH_DOCKER_CACHE/kind/kind-linux-amd64-$TDH_KIND $TDH_DOCKER_FILES/kind
    chmod +x $TDH_DOCKER_FILES/kind
  fi

  # --- GOVC BINARY ---
  filename="govc_Linux_x86_64"
  if [ ! -s $TDH_DOCKER_CACHE/govc/${filename}-$TDH_GOVC ]; then
    messagePrint " ▪ Downloading vSphere CLI (GOVC)"                                "govc-${TDH_GOVC}-linux-amd64"
    [ ! -s $TDH_DOCKER_CACHE/govc ] && mkdir -p $TDH_DOCKER_CACHE/govc

    wget https://github.com/vmware/govmomi/releases/download/v${TDH_GOVC}/${filename}.tar.gz \
         -O /tmp/${filename}.tar.gz > /tmp/error.log 2>&1
    if [ ! -f /tmp/${filename}.tar.gz ]; then
      logMessages /tmp/error.log
      echo "ERROR: failed to download ${filename}.tar.gz"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ wget https://github.com/vmware/govmomi/releases/download/v${TDH_GOVC}/${filename}.tar.gz"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => wget https://github.com/vmware/govmomi/releases/download/v${TDH_GOVC}/${filename}.tar.gz"
      fi

      exit 1
    else
      (cd /tmp; tar zxf ${filename}.tar.gz)
      mv /tmp/govc $TDH_DOCKER_CACHE/govc/${filename}-${TDH_GOVC}
      [ -L $TDH_DOCKER_CACHE/govc/govc ] && rm -f $TDH_DOCKER_CACHE/govc/govc
      ln -s ${filename}-${TDH_GOVC} $TDH_DOCKER_CACHE/govc/govc
    fi

    REBUILD_REQUIRED_SOFTWARE=1       ## REBUILD REQUIRED BECAUSE OF SOFTWARE CHANGE
  fi

  cp $TDH_DOCKER_CACHE/govc/govc $TDH_DOCKER_FILES/govc
  chmod +x $TDH_DOCKER_FILES/govc

  if [ ! -s $TDH_DOCKER_FILES/ovftool ]; then
    echo -e "\n\n\nyes" | nohup $TDHPATH/software/VMware-ovftool-4.4.1-16812187-lin.x86_64.bundle > /dev/null 2>&1
  fi

  #messagePrint " ▪ Docker Image ($TDH_TOOLS_IMAGE) Name / ID"                  "$TDH_TOOLS_IMAGE:latest / $DOCKER_IMAGE_LATEST_ID"

  cnt=$(docker images | egrep -c "^$TDH_TOOLS_IMAGE ") 
  if [ $cnt -gt 0 ]; then 
    if [ "$DOCKER_IMAGE_CHKSUM" == "$DOCKER_CHKSUM" ]; then 
      messagePrint " ▪ Dockerfile Checksum (files/tdh-tools/$TDH_TOOLS_DOCKERFILE)"  "$DOCKER_CHKSUM"
      messagePrint " ▪ Docker Image ($TDH_TOOLS_IMAGE) Checksum"                 "$DOCKER_IMAGE_CHKSUM => No Rebuild required"
    else
      if [ "$DOCKER_IMAGE_CHKSUM" == "" ]; then 
        messagePrint " ▪ Dockerfile Checksum (iles/tdh-tools/$TDH_TOOLS_DOCKERFILE)"  "$DOCKER_CHKSUM"
        messagePrint " ▪ Docker Image ($TDH_TOOLS_IMAGE) Checksum"                     "container not builded yet"
      else
        messagePrint " ▪ Dockerfile Checksum (files/tdh-tools/$TDH_TOOLS_DOCKERFILE)"  "$DOCKER_CHKSUM"
        messagePrint " ▪ Docker Image ($TDH_TOOLS_IMAGE) Checksum"                 "$DOCKER_IMAGE_CHKSUM => Rebuild required"
      fi
      REBUILD_REQUIRED=1

      docker image rm $DOCKER_IMAGE_ID -f > /dev/null 2>&1
    fi
  else
    messagePrint " ▪ Docker Image ($TDH_TOOLS_IMAGE:latest) Not available"       "Initial Build"
    REBUILD_REQUIRED=1
  fi

  if [ ! -s $TDH_DOCKER_FILES/mc ]; then
    wget https://dl.min.io/client/mc/release/linux-amd64/mc > /dev/null 2>&1
    mv mc $TDH_DOCKER_FILES && chmod a+x $TDH_DOCKER_FILES/mc
  fi

  # --- DOWNLOAD CARVEL TOOLS ---
  if [ $BUILD_MODE == "tce" ]; then
    TCE_DOWNLOAD_URL=https://github.com/vmware-tanzu/community-edition/releases/download/${TKG_COMNUNITY_EDITION_VERSION}
    TCE_LINUX_PACKAGE=tce-linux-amd64-${TKG_COMNUNITY_EDITION_VERSION}

    appname="tce"
    filename="${appname}-linux-amd64"
    version="$TKG_COMNUNITY_EDITION_VERSION"
    if [ ! -s $TDH_DOCKER_CACHE/$appname/${filename}-${version}.tar.gz ]; then
      messagePrint " ▪ Downloading Tanzu Comnunity Edition (TCE)"                                "${filename}.tar.gz"
      [ ! -s $TDH_DOCKER_CACHE/$appname ] && mkdir -p $TDH_DOCKER_CACHE/$appname

      wget -q $TCE_DOWNLOAD_URL/${TCE_LINUX_PACKAGE}.tar.gz \
           -O /tmp/${filename}-${version}.tar.gz > /tmp/error.log 2>&1
      if [ ! -s /tmp/${filename}-${version}.tar.gz ]; then
        logMessages /tmp/error.log
        echo "ERROR: failed to download ${filename}.tar.gz"
        if [ "$NATIVE" == "0" ]; then
          echo "       => tools/${TDH_TOOLS}.sh"
          echo "          tdh-tools:/$ wget $TCE_DOWNLOAD_URL/${TCE_LINUX_PACKAGE}.tar.gz"
          echo "          tdh-tools:/$ exit"
        else
          echo "       => wget $TCE_DOWNLOAD_URL/${TCE_LINUX_PACKAGE}.tar.gz"
        fi
  
        exit 1
      else
        mv /tmp/${filename}-${version}.tar.gz $TDH_DOCKER_CACHE/$appname/
        (cd $TDH_DOCKER_FILES; tar xvfz $TDH_DOCKER_CACHE/$appname/${filename}-${version}.tar.gz > /dev/null 2>&1)
        rm -f $TDH_DOCKER_FILES/tce-linux-amd64
        ln -s ${filename}-${version} $TDH_DOCKER_FILES/${filename}
      fi
  
      REBUILD_REQUIRED_SOFTWARE=1       ## REBUILD REQUIRED BECAUSE OF SOFTWARE CHANGE
    fi
  
    (cd $TDH_DOCKER_FILES; tar xvfz $TDH_DOCKER_CACHE/$appname/${filename}-${version}.tar.gz > /dev/null 2>&1) 
    rm -f $TDH_DOCKER_FILES/tce-linux-amd64
    ln -s ${filename}-${version} $TDH_DOCKER_FILES/${filename}
  fi

  # --- POSTINSTALL SCRIPTS
  cp $TDHPATH/files/tdh-tools/tdh-postinstall-root.sh $TDH_DOCKER_FILES
  chmod a+x $TDH_DOCKER_FILES/tdh-postinstall-root.sh
  cp $TDHPATH/files/tdh-tools/tdh-context.sh $TDH_DOCKER_FILES
  chmod a+x $TDH_DOCKER_FILES/tdh-context.sh

  if [ "$MODE" == "tkg" ]; then 
    cp $TDHPATH/files/tdh-tools/tdh-postinstall-user-tkg-${TDH_TOOLS_TAG}.sh $TDH_DOCKER_FILES/tdh-postinstall-user.sh
    chmod a+x $TDH_DOCKER_FILES/tdh-postinstall-user.sh
  fi

  if [ "$MODE" == "tce" ]; then 
    cp $TDHPATH/files/tdh-tools/tdh-postinstall-user-tce-${TDH_TOOLS_TAG}.sh $TDH_DOCKER_FILES/tdh-postinstall-user.sh
    chmod a+x $TDH_DOCKER_FILES/tdh-postinstall-user.sh
  fi

  echo tdh-tools > $TDH_DOCKER_FILES/hostname

  if [ $BUILD_MODE == "tkg" ]; then
    # --- CLEANUP ---
    [ "$TDH_DOCKER_FILES" != "" ] && rm -rf $TDH_DOCKER_FILES/cli $TDH_DOCKER_FILES/cli-${TKG_TANZU_KUBERNETES_GRID} 

    # --- CHECK WETHER THE BUNDLE IS INSTALLED ---
    if [ ! -d $TDH_DOCKER_FILES/cli-${TKG_TANZU_KUBERNETES_GRID} -o $TDH_DOCKER_FILES/cli-${TKG_TANZU_KUBERNETES_GRID}/core/v*/tanzu-core-linux_amd64 ]; then
      TANZU_DEMO_HUB_CACHE=$HOME/.tanzu-demo-hub/cache.orig
      TANZU_DEMO_HUB_CACHE=$HOME/.tanzu-demo-hub/cache
      TANZU_CLI_BUNDLE=$TDH_DOCKER_FILES/tkg-${TKG_TANZU_KUBERNETES_GRID}
      TANZU_CLI_BUNDLE=$TANZU_DEMO_HUB_CACHE/tkg-${TKG_TANZU_KUBERNETES_GRID}

      [ -s $TANZU_CLI_BUNDLE/tanzu-cli-bundle-v${TKG_TANZU_KUBERNETES_GRID}-linux-amd64.tar.gz ] && gunzip $TANZU_CLI_BUNDLE/tanzu-cli-bundle-v${TKG_TANZU_KUBERNETES_GRID}-linux-amd64.tar
      [ -s $TANZU_CLI_BUNDLE/tanzu-cli-bundle-linux-amd64.tar.gz ] && gunzip $TANZU_CLI_BUNDLE/tanzu-cli-bundle-linux-amd64.tar

      if [ ! -f $TANZU_CLI_BUNDLE/tanzu-cli-bundle-v${TKG_TANZU_KUBERNETES_GRID}-linux-amd64.tar -a \
           ! -f $TANZU_CLI_BUNDLE/tanzu-cli-bundle-linux-amd64.tar ]; then
        VMWCLI=/tmp/vmw-cli

        if [ ! -s $VMWCLI ]; then
          messagePrint " ▪ Install Package (vmw-cli)" "installing"
          docker run apnex/vmw-cli shell > $VMWCLI 2>/dev/null
          if [ $? -ne 0 ]; then
            echo "ERROR: faileed to run vmw-cli docker container"
            echo "       => docker run apnex/vmw-cli shell > /tmp/vmw-cli"
            exit 1
          fi

          chmod 755 $VMWCLI
        fi

        #################################################################################
        ################# DOWNLOAD TANZU-CLI-BUNDLE FROM MYVMWARE.COM ###################
        #################################################################################
        export VMWUSER="$TDH_MYVMWARE_USER"
        export VMWPASS="$TDH_MYVMWARE_PASS"
        $VMWCLI ls vmware_tanzu_kubernetes_grid > /dev/null 2>&1

        cnt=$($VMWCLI ls vmware_tanzu_kubernetes_grid 2>/dev/null | egrep "^tanzu-cli-bundle" | grep -c "$TKG_TANZU_KUBERNETES_GRID")
        if [ $cnt -eq 0 ]; then
          echo "ERROR: failed to download tanzu-cli-bundle version: $TKG_TANZU_KUBERNETES_GRID from myVMware.com. This version does not"
          echo "       seam to be available anymore"
          messageLine
          $VMWCLI ls vmware_tanzu_kubernetes_grid | egrep "^tanzu-cli-bundle"
          messageLine
          exit 1
        fi

        cnt=0
        vmwfile=$($VMWCLI ls vmware_tanzu_kubernetes_grid 2>/dev/null | egrep "^tanzu-cli-bundle" | grep linux | tail -1 | awk '{ print $1 }')
        while [ "$vmwfile" == "" -a $cnt -lt 5 ]; do
          vmwfile=$($VMWCLI ls vmware_tanzu_kubernetes_grid 2>/dev/null | egrep "^tanzu-cli-bundle" | grep linux | tail -1 | awk '{ print $1 }')
          let cnt=cnt+1
          sleep 10
        done

        messagePrint " ▪ Downloading Tanzu CLI Bundle" "${TANZU_CLI_BUNDLE}/$vmwfile"

        if [ "$vmwfile" == "" ]; then
          echo "ERROR: failed to download $vmwfile"
          echo "       => export VMWUSER=\"$TDH_MYVMWARE_USER\""
          echo "       => export VMWPASS=\"$TDH_MYVMWARE_PASS\""
          echo "       => $VMWCLI ls vmware_tanzu_kubernetes_grid"
          echo ""
          echo "INFO:  You may download tanzu-cli-bundle $TKG_TANZU_KUBERNETES_GRID manually from myVMware.com. an place it under:"
          echo "       $TANZU_CLI_BUNDLE"
          exit 1
        else
          cnt=0
          while [ ! -f "$vmwfile" -a $cnt -lt 10 ]; do
            $VMWCLI ls vmware_tanzu_kubernetes_grid > /dev/null 2>&1
            $VMWCLI cp $vmwfile > /dev/null 2>&1
            let cnt=cnt+1
            sleep 30
          done

          if [ ! -f $vmwfile ]; then
            echo "ERROR: failed to download $vmwfile"
            echo "       => export VMWUSER=\"$TDH_MYVMWARE_USER\""
            echo "       => export VMWPASS=\"$TDH_MYVMWARE_PASS\""
            echo "       => $VMWCLI ls vmware_tanzu_kubernetes_grid"
            echo "       => $VMWCLI cp $vmwfile"
            echo ""
            echo "INFO:  You may download tanzu-cli-bundle $TKG_TANZU_KUBERNETES_GRID manually from myVMware.com. an place it under:"
            echo "       $TANZU_CLI_BUNDLE"
            exit 1
          else
            [ ! -d $TANZU_CLI_BUNDLE ] && mkdir -p $TANZU_CLI_BUNDLE
            mv $vmwfile ${TANZU_CLI_BUNDLE}/$vmwfile
          fi
        fi

        TANZU_CLI_BUNDLE=${TANZU_CLI_BUNDLE}/$vmwfile
      else
        if [ -f $TANZU_CLI_BUNDLE/tanzu-cli-bundle-linux-amd64.tar ]; then  
          TANZU_CLI_BUNDLE=${TANZU_DEMO_HUB_CACHE}/tkg-${TKG_TANZU_KUBERNETES_GRID}/tanzu-cli-bundle-linux-amd64.tar 
        else
          TANZU_CLI_BUNDLE=${TANZU_DEMO_HUB_CACHE}/tkg-${TKG_TANZU_KUBERNETES_GRID}/tanzu-cli-bundle-v${TKG_TANZU_KUBERNETES_GRID}-linux-amd64.tar
        fi
      fi

      if [ ! -d $TDH_DOCKER_FILES/cli-$TKG_TANZU_KUBERNETES_GRID  ]; then
        rm -f ${TDH_DOCKER_FILES}/cli 
        tar xf $TANZU_CLI_BUNDLE && mv cli ${TDH_DOCKER_FILES}/cli-$TKG_TANZU_KUBERNETES_GRID
        cd $TDH_DOCKER_FILES && ln -s  cli-$TKG_TANZU_KUBERNETES_GRID cli
      fi
    fi

    if [ $REBUILD_REQUIRED_SOFTWARE -gt 0 ]; then 
      messagePrint " ▪ Software Updates (files/tdh-tools/tdh-tools.cfg) )"  "=> Rebuild required"
  
      REBUILD_REQUIRED=1
    fi
  fi

  if [ $REBUILD_REQUIRED -gt 0 ]; then 
    if [ ! -s $TDH_DOCKER_FILES/kubectl ]; then
      curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
      chmod a+x kubectl
      mv kubectl $TDH_DOCKER_FILES/kubectl
    fi
  
    # --- CLEANUP SOME DISK SPACE ---
    messagePrint " ▪ Cleanup unsued Docker images" "docker system prune, docker container prune"
    docker rmi $(docker images -q) > /dev/null 2>&1
    docker rm -v $(docker ps -qa) > /dev/null 2>&1
    docker container prune -f > /dev/null 2>&1
    docker system prune -f > /dev/null 2>&1
  
    time1=$(echo "($(hourInSec)*360)+($(minInSec)*60)+$(secInSec)" | bc)
    messagePrint " ▪ Building TDH Tools docker container" "$TDH_TOOLS_IMAGE:CHKSUM-$DOCKER_CHKSUM"
    #cd $TDH_DOCKER_FILES && docker build -q -t $TDH_TOOLS_IMAGE:CHKSUM-$DOCKER_CHKSUM \
    cd $TDH_DOCKER_FILES && docker build -q -t $TDH_TOOLS_IMAGE:CHKSUM-$DOCKER_CHKSUM \
      --build-arg UID=$(id -u) \
      --build-arg GID=$(id -g) \
      --build-arg USER=$USER \
      --build-arg HOME=$HOME \
      --build-arg TANZU_DEMO_HUB=$TANZU_DEMO_HUB \
      --build-arg TDHPATH=$TDHPATH \
      --build-arg ARCH=$ARCH \
      --build-arg TZ=$(ls -l /etc/localtime | /usr/bin/cut -d '/' -f 8,9) .; ret=$?
    time2=$(echo "($(hourInSec)*360)+($(minInSec)*60)+$(secInSec)" | bc)
    let time3=time2-time1
  
    if [ $ret -ne 0 ]; then 
      echo "ERROR: failed to create $TDH_TOOLS_IMAGE container"
      echo "       => cd $TDH_DOCKER_FILES && docker build -q -t $TDH_TOOLS_IMAGE:CHKSUM-$DOCKER_CHKSUM \\"
      echo "             --build-arg UID=$(id -u) \\"
      echo "             --build-arg GID=$(id -g) \\"
      echo "             --build-arg USER=$USER \\"
      echo "             --build-arg HOME=$HOME \\"
      echo "             --build-arg TANZU_DEMO_HUB=$TANZU_DEMO_HUB \\"
      echo "             --build-arg TDHPATH=$TDHPATH \\"
      echo "             --build-arg ARCH=$ARCH \\"
      echo "             --build-arg TZ=$(ls -l /etc/localtime | /usr/bin/cut -d '/' -f 8,9) ."
      exit 1
    fi
  
    messagePrint " ▪ Container Build for ($TDH_TOOLS_IMAGE) completed in" "$(duration $time3)"
    messagePrint " ▪ Create Tag (latest) for  TDH Tools docker container" "$TDH_TOOLS_IMAGE:latest"
    docker tag $TDH_TOOLS_IMAGE:CHKSUM-$DOCKER_CHKSUM $TDH_TOOLS_IMAGE:latest
  fi
}

verify_datacenter() {
  OVFCONN="vi://${VSPHERE_VCENTER_ADMIN}@${VSPHERE_VCENTER_SERVER}/${VSPHERE_DATACENTER}/host/${VSPHERE_CLUSTER}"
  export GOVC_INSECURE=1
  export GOVC_URL=https://${VSPHERE_VCENTER_SERVER}/sdk
  export GOVC_USERNAME=$VSPHERE_VCENTER_ADMIN
  export GOVC_PASSWORD=$VSPHERE_VCENTER_PASSWORD

  govc ls / > /dev/null 2>&1
  if [ $? -ne 0 ]; then
    echo ""
    echo "ERROR: Can not login to vCenter, please verify manually"
    echo "       export GOVC_INSECURE=1"
    echo "       export GOVC_URL=https://${VSPHERE_VCENTER_SERVER}/sdk"
    echo "       export GOVC_USERNAME=$VSPHERE_VCENTER_ADMIN"
    echo "       export GOVC_PASSWORD=$GOVC_PASSWORD"
    echo "       govc ls /"
    exit 1
  fi

  if [ "$VSPHERE_DATACENTER" == "" ]; then
    echo ""
    echo "Please set the variale: VSPHERE_DATACENTER=<datacenter> in $HOME/.tanzu-demo-hub.cfg"
    echo "from the following list:"
    messageLine
    govc ls /
    messageLine
    exit
  else
    cnt=$(echo "$VSPHERE_DATACENTER" | egrep -c "/")
    if [ $cnt -gt 0 ]; then
      echo "ERROR: Please set the Variable 'VSPHERE_DATACENTER' without any '/'."
      echo "       WRONG => $VSPHERE_DATACENTER"
      echo "       RIGHT => $(echo $VSPHERE_DATACENTER | awk -F'/' '{ print $NF }')"
      exit
    fi

    cnt=$(govc ls /$VSPHERE_DATACENTER | wc -l | sed 's/  *//g')
    if [ $cnt -eq 0 ]; then
      echo ""
      echo "Datacenter: '$VSPHERE_DATACENTER' not found, please choose on from the list"
      echo "below and set the $VSPHERE_DATACENTER=<datacenter> in $HOME/.tanzu-demo-hub.cfg"
      messageLine
      govc ls /$VSPHERE_DATACENTER
      messageLine
      exit 1
    fi
  fi

  if [ "$VSPHERE_DATASTORE" == "" ]; then
    echo ""
    echo "Please set the variale: VSPHERE_DATASTORE=<datastore> in $HOME/.tanzu-demo-hub.cfg"
    echo "from the following list:"
    messageLine
    govc ls /$VSPHERE_DATACENTER/datastore
    messageLine
    exit 1
  else
    cnt=$(echo "$VSPHERE_DATASTORE" | egrep -c "/")
    if [ $cnt -gt 0 ]; then
      echo "ERROR: Please set the Variable 'VSPHERE_DATASTORE' without any '/'."
      echo "       WRONG => $VSPHERE_DATASTORE"
      echo "       RIGHT => $(echo $VSPHERE_DATASTORE | awk -F'/' '{ print $NF }')"
      exit
    fi

    cnt=$(govc ls /$VSPHERE_DATACENTER/datastore/$VSPHERE_DATASTORE | wc -l | sed 's/  *//g')
    if [ $cnt -eq 0 ]; then
      echo ""
      echo "Datastore: '$VSPHERE_DATASTORE' not found, please choose on from the list"
      echo "below and set the VSPHERE_DATASTORE=<datastore> in $HOME/.tanzu-demo-hub.cfg"
      messageLine
      govc ls /$VSPHERE_DATACENTER/datastore
      messageLine
      exit 1
    fi
  fi

  if [ "$VSPHERE_CLUSTER" == "" ]; then
    echo ""
    echo "Please set the variale: VSPHERE_CLUSTER=<cluster> in $HOME/.tanzu-demo-hub.cfg"
    echo "from the following list:"
    messageLine
    govc ls /$VSPHERE_DATACENTER/host
    messageLine
    exit 1
  else
    cnt=$(echo "$VSPHERE_CLUSTER" | egrep -c "/")
    if [ $cnt -gt 0 ]; then
      echo "ERROR: Please set the Variable 'VSPHERE_CLUSTER' without any '/'."
      echo "       WRONG => $VSPHERE_CLUSTER"
      echo "       RIGHT => $(echo $VSPHERE_CLUSTER | awk -F'/' '{ print $NF }')"
      exit
    fi

    cnt=$(govc ls /$VSPHERE_DATACENTER/host/$VSPHERE_CLUSTER | wc -l | sed 's/  *//g')
    if [ $cnt -eq 0 ]; then
      echo ""
      echo "Datacenter: '$VSPHERE_CLUSTER' not found, please choose on from the list"
      echo "below and set the $VSPHERE_CLUSTER=<cluster> in $HOME/.tanzu-demo-hub.cfg"
      messageLine
      govc ls /$VSPHERE_DATACENTER/host
      messageLine
      exit 1
    fi
  fi

  if [ "$VSPHERE_NETWORK" == "" ]; then
    echo ""
    echo "Please set the variale: VSPHERE_NETWORK=<network> in $HOME/.tanzu-demo-hub.cfg"
    echo "from the following list:"
    messageLine
    govc ls /$VSPHERE_DATACENTER/network
    messageLine
    exit 1
  else
    cnt=$(echo "$VSPHERE_NETWORK" | egrep -c "/")
    if [ $cnt -gt 0 ]; then
      echo "ERROR: Please set the Variable 'VVSPHERE_NETWORK' without any '/'."
      echo "       WRONG => $VSPHERE_NETWORK"
      echo "       RIGHT => $(echo $VSPHERE_NETWORK | awk -F'/' '{ print $NF }')"
      exit
    fi

    cnt=$(govc ls "/$VSPHERE_DATACENTER/network/$VSPHERE_NETWORK" | wc -l | sed 's/  *//g')
    if [ $cnt -eq 0 ]; then
      echo ""
      echo "Datacenter: '$VSPHERE_NETWORK' not found, please choose on from the list"
      echo "below and set the $VSPHERE_NETWORK=<network> in $HOME/.tanzu-demo-hub.cfg"
      messageLine
      govc ls /$VSPHERE_DATACENTER/network
      messageLine
      exit 1
    fi
  fi

}

getAPItoken() {
  LOGIN_STR="$VSPHERE_VCENTER_ADMIN:$VSPHERE_VCENTER_PASSWORD"
  curl -k -X POST -u $LOGIN_STR https://$VSPHERE_VCENTER_SERVER/rest/com/vmware/cis/session 2>/dev/null| jq -r '.value'
}

vsNamespaceCreate() {
  NAMESPACE=$1
  STORARGE_POLICY=$2
  USER=$3
  API_HOST=$VSPHERE_VCENTER_SERVER
  API_TOKEN=$(getAPItoken)

  CLUSTER_DOMAIN=$(curl -k -H "vmware-api-session-id: $API_TOKEN" https://$API_HOST/api/vcenter/namespaces/instances 2>/dev/null | jq -r '.[].cluster' | head -1)
  STORARGE_POLICY_ID=$(curl -k -H "vmware-api-session-id: $API_TOKEN" https://$API_HOST/api/vcenter/storage/policies 2>/dev/null | \
    jq --arg policy "$STORARGE_POLICY" -r '.[] | select(.name == $policy).policy')

  curl -k -X POST -H "vmware-api-session-id: $API_TOKEN" -H "Content-Type: application/json" \
  -d "{ \"cluster\": \"$CLUSTER_DOMAIN\", \"namespace\": \"$NAMESPACE\", \"storage_specs\": [{\"policy\":\"$STORARGE_POLICY_ID\"}], \"access_list\":[{\"domain\":\"vsphere.local\",\"role\":\"EDIT\",\"subject\":\"$USER\",\"subject_type\":\"USER\"}] }" \
  https://$API_HOST/api/vcenter/namespaces/instances 2>/dev/null; ret=$?
  
  if [ $? -ne 0 ]; then 
    echo "ERROR: failed to create vSphere Namespace $NAMESPACE"
    exit 1
  fi
}


vsNamespaceGet() {
  NAMESPACE=$1
  API_HOST=$VSPHERE_VCENTER_SERVER
  API_TOKEN=$(getAPItoken)

  curl -k -H "vmware-api-session-id: $API_TOKEN" https://$API_HOST/api/vcenter/namespaces/instances 2>/dev/null | jq -r
} 

vsNanespaceExists() {
  NAMESPACE=$1
  API_HOST=$VSPHERE_VCENTER_SERVER
  API_TOKEN=$(getAPItoken)

  STT=$(curl -k -H "vmware-api-session-id: $API_TOKEN" https://$API_HOST/api/vcenter/namespaces/instances 2>/dev/null | \
    jq -r --arg ns "$NAMESPACE" '.[] | select(.namespace == $ns).namespace')

  if [ "$STT" != "" ]; then echo "true"; else echo "false"; fi
}

installTanzuPlugins() {
  cnt=$(tanzu plugin list | grep -v "alpha" | grep -c "not installed") 
  if [ $cnt -gt 0 -a $NATIVE -eq 0 ]; then
    messagePrint " ▪ Install Tanzu Plugins" "tanzu plugin install --local cli all"
    cd /tanzu && tanzu plugin install --local cli all > /dev/null 2>&1
  fi
}

patchServiceAccount() {
  SERVICE_ACCOUNT=$1
  NAMESPACE=$2

  messagePrint " ▪ Patch Docker RateLimit in ServiceAccount"  "$NAMESPACE/$SERVICE_ACCOUNT"
  kubectl -n $NAMESPACE delete secret tdh-docker-repo >/dev/null 2>&1
  cmdLoop kubectl -n $NAMESPACE create secret docker-registry tdh-docker-repo \
          --docker-server $TDH_REGISTRY_DOCKER_NAME \
          --docker-username $TDH_REGISTRY_DOCKER_USER \
          --docker-password $TDH_REGISTRY_DOCKER_PASS >/dev/null 2>&1

  kubectl patch serviceaccount default -p '{"imagePullSecrets": [{"name": "tdh-docker-repo"}]}' -n $NAMESPACE > /dev/null 2>&1; ret=$?
  if [ $ret -ne 0 ]; then
    echo "ERROR: Failed to Patch Service account"
    echo "       => kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"tdh-docker-repo\"}]}' -n $NAMESPACE"
    exit
  fi
} 

checkExecutionLock() {
  LOCK_APP=$1
  LOCK_FILE=/var/tmp/${LOCK_APP}.lock

  if [ -f $LOCK_FILE ]; then
    read pid < $LOCK_FILE
    ps -p $pid > /dev/null 2>&1
    if [ $? -eq 0 ]; then
      return 1
    else
      echo $$ > $LOCK_FILE
      return 0
    fi
  else
    echo $$ > $LOCK_FILE
    return 0
  fi
}

setK8sEnvironment() {
  CLUSTER=$1

  messagePrint " ▪ Set Kubernetes Cluster"               "$CLUSTER"
  cmdLoop kubectl config set-cluster $CLUSTER > /dev/null 2>&1
  if [ $? -ne 0 ]; then 
    echo "ERROR: failed to set cluster $CLUSTER"
    echo "       => kubectl config set-cluster $CLUSTER"
    exit 1
  fi

  ##DELETEME
  #CONTEXT=$(kubectl config get-contexts | grep " $CLUSTER " | sed 's/*//g' | awk '{ print $1 }')

echo gaga1
  rm -f /tmp/output.json && touch /tmp/output.json
  while [ -s /tmp/output.json ]; do
echo gaga11
    cmdLoop kubectl config view -o json > /tmp/output.json
    CONTEXT=$(jq -r --arg key tdh-docker-sadubois '.contexts[] | select(.context.cluster == $key).name' /tmp/output.json 2>/dev/null) 
echo "gagadu: CONTEXT:$CONTEXT"
    [ "$CONTEXT" == "" -o "$CONTEXT" == "null" ] && rm -f /tmp/output.json && touch /tmp/output.json
    [ ! -s /tmp/output.json ] && break

    sleep 10
  done

ls -la /tmp/output.json
cat /tmp/output.json
kubectl config view -o json 

  #CONTEXT=$(jq -r --arg key tdh-docker-sadubois '.contexts[] | select(.context.cluster == $key).name' /tmp/output.json 2>/dev/null) 
  messagePrint " ▪ Set Kubernetes Context"               "$CONTEXT"
  cmdLoop kubectl config use-context $CONTEXT > /dev/null 2>&1
  if [ $? -ne 0 ]; then
    echo "ERROR: failed to set cluster context"
    echo "       => kubectl config use-context $CONTEXT"
    exit 1
  fi

  messagePrint " ▪ Set Kubernetes clusterrolebinding"    "tanzu-demo-hub-privileged-cluster-role-binding"
  cmdLoop kubectl create clusterrolebinding tanzu-demo-hub-privileged-cluster-role-binding \
          --clusterrole=vmware-system-tmc-psp-privileged --group=system:authenticated > /dev/null 2>&1

}

cleanKindCluster() {
  cnt=$(kind get clusters 2>/dev/null | wc -l | awk '{ print $1 }') 
  if [ $cnt -gt 0 ]; then
    messageTitle "CleaningUp Kubernetes Kind Clusters"

    for n in $(kind get clusters 2>/dev/null); do
      messagePrint " ▪ Deleting Kind Cluster"               "$n"
      kind delete clusters $n > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failed to delete kind cluster $n"
        echo "       => kind delete clusters $n"
        exit 1
      fi

    done
  fi
}

usage_tdh_demo() {
  echo ""
  echo "USAGE: $0 [options]" 
  echo "                     --native   ## RUN's on Local host instead of the tdh-tools container"
  echo "                     --help     ## Show this information"
  echo ""
  exit 1
}

checkCLIarguments() {
  while [ "$1" != "" ]; do
    case $1 in
      --type)        TDH_TOOLS_TYPE=$2;;
      --version)     TDH_TOOLS_VERS=$2;;
      --context)     TDH_CONTEXT=$2;;
      --usage)       usage_tdh_demo;;
      --help)        usage_tdh_demo;;
      -h)            usage_tdh_demo;;
    esac
    shift
  done
}

#createStorageClaim  registry-harbor redis-data-harbor-redis 100Gi ReadWriteOnce
createStorageClaim() {
  NAMESPACE=$1
  CLAIMNAME=$2
  SIZE=$3
  MODE=$4

  TMP_CLAIN=/tmp/storage_claim.yaml

  echo "apiVersion: v1"                             >  $TMP_CLAIN
  echo "kind: PersistentVolumeClaim"                >> $TMP_CLAIN
  echo "metadata:"                                  >> $TMP_CLAIN
  echo "  labels:"                                  >> $TMP_CLAIN
  echo "    app: redis"                             >> $TMP_CLAIN
  echo "    component: master"                      >> $TMP_CLAIN
  echo "    heritage: Helm"                         >> $TMP_CLAIN
  echo "    release: harbor"                        >> $TMP_CLAIN
  echo "    role: master"                           >> $TMP_CLAIN
  echo "  name: $CLAIMNAME"                         >> $TMP_CLAIN
  echo "spec:"                                      >> $TMP_CLAIN
  echo "  accessModes:"                             >> $TMP_CLAIN
  echo "  - $MODE"                                  >> $TMP_CLAIN
  echo "  resources:   "                            >> $TMP_CLAIN
  echo "    requests:"                              >> $TMP_CLAIN
  echo "      storage: $SIZE"                       >> $TMP_CLAIN
  echo "  storageClassName: standard"               >> $TMP_CLAIN
  echo "  volumeMode: Filesystem"                   >> $TMP_CLAIN

  kubectl create -f $TMP_CLAIN -n $NAMESPACE
}

spaceFiller() {
  FORMAT="%${1}s\\n"

  printf $FORMAT ""
} 

centerText() {
  LENGTH=$1
  TEXT=$2

  cnt=$(echo $TEXT | wc -c | sed 's/ *//g')
  let cnt=cnt-1
  let gap=(LENGTH-cnt)/2
  let rnd=(LENGTH-cnt)%2

  str="$(spaceFiller $gap)${TEXT}$(spaceFiller $gap)$(spaceFiller $rnd)"
  echo "$str"
}

leftText() {
  LENGTH=$1
  TEXT=$2

  str="%-${LENGTH}s"
  printf "$str\n" $TEXT
}

# ------------------------------------------------------------------------------------------
# Function Name ......: tmcAPI_getToken
# Function Purpose ...: Get TMC API Bearer Token
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: TMC Refresh Token
# ------------------------------------------------------------------------------------------
# Return Value .......: API Access Token
# ------------------------------------------------------------------------------------------
# Reference Documentation: 
# https://developer.vmware.com/apis/897/tanzu-mission-control
# ------------------------------------------------------------------------------------------
tmcAPI_getToken() {
  REFRESH_TOKEN="$1"
  VMC_SERVER="https://console.cloud.vmware.com/csp/gateway/am"

  curl -k -X POST "$VMC_SERVER/api/auth/api-tokens/authorize?refresh_token=$TMC_SERVICE_TOKEN" 2>/dev/null | jq -r '.access_token'
}

# ------------------------------------------------------------------------------------------
# Function Name ......: giteaMirrorRepo
# Function Purpose ...: Get Gitea Repo
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: n/a
# ------------------------------------------------------------------------------------------
# Return Value .......: n/a
# ------------------------------------------------------------------------------------------
# Example.............: giteaForkRepo <fork-user> <fork-repo> <org> <name
#                       giteaForkRepo parth-pandit fortune-demo tdh fortune-demo
# ------------------------------------------------------------------------------------------
giteaMirrorRepo() {
  GITEA_REMOTE_REPO=$1
  GITEA_NEW_ORG=$2
  GITEA_NEW_REPO=$3

  TDH_SERVICE_GITEA_ADMIN_USER=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_USER)
  TDH_SERVICE_GITEA_ADMIN_PASS=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_PASS)
  TDH_SERVICE_GITEA_SERVER=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_SERVER)
  TDH_SERVICE_GITEA_SERVER_URL=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_SERVER_URL)
  TDH_SERVICE_GITEA_ADMIN_TOKEN_ID=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_TOKEN_ID)

  curl -X POST ${TDH_SERVICE_GITEA_SERVER_URL}/api/v1/repos/$GITEA_NEW_ORG/$GITEA_NEW_REPO \
       -H 'accept: application/json' 2>/dev/null > /tmp/output.json

  nam=$(jq -r '.full_name' /tmp/output.json) 
  if [ "$nam" == "" -o "$nam" == "null" ]; then
    TMP_API_DATA=/tmp/gitea_api_data.json
    echo "{"                                                    >  $TMP_API_DATA
    echo "  \"clone_addr\": \"$GITEA_REMOTE_REPO\","            >> $TMP_API_DATA
    echo "  \"repo_name\": \"$GITEA_NEW_REPO\","                >> $TMP_API_DATA
    echo "  \"repo_owner\": \"$GITEA_NEW_ORG\","                >> $TMP_API_DATA
    echo "  \"mirror\": true,"                                  >> $TMP_API_DATA
    echo "  \"private\": false"                                 >> $TMP_API_DATA
    echo "}"                                                    >> $TMP_API_DATA

    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      curl -X POST ${TDH_SERVICE_GITEA_SERVER_URL}/api/v1/repos/migrate \
                 -H 'accept: application/json' \
                 -u "$TDH_SERVICE_GITEA_ADMIN_USER:$TDH_SERVICE_GITEA_ADMIN_PASS" \
                 -H 'Content-Type: application/json' \
         --data "@$TMP_API_DATA" >/dev/null 2>&1; ret=$?
      [ $ret -eq 0 ] && break

      let cnt=cnt+1
      sleep 10
    done
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: giteaForkRepo
# Function Purpose ...: Get Gitea Repo
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: n/a
# ------------------------------------------------------------------------------------------
# Return Value .......: n/a
# ------------------------------------------------------------------------------------------
# Example.............: giteaForkRepo <fork-user> <fork-repo> <org> <name
#                       giteaForkRepo parth-pandit fortune-demo tdh fortune-demo
# ------------------------------------------------------------------------------------------
giteaForkRepo() {
  GITEA_FORK_USER=$1
  GITEA_FORK_REPO=$2
  GITEA_REPO=$3
  GITEA_ORG=$4

  TDH_SERVICE_GITEA_ADMIN_USER=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_USER)
  TDH_SERVICE_GITEA_ADMIN_PASS=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_PASS)
  TDH_SERVICE_GITEA_SERVER=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_SERVER)
  TDH_SERVICE_GITEA_SERVER_URL=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_SERVER_URL)
  TDH_SERVICE_GITEA_ADMIN_TOKEN_ID=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_TOKEN_ID)

  curl -X GET ${TDH_SERVICE_GITEA_SERVER_URL}/api/v1/repos/$GITEA_ORG/$GITEA_REPO \
       -H 'accept: application/json' 2>/dev/null > /tmp/output.json
  nam=$(jq -r '.name' /tmp/output.json) 
  if [ "$nam" == "" -o "$nam" == "null" ]; then
    TMP_API_DATA=/tmp/gitea_api_data.json
    echo "{"                                                    >  $TMP_API_DATA
    echo "  \"name\": \"$GITEA_REPO\","                         >> $TMP_API_DATA
    echo "  \"organization\": \"$GITEA_ORG\""                   >> $TMP_API_DATA
    echo "}"                                                    >> $TMP_API_DATA

    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      curl -X POST ${TDH_SERVICE_GITEA_SERVER_URL}/api/v1/repos/$GITEA_FORK_USER/$GITEA_FORK_REPO/forks \
                 -H 'accept: application/json' \
                 -H 'Content-Type: application/json' \
                 -u "$TDH_SERVICE_GITEA_ADMIN_USER:$TDH_SERVICE_GITEA_ADMIN_PASS" \
                 --data "@$TMP_API_DATA" >/dev/null 2>&1; ret=$?
      [ $ret -eq 0 ] && break

      let cnt=cnt+1
      sleep 10
    done
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: createGiteaRepo
# Function Purpose ...: Get Gitea Repo
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: n/a
# ------------------------------------------------------------------------------------------
# Return Value .......: n/a
# ------------------------------------------------------------------------------------------
# Reference Documentation:
# https://developer.vmware.com/apis/897/tanzu-mission-control
# ------------------------------------------------------------------------------------------
createGiteaRepo() {
  GITEA_REPO=$1
  
  TDH_SERVICE_GITEA_ADMIN_USER=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_USER)
  TDH_SERVICE_GITEA_ADMIN_PASS=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_PASS)
  TDH_SERVICE_GITEA_SERVER=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_SERVER)
  TDH_SERVICE_GITEA_SERVER_URL=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_SERVER_URL)
  TDH_SERVICE_GITEA_ADMIN_TOKEN_ID=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_TOKEN_ID)
  
  curl -X 'GET' ${TDH_SERVICE_GITEA_SERVER_URL}/api/v1/orgs \
       -H 'accept: application/json' 2>/dev/null > /tmp/output.json


exit
  nam=$(jq -r --arg key $GITEA_REPO '.[] | select(.username == $key).username' /tmp/output.json)
  if [ "$nam" == "" ]; then 
    TMP_API_DATA=/tmp/gitea_api_data.json                       
    echo "{"                                                    >  $TMP_API_DATA
    echo "  \"description\": \"Tanzu Organisation\","           >> $TMP_API_DATA
    echo "  \"full_name\": \"Tanzu Demo Hub\","                 >> $TMP_API_DATA
    echo "  \"location\": \"\","                                >> $TMP_API_DATA
    echo "  \"repo_admin_change_team_access\": true,"           >> $TMP_API_DATA
    echo "  \"username\": \"tdh\","                             >> $TMP_API_DATA
    echo "  \"visibility\": \"public\","                        >> $TMP_API_DATA
    echo "  \"website\": \"\""                                  >> $TMP_API_DATA
    echo "}"                                                    >> $TMP_API_DATA
    
    cmdLoop curl -X POST ${TDH_SERVICE_GITEA_SERVER_URL}/api/v1/orgs \
         -H 'accept: application/json' \
         -u "$TDH_SERVICE_GITEA_ADMIN_USER:$TDH_SERVICE_GITEA_ADMIN_PASS" \
         -H 'Content-Type: application/json' \
         --data "@$TMP_API_DATA"
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: createGiteaOrg
# Function Purpose ...: Get Gitea Org
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: n/a
# ------------------------------------------------------------------------------------------
# Return Value .......: n/a
# ------------------------------------------------------------------------------------------
# Reference Documentation:
# https://developer.vmware.com/apis/897/tanzu-mission-control
# ------------------------------------------------------------------------------------------
createGiteaOrg() {
  GITEA_REPO=$1
  GITEA_TEXT="$2"

  TDH_SERVICE_GITEA_ADMIN_USER=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_USER)
  TDH_SERVICE_GITEA_ADMIN_PASS=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_PASS)
  TDH_SERVICE_GITEA_SERVER=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_SERVER)
  TDH_SERVICE_GITEA_SERVER_URL=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_SERVER_URL)
  TDH_SERVICE_GITEA_ADMIN_TOKEN_ID=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_TOKEN_ID)

  curl -X 'GET' ${TDH_SERVICE_GITEA_SERVER_URL}/api/v1/orgs \
       -H 'accept: application/json' 2>/dev/null > /tmp/output.json
  nam=$(jq -r --arg key $GITEA_REPO '.[] | select(.username == $key).username' /tmp/output.json) 
  if [ "$nam" == "" ]; then 
    TMP_API_DATA=/tmp/gitea_api_data.json
    echo "{"                                                    >  $TMP_API_DATA
    echo "  \"description\": \"$GITEA_TEXT\","                  >> $TMP_API_DATA
    echo "  \"location\": \"\","                                >> $TMP_API_DATA
    echo "  \"repo_admin_change_team_access\": true,"           >> $TMP_API_DATA
    echo "  \"username\": \"$GITEA_REPO\","                     >> $TMP_API_DATA
    echo "  \"visibility\": \"public\","                        >> $TMP_API_DATA
    echo "  \"website\": \"\""                                  >> $TMP_API_DATA
    echo "}"                                                    >> $TMP_API_DATA

    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
    curl -X POST ${TDH_SERVICE_GITEA_SERVER_URL}/api/v1/orgs \
         -H 'accept: application/json' \
         -u "$TDH_SERVICE_GITEA_ADMIN_USER:$TDH_SERVICE_GITEA_ADMIN_PASS" \
         -H 'Content-Type: application/json' \
         --data "@$TMP_API_DATA" >/dev/null 2>&1; ret=$?
      [ $ret -eq 0 ] && break
      
      let cnt=cnt+1
      sleep 10
    done
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: createGiteaAPItoken
# Function Purpose ...: Get Gitea API Bearer Token
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: n/a
# ------------------------------------------------------------------------------------------
# Return Value .......: n/a
# ------------------------------------------------------------------------------------------
# Reference Documentation:
# https://developer.vmware.com/apis/897/tanzu-mission-control
# ------------------------------------------------------------------------------------------
createGiteaAPItoken() {
  TDH_SERVICE_GITEA_ADMIN_USER=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_USER)
  TDH_SERVICE_GITEA_ADMIN_PASS=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_PASS)
  TDH_SERVICE_GITEA_SERVER=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_SERVER)
  TDH_SERVICE_GITEA_SERVER_URL=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_SERVER_URL)
  TDH_SERVICE_GITEA_ADMIN_TOKEN_ID=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_TOKEN_ID)

  if [ "$TDH_SERVICE_GITEA_ADMIN_TOKEN_ID" == "" -o "$TDH_SERVICE_GITEA_ADMIN_TOKEN_ID" == "null" ]; then 
    token=$(curl -X GET https://${TDH_SERVICE_GITEA_SERVER}/api/v1/users/$TDH_SERVICE_GITEA_ADMIN_USER/tokens \
                 -H 'accept: application/json' \
                 -H 'Content-Type: application/json' \
                 -u "$TDH_SERVICE_GITEA_ADMIN_USER:$TDH_SERVICE_GITEA_ADMIN_PASS" 2>/dev/null | \
                 jq -r '.[] | select(.name == "tanzu-demo-hub").name') 

    if [ "$token" == "tanzu-demo-hub" ]; then
      curl -X DELETE https://${TDH_SERVICE_GITEA_SERVER}/api/v1/users/$TDH_SERVICE_GITEA_ADMIN_USER/tokens/tanzu-demo-hub \
           -H 'accept: application/json' \
           -u "$TDH_SERVICE_GITEA_ADMIN_USER:$TDH_SERVICE_GITEA_ADMIN_PASS" >/dev/null 2>&1
    fi

    token=$(curl -X POST ${TDH_SERVICE_GITEA_SERVER_URL}/api/v1/users/sacha/tokens \
                 -H 'accept: application/json' \
                 -H 'Content-Type: application/json' \
                 -u "$TDH_SERVICE_GITEA_ADMIN_USER:$TDH_SERVICE_GITEA_ADMIN_PASS" \
                 -d '{ "name": "tanzu-demo-hub" }' 2>/dev/null | jq -r '.sha1') 

    uodateConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_TOKEN_ID  "$token"
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: getGiteaAPItoken
# Function Purpose ...: Get Gitea API Bearer Token
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: TMC Refresh Token
# ------------------------------------------------------------------------------------------
# Return Value .......: API Access Token
# ------------------------------------------------------------------------------------------
# Reference Documentation:
# https://developer.vmware.com/apis/897/tanzu-mission-control
# ------------------------------------------------------------------------------------------
getGiteaAPItoken() {
  REFRESH_TOKEN="$1"

  TDH_SERVICE_GITEA_ADMIN_USER=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_USER)
  TDH_SERVICE_GITEA_ADMIN_PASS=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_ADMIN_PASS)
  TDH_SERVICE_GITEA_SERVER=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_SERVER)
  TDH_SERVICE_GITEA_SERVER_URL=$(getConfigMap tanzu-demo-hub TDH_SERVICE_GITEA_SERVER_URL)

echo "curl -XPOST -H "Content-Type: application/json"  -k -d '{"name":"test"}' -u \"$TDH_SERVICE_GITEA_ADMIN_USER:$TDH_SERVICE_GITEA_ADMIN_PASS\" $TDH_SERVICE_GITEA_SERVER_URL"
  curl -XPOST -H "Content-Type: application/json"  -k -d '{"name":"test"}' \
       -u "$TDH_SERVICE_GITEA_ADMIN_USER:$TDH_SERVICE_GITEA_ADMIN_PASS" $TDH_SERVICE_GITEA_SERVER_URL 

  #curl -k -X POST "$VMC_SERVER/api/auth/api-tokens/authorize?refresh_token=$TMC_SERVICE_TOKEN" 2>/dev/null | jq -r '.access_token'
}


#curl -XPOST -H "Content-Type: application/json"  -k -d '{"name":"test"}' -u username:password https://gitea.your.host/api/v1/users/<username>/tokens
#{"id":1,"name":"test","sha1":"9fcb1158165773dd010fca5f0cf7174316c3e37d","token_last_eight":"16c3e37d"}

# ------------------------------------------------------------------------------------------
# Function Name ......: tmcAPI_getCliBinary
# Function Purpose ...: Get TMC CLI Binary 
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: TMC BEARER Token 
#          ($2) ......: TMC Server URL
# ------------------------------------------------------------------------------------------
# Return Value .......: API Access Token
# ------------------------------------------------------------------------------------------
# darwinX86 .........: MacOS x86
# darwinX64 .........: MacOS x86_64
# linuxX86 ........ .: Linux x86
# linuxX64 ..........: Linux x86_64
# windowsX86 ........: Windows x86
# windowsX64 ........: Windows x86_64
# ------------------------------------------------------------------------------------------
# Reference Documentation: 
# https://developer.vmware.com/apis/897/tanzu-mission-control
# ------------------------------------------------------------------------------------------
tmcAPI_getCliBinary() {
  REFRESH_TOKEN="$1"
  TMC_SERVER="$2"
  TMC_TYPE="$3"

  latest=$(curl -k -X GET -H "Authorization: Bearer $REFRESH_TOKEN" \
                          -H 'Content-Type: application/x-www-form-urlencoded' \
                          ${TMC_SERVER}/v1alpha/system/binaries 2>/dev/null | jq -r '.latestVersion')

  curl -k -X GET -H "Authdrization: Bearer $REFRESH_TOKEN" \
                 -H 'Content-Type: application/x-www-form-urlencoded' \
                 ${TMC_SERVER}/v1alpha/system/binaries 2>/dev/null | jq -r ".versions.\"$latest\".$TMC_TYPE"
}


# ------------------------------------------------------------------------------------------
# Function Name ......: vSphereAPI_getToken
# Function Purpose ...: Create a vSphere Namespace
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: vSphere API Token
#          ($2) ......: vSphere Server Name
#          ($3) ......: vSphere Namespace name
# ------------------------------------------------------------------------------------------
# Return Value .......: API Access Token
# ------------------------------------------------------------------------------------------
vSphereAPI_getToken() {
  VCENTER_SERVER="$1"
  VCENTER_USER="$2"
  VCENTER_PASS="$3"
  USERPASS="${VCENTER_USER}:${VCENTER_PASS}"

  curl -k -X POST "${VCENTER_SERVER}/rest/com/vmware/cis/session" -u "$USERPASS" 2>/dev/null | jq -r '.value'

}


# ------------------------------------------------------------------------------------------
# Function Name ......: vSphereAPI_getToken
# Function Purpose ...: Create a vSphere Namespace
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: vSphere API Token
#          ($2) ......: vSphere Server Name
#          ($3) ......: vSphere Namespace name
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 on Success, 1 on failure
# ------------------------------------------------------------------------------------------
vSphereAPI_getToken() {
  VCENTER_SERVER="$1"
  VCENTER_USER="$2"
  VCENTER_PASS="$3"
  USERPASS="${VCENTER_USER}:${VCENTER_PASS}"

  curl -k -X POST "${VCENTER_SERVER}/rest/com/vmware/cis/session" -u "$USERPASS" 2>/dev/null | jq -r '.value'

}

# ------------------------------------------------------------------------------------------
# Function Name ......: vSphereAPI_createNamespace
# Function Purpose ...: Create a vSphere Namespace
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: vSphere API Token
#          ($2) ......: vSphere Server Name
#          ($3) ......: vSphere Namespace name
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 on Success, 1 on failure
# ------------------------------------------------------------------------------------------
vSphereAPI_createNamespace() {
  API_TOKEN=$1
  VCENTER_SERVER=$2
  NAMESPACE=$3

  # --- CREATE VSPHERE NAMESPACE ---
  stt=$(curl -k -X GET -H "vmware-api-session-id: $API_TOKEN" ${VCENTER_SERVER}/api/vcenter/namespaces/instances/$NAMESPACE 2>/dev/null | jq -r '.cluster')
  if [ "$stt" == "" -o "$stt" == "null" ]; then
    messageTitle "Create vSphere Namespace for Tanzu Demo Hub Environment"
    messagePrint " ▪ vSphere Server"     "$VCENTER_SERVER"
    messagePrint " ▪ vSphere Namespace"  "$NAMESPACE"

    vs_cluster=$(curl -k -X GET -H "vmware-api-session-id: $API_TOKEN" ${VCENTER_SERVER}/api/vcenter/namespace-management/clusters 2>/dev/null | jq -r '.[].cluster')
    VSPHERE_CONFIG=/tmp/vsphere_config.json; rm -f $OKTA_CONFIG
    echo '{'                                                                         >  $VSPHERE_CONFIG
    echo "  \"cluster\": \"$vs_cluster\","                                           >> $VSPHERE_CONFIG
    echo "  \"namespace\": \"$NAMESPACE\""                                           >> $VSPHERE_CONFIG
    echo '}'                                                                         >> $VSPHERE_CONFIG

    curl -k -X POST -H "vmware-api-session-id: $API_TOKEN" ${VCENTER_SERVER}/api/vcenter/namespaces/instances \
         -H 'Content-type: application/json' -d "@$VSPHERE_CONFIG" > /tmp/error.log 2>&1; ret=$?
    if [ $ret -ne 0 ]; then
      logMessages /tmp/error.log
      logMessages $VSPHERE_CONFIG
      echo "ERROR: failed to set vSphere Namespace $NAMESPACE"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ curl -k -X POST -H \"vmware-api-session-id: $API_TOKEN\" ${VCENTER_SERVER}/api/vcenter/namespaces/instances \\"
        echo "                            -H 'Content-type: application/json' -d \"@$VSPHERE_CONFIG\""
        echo "          tdh-tools:/$ exit"
      else
        echo "       => curl -k -X POST -H \"vmware-api-session-id: $API_TOKEN\" ${VCENTER_SERVER}/api/vcenter/namespaces/instances \\"
        echo "               -H 'Content-type: application/json' -d \"@$VSPHERE_CONFIG\""
      fi

      exit 1
    fi
  else
    messageTitle "Verify vSphere Namespace for Tanzu Demo Hub Environment"
    messagePrint " ▪ vSphere Server"     "$VCENTER_SERVER"
    messagePrint " ▪ vSphere Namespace"  "$NAMESPACE"
  fi

  # --- GET STORAGE POLICY ID FOR 'TANZU' ---
  vm_policy=$(curl -k -X GET -H 'Content-type: application/json' \
                   -H "vmware-api-session-id: $API_TOKEN" ${VCENTER_SERVER}/rest/vcenter/storage/policies 2>/dev/null | \
                   jq -r '.value[] | select(.name == "tanzu" ).policy')

  # --- UPDATE VSPHERE NAMESPACE CONFIG ---
  VSPHERE_CONFIG=/tmp/vsphere_config.json; rm -f $OKTA_CONFIG
  echo '{'                                                                         >  $VSPHERE_CONFIG
  echo '  "description": "Tanzu Demo Hub - Demo Namespace 3",'                     >> $VSPHERE_CONFIG
  echo '  "vm_service_spec": {'                                                    >> $VSPHERE_CONFIG
  echo '    "vm_classes": ['                                                       >> $VSPHERE_CONFIG
  echo '      "best-effort-medium",'                                               >> $VSPHERE_CONFIG
  echo '      "best-effort-large",'                                                >> $VSPHERE_CONFIG
  echo '      "best-effort-xlarge",'                                               >> $VSPHERE_CONFIG
  echo '      "best-effort-2xlarge",'                                              >> $VSPHERE_CONFIG
  echo '      "guaranteed-medium",'                                                >> $VSPHERE_CONFIG
  echo '      "guaranteed-large",'                                                 >> $VSPHERE_CONFIG
  echo '      "guaranteed-xlarge",'                                                 >> $VSPHERE_CONFIG
  echo '      "guaranteed-2xlarge"'                                                >> $VSPHERE_CONFIG
  echo '    ]'                                                                     >> $VSPHERE_CONFIG
  echo '  },'                                                                      >> $VSPHERE_CONFIG
  echo '  "storage_specs": ['                                                      >> $VSPHERE_CONFIG
  echo '    {'                                                                     >> $VSPHERE_CONFIG
  echo "      \"policy\": \"$vm_policy\""                                          >> $VSPHERE_CONFIG
  echo '    }'                                                                     >> $VSPHERE_CONFIG
  echo '  ]'                                                                       >> $VSPHERE_CONFIG
  echo '}'                                                                         >> $VSPHERE_CONFIG

  curl -k -X PATCH -H 'Content-type: application/json' \
                   -H "vmware-api-session-id: $API_TOKEN" ${VCENTER_SERVER}/api/vcenter/namespaces/instances/$NAMESPACE \
                   -d "@$VSPHERE_CONFIG" > /tmp/error.log 2>&1; ret=$?
  if [ $ret -ne 0 ]; then
    logMessages /tmp/error.log
    logMessages $VSPHERE_CONFIG
    echo "ERROR: failed to Patch vSphere Namespace $NAMESPACE"
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ curl -k -X PATH -H \"vmware-api-session-id: $API_TOKEN\" ${VCENTER_SERVER}/api/vcenter/namespaces/instances/$NAMESPACE \\"
      echo "                            -H 'Content-type: application/json' -d \"@$VSPHERE_CONFIG\""
      echo "          tdh-tools:/$ exit"
    else
      echo "       => curl -k -X PATH -H \"vmware-api-session-id: $API_TOKEN\" ${VCENTER_SERVER}/api/vcenter/namespaces/instances/$NAMESPACE \\"
      echo "               -H 'Content-type: application/json' -d \"@$VSPHERE_CONFIG\""
    fi

    exit 1
  fi


}


oktaAPIdata() {
  curl --location --request $1 "$2" \
    --header 'Accept: application/json' \
    --header 'Content-Type: application/json' \
    --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" \
    --data "@$3" 2>/dev/null
}

oktaAPI() {
  MODE=$1

  if [ "$MODE" == "app_delete" ]; then 
    curl --location -g --request DELETE "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/apps/$2" \
    --header 'Accept: application/json' \
    --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" 
  fi

  if [ "$MODE" == "create_AuthServerClaim" ]; then
    curl --location --request POST "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/authorizationServers/$3//claims" \
      --header 'Accept: application/json' \
      --header 'Content-Type: application/json' \
      --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" \
      --data "@$2"
  fi

  if [ "$MODE" == "create_AuthServerId" ]; then
    curl --location --request POST "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/authorizationServers" \
      --header 'Accept: application/json' \
      --header 'Content-Type: application/json' \
      --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" \
      --data "@$2"
  fi

  if [ "$MODE" == "create_group" ]; then
    curl --location --request POST "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/groups" \
      --header 'Accept: application/json' \
      --header 'Content-Type: application/json' \
      --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" \
      --data "@$2"
  fi

  if [ "$MODE" == "app_add_user" ]; then
    APPID=$2

    curl --location -g --request POST "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/apps/$APPID/users" \
      --header 'Accept: application/json' \
      --header 'Content-Type: application/json' \
      --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" \
      --data "@$3" 
  fi

  if [ "$MODE" == "app_add_group" ]; then
    APPID="$2"
    GROUPID="$3"

    curl --location -g --request PUT "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/apps/$APPID/groups/$GROUPID" \
      --header 'Accept: application/json' \
      --header 'Content-Type: application/json' \
      --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" --data-raw ''
  fi

  if [ "$MODE" == "app_assigned_groups" ]; then
    APPID=$2

    curl --location -g --request GET "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/apps/$APPID/groups" \
      --header 'Accept: application/json' \
      --header 'Content-Type: application/json' \
      --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" 2>/dev/null
  fi

  if [ "$MODE" == "group_add_user" ]; then
    groupId=$2
    userId=$3

    curl --location -g --request PUT "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/groups/$groupId/users/$userId" \
      --header 'Accept: application/json' \
      --header 'Content-Type: application/json' \
      --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" \
      --data-raw ''
  fi

  if [ "$MODE" == "group_users" ]; then
    curl --location --request GET "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/users/$2/groups" \
      --header 'Accept: application/json' \
      --header 'Content-Type: application/json' \
      --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" 2>/dev/null
  fi

  if [ "$MODE" == "app_users" ]; then
    curl --location --request GET "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/apps/$2/users" \
      --header 'Accept: application/json' \
      --header 'Content-Type: application/json' \
      --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" 2>/dev/null
  fi

  if [ "$MODE" == "groups" ]; then
    curl --location --request GET "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/groups" \
      --header 'Accept: application/json' \
      --header 'Content-Type: application/json' \
      --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" 2>/dev/null 
  fi

  if [ "$MODE" == "getOauthScope" ]; then
echo "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/authorizationServers/$2/claims" 1>&2
    curl --location --request GET "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/authorizationServers/$2/claims" \
      --header 'Accept: application/json' \
      --header 'Content-Type: application/json' \
      --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" 2>/dev/null
  fi

  if [ "$MODE" == "authorizationServers" ]; then
    curl --location --request GET "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/authorizationServers" \
      --header 'Accept: application/json' \
      --header 'Content-Type: application/json' \
      --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" 2>/dev/null
  fi


  if [ "$MODE" == "users" ]; then
    curl --location --request GET "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/users?limit=25" \
      --header 'Accept: application/json' \
      --header 'Content-Type: application/json' \
      --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" 2>/dev/null 
  fi

  # --- TEST LOGIN ---
  if [ "$MODE" == "login" -o "$MODE" == "verify" ]; then 
    curl --location --request GET "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/users?limit=25" \
      --header 'Accept: application/json' \
      --header 'Content-Type: application/json' \
      --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" 2>/dev/null  > /tmp/okta_status.yaml

    err=$(jq -r '.errorCode' /tmp/okta_status.yaml  2>/dev/null) 
    msg=$(jq -r '.errorSummary' /tmp/okta_status.yaml  2>/dev/null) 

    if [ "$err" != "" ]; then 
      echo "TDH_OKTA_USER:      $TDH_OKTA_USER"
      echo "TDH_OKTA_DOMAIN:    $TDH_OKTA_DOMAIN"
      echo "TDH_OKTA_API_TOKEN: $TDH_OKTA_API_TOKEN"
      
      echo 
      echo "ERROR: Failed to login to Okta, please verify the credentials"
      echo "       Create API Token => Okta Dahshboard -> Security -> API -> Create Token"

      messageLine
      jq -r '.' /tmp/okta_status.yaml
      messageLine
      
      rm -f /tmp/okta_status.yaml 
      exit 1
    fi
  fi

  if [ "$MODE" == "deactivate" ]; then
    curl --location -g --request POST "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/apps/$2/lifecycle/deactivate" \
    --header 'Accept: application/json' \
    --header 'Content-Type: application/json' \
    --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" --data-raw '' >/dev/null 2>&1
  fi

  if [ "$MODE" == "active_apps" ]; then
    curl --location --request GET "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/apps?filter=status eq \"ACTIVE\"" \
    --header 'Accept: application/json' \
    --header 'Content-Type: application/json' \
    --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" 2>/dev/null
  fi

  # --- TEST LOGIN ---
  if [ "$MODE" == "apps" -o "$MODE" == "apps" ]; then 
    curl --location --request GET "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/apps?limit=25" \
    --header 'Accept: application/json' \
    --header 'Content-Type: application/json' \
    --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" 2>/dev/null 
  fi

  if [ "$MODE" == "create_app" ]; then 
    curl --location --request POST "https://${TDH_OKTA_DOMAIN}.okta.com/api/v1/apps" \
    --header 'Accept: application/json' \
    --header 'Content-Type: application/json' \
    --header "Authorization: SSWS $TDH_OKTA_API_TOKEN" \
    --data-raw '{
        "name": "oidc_client",
        "label": "Sample Client2",
        "signOnMode": "OPENID_CONNECT",
        "credentials": {
          "oauthClient": {
            "token_endpoint_auth_method": "client_secret_post"
          }
        },
        "settings": {
          "oauthClient": {
            "client_uri": "http://localhost:8080",
            "logo_uri": "http://developer.okta.com/assets/images/logo-new.png",
            "redirect_uris": [
              "https://example.com/oauth2/callback",
              "myapp://callback"
            ],
            "response_types": [
              "code"
            ],
            "grant_types": [
              "authorization_code"
            ],
            "application_type": "web"
          }
        }
      }'
  fi
}

checkConnectivity() {
  curl http://ipv4.icanhazip.com > /dev/null 2>&1; ret=$?

  if [ $ret -ne 0 ]; then
    echo "ERROR: The site http://ipv4.icanhazip.com can not be reached which is required for terraforms"
    echo "       Most likely the reason is that GlobalVPN is active. Please deactivate it and try again"
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ curl http://ipv4.icanhazip.com"
      echo "          tdh-tools:/$ exit"
    else
      echo "       => curl http://ipv4.icanhazip.com"
    fi

    exit 1
  fi
}

#kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.15.1/cert-manager.crds.yaml
#kubectl -n cert-manager apply -f /tmp/letsencrypt-staging

# ------------------------------------------------------------------------------------------
# Function Name ......: installTanzuPackages
# Function Purpose ...: Install Tanzu Packages on TKG Workload Cluster
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Tanzi Package Display Name
#          ($2) ......: Namespace Name
#          ($3) ......: Tanzu Package Name
#          ($4) ......: Package Description
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 on Success, 1 on failure
# ------------------------------------------------------------------------------------------
installTanzuPackages() {
  PACKAGE_NAME=$1
  PACKAGE_NAMESPACE=tanzu-system-packages
  NAMESPACE=$2
  PACKAGE_ID=$3
  PACKAGE_DES=$4

  pkg=$(tanzu package installed get $PACKAGE_NAME -n $NAMESPACE -o json 2>/dev/null | jq -r '.[].name')
  if [ "$pkg" != "$PACKAGE_NAME" ]; then 
    package_ver=$(tanzu package available list $PACKAGE_ID -A -o json | jq -r '.[]."version"')
    package_rel=$(tanzu package available list $PACKAGE_ID -A -o json | jq -r '.[]."released-at"')

    messageTitle "Installing $PACKAGE_DES"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME)"   "$PACKAGE_ID ($package_ver/$package_rel)"
    tanzu package install $PACKAGE_NAME --package-name $PACKAGE_ID --namespace $NAMESPACE --version $package_ver --create-namespace > /dev/null 2>&1; ret=$?
    if [ $ret -ne 0 ]; then
      echo "ERROR: failed to install Tanzu Package $PACKAGE_NAME)"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ tanzu package install $PACKAGE_NAME --package-name $PACKAGE_ID --namespace $NAMESPACE --version $package_ver --create-namespace"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => tanzu package install $PACKAGE_NAME --package-name $PACKAGE_ID --namespace $NAMESPACE --version $package_ver --create-namespace"
      fi
  
      exit 1
    fi
  fi

  tanzu package installed get $PACKAGE_NAME -n $NAMESPACE -o json > /tmp/pkg.json
  pkg_stt=$(jq -r '.[]."status"' /tmp/pkg.json)
  pkg_nam=$(jq -r '.[]."package-name"' /tmp/pkg.json)
  pkg_ver=$(jq -r '.[]."package-version"' /tmp/pkg.json)

  messageTitle "Verify $PACKAGE_DES"
  messagePrint " ▪ Tanzu Package ($PACKAGE_NAME)"               "$pkg_nam"
  messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) Namespace"     "$NAMESPACE"
  messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) Version"       "$pkg_ver"
  messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) Status"        "$pkg_stt"
}

# ------------------------------------------------------------------------------------------
# Function Name ......: TanzuPackage_AddRep
# Function Purpose ...: Add Tanzu Package Repositories
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Tanzu Package Namespace Name
#          ($2) ......: Tanzu Package Repo
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 on Success, 1 on failure
# ------------------------------------------------------------------------------------------
TanzuPackage_AddRepo() {
  NAMESPACE=$1
  TANZU_REPO_NAME=$2
  TANZU_REPO_URL=$3

  stt=$(tanzu package repository list -n tanzu-package-repo-global -o json 2>/dev/null | \
        jq -r --arg pkg tce-repo '.[] | select(.name == $pkg).status' 2>/dev/null) 
  if [ "$stt" == "" ]; then 
    messagePrint " ▪ Adding Tanzu Package Repository"   "$TANZU_REPO_URL"
    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      tanzu package repository add $TANZU_REPO_NAME --url $TANZU_REPO_URL \
           --namespace $NAMESPACE > /tmp/error.log 2>&1; ret=$?

      [ $ret -eq 0 ] && break
      let cnt=cnt+1
      sleep 30
    done

    if [ $ret -ne 0 ]; then
      logMessages /tmp/error.log
      echo "ERROR: failed to reconsilte Tanzu Packages in $NAMESPACE"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ tanzu tanzu package repository add $TANZU_REPO_NAME --url $TANZU_REPO_URL --namespace $NAMESPACE"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => tanzu tanzu package repository add $TANZU_REPO_NAME --url $TANZU_REPO_URL --namespace $NAMESPACE"
      fi
    fi
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: TanzuPackage_RepoSync
# Function Purpose ...: Wait for Tanzu Package Repository to be ssynced
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Namespace Name
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 on Success, 1 on failure
# ------------------------------------------------------------------------------------------
TanzuPackage_RepoSync() {
  PACKAGE_REPOSITORY=$1
  PACKAGE_NAMESPACE=tanzu-system-packages

  # --- CREATE PACKAGE_NAMESPACE NAMESPACE ---
  createNamespace $PACKAGE_NAMESPACE > /dev/null 2>&1
  createNamespace $PACKAGE_REPOSITORY > /dev/null 2>&1

  messagePrint " ▪ Wait for Tanzu Package Reconsile"   "$PACKAGE_REPOSITORY"

  ret=""; cnt=0
  while [ "$ret" != "Reconcile succeeded" -a $cnt -lt 10 ]; do
    ret=$(tanzu package repository list -n $PACKAGE_REPOSITORY -o json 2>/dev/null | jq -r '.[] | select(.status != "Reconcile succeeded").status' | wc -l) 

    [ $ret -eq 0 ] && break
    sleep 60
    let cnt=cnt+1
  done

  if [ $ret -gt 0 ]; then
    echo "ERROR: failed to reconsilte Tanzu Packages in $PACKAGE_REPOSITORY"
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ tanzu package repository list -n $PACKAGE_REPOSITORY"
      echo "          tdh-tools:/$ exit"
    else
      echo "       => tanzu package repository list -n $PACKAGE_REPOSITORY"
    fi

    exit 1
#  else
#    messageLine
#    tanzu package repository list -A
#    messageLine
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: TanzuPackage_LocalStorage
# Function Purpose ...: Install Cert Manager
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Tanzu Package Name
#          ($2) ......: Tanzu Package Id
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 on Success, 1 on failure
# ------------------------------------------------------------------------------------------
# https://tanzucommunityedition.io/docs/latest/package-readme-local-path-storage-0.0.20/
# ------------------------------------------------------------------------------------------
TanzuPackage_LocalStorage() {
  PACKAGE_NAME="$1"
  NAMESPACE="$1"
  PACKAGE_NAMESPACE=tanzu-system-packages
  PACKAGE_ID="$2"
  PACKAGE_DES="Local Storage (RWO AccessMode)"

  cmdLoop tanzu package installed list -A -o json > /tmp/output.json 2>/dev/null
  pkg=$(jq -r --arg key "$PACKAGE_NAME" '.[] | select(.name == $key).name' /tmp/output.json)
  if [ "$pkg" != "$PACKAGE_NAME" ]; then
    # --- DELETE SERVICE ACCOUNT IF IT EXIST ----
    cmdLoop kubectl get serviceaccount -n tanzu-system-packages -o json > /tmp/output.json
    nam=$(jq -r --arg key "${PACKAGE_NAME}-${PACKAGE_NAMESPACE}-sa" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
    [ "$nam" != "" ] && cmdLoop kubectl -n tanzu-system-packages delete serviceaccount $nam

    cmdLoop tanzu package available list $PACKAGE_ID -A -o json > /tmp/output.json
    if [ -s /tmp/output.json ]; then 
      package_ver=$(jq -r '.[]."version"' /tmp/output.json | tail -1)
      package_rel=$(jq -r '.[]."released-at"' /tmp/output.json | tail -1)
    else
      package_ver=""; package_rel=""
    fi

    messageTitle "Installing $PACKAGE_DES"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME)"   "$PACKAGE_ID ($package_ver/$package_rel)"
    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      # --- DELETE SERVICE ACCOUNT IF IT EXIST ----
      cmdLoop kubectl get serviceaccount -n tanzu-system-packages -o json > /tmp/output.json
      nam=$(jq -r --arg key "${PACKAGE_NAME}-${PACKAGE_NAMESPACE}-sa" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
      [ "$nam" != "" ] && cmdLoop kubectl -n tanzu-system-packages delete serviceaccount $nam

      # --- DELETE SERVICE ACCOUNT IF IT EXIST ----
      cmdLoop kubectl get clusterroles -n tanzu-system-packages -o json > /tmp/output.json
      nam=$(jq -r --arg key "${PACKAGE_NAME}-${PACKAGE_NAMESPACE}-role" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
      [ "$nam" != "" ] && cmdLoop kubectl -n tanzu-system-packages delete clusterrole $nam

      # --- DELETE SERVICE ACCOUNT IF IT EXIST ----
      cmdLoop kubectl get clusterrolebinding -n tanzu-system-packages -o json > /tmp/output.json
      nam=$(jq -r --arg key "${PACKAGE_NAME}-${PACKAGE_NAMESPACE}-cluster-rolebinding" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
      [ "$nam" != "" ] && cmdLoop kubectl -n tanzu-system-packages delete clusterrolebinding $nam

#bbbbbbb1
      tanzu package install $PACKAGE_NAME --package-name $PACKAGE_ID --namespace $PACKAGE_NAMESPACE --version $package_ver --create-namespace --wait=true > /tmp/error.log 2>&1
      waitTanzuPackageReconsile $PACKAGE_NAME $PACKAGE_NAMESPACE; ret=$? 
      if [ $ret -ne 0 ]; then 
        # --- DELETE PACKAGE AND START OVER ---
        deleteTanzuPackage $PACKAGE_NAME $PACKAGE_NAMESPACE > /dev/null 2>&1
      else
        break
      fi

      let cnt=cnt+1
      sleep 30
    done

    if [ $ret -ne 0 ]; then
      logMessages /tmp/error.log
      echo "ERROR: failed to install Tanzu Package $PACKAGE_NAME)"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ tanzu package install $PACKAGE_NAME --package-name $PACKAGE_ID --namespace $NAMESPACE --version $package_ver --create-namespace"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => tanzu package install $PACKAGE_NAME --package-name $PACKAGE_ID --namespace $NAMESPACE --version $package_ver --create-namespace"
      fi

      exit 1
    fi
  fi

  cmdLoop tanzu package installed list -A -o json > /tmp/output.json 2> /dev/null
  stt=$(jq -r --arg key "$PACKAGE_NAME" '.[] | select(.name == $key).name' /tmp/output.json)
  if [ "$stt" == "$PACKAGE_NAME" ]; then 
    cmdLoop tanzu package installed get $PACKAGE_NAME -n $PACKAGE_NAMESPACE -o json > /tmp/pkg.json
    pkg_stt=$(jq -r '.[]."status"' /tmp/pkg.json)
    pkg_nam=$(jq -r '.[]."package-name"' /tmp/pkg.json)
    pkg_ver=$(jq -r '.[]."package-version"' /tmp/pkg.json)

    messageTitle "Verify $PACKAGE_DES"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME)"                 "$pkg_nam"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) Namespace"       "$NAMESPACE"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) Version"         "$pkg_ver"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) Status"          "$pkg_stt"
  else
    echo "ERROR: The Tanzu Package: $PACKAGE_NAME has not been installed"
    exit 1
  fi

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_ENABLED          "true"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_NAMESPACE        "$NAMESPACE"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_PACKAGE_NAME     "$pkg_nam"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_PACKAGE_VERSION  "$pkg_ver"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGERPACKAGE__STATUS   "$pkg_stt"
}

# ------------------------------------------------------------------------------------------
# Function Name ......: TanzuPackage_CertManager
# Function Purpose ...: Install Cert Manager
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Tanzi Package Display Name
#          ($2) ......: Namespace Name
#          ($3) ......: Tanzu Package Name
#          ($4) ......: Package Description
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 on Success, 1 on failure
# ------------------------------------------------------------------------------------------
# https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.4/vmware-tanzu-kubernetes-grid-14/GUID-packages-cert-manager.html
# ------------------------------------------------------------------------------------------
TanzuPackage_CertManager() {
  PACKAGE_NAME=$1
  PACKAGE_NAMESPACE=tanzu-system-packages
  NAMESPACE=tanzu-system-ingress 
  PACKAGE_ID=$2
  PACKAGE_DES="Certificate management (cert-manager)"

  cmdLoop tanzu package installed list -A -o json > /tmp/output.json 2>/dev/null
  pkg=$(jq -r --arg key "$PACKAGE_NAME" '.[] | select(.name == $key).name' /tmp/output.json)
  if [ "$pkg" != "$PACKAGE_NAME" ]; then
    cmdLoop tanzu package available list $PACKAGE_ID -A -o json > /tmp/output.json
    if [ -s /tmp/output.json ]; then 
      package_ver=$(jq -r '.[]."version"' /tmp/output.json | tail -1)
      package_rel=$(jq -r '.[]."released-at"' /tmp/output.json | tail -1)
    else
      package_ver=""; package_rel=""
    fi

    messageTitle "Installing $PACKAGE_DES in $PACKAGE_NAMESPACE"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME)"   "$PACKAGE_ID ($package_ver/$package_rel)"

#bbbb9
    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      # --- DELETE SERVICE ACCOUNT IF IT EXIST ----
      cmdLoop kubectl get serviceaccount -n tanzu-system-packages -o json > /tmp/output.json
      nam=$(jq -r --arg key "${PACKAGE_NAME}-${PACKAGE_NAMESPACE}-sa" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
      [ "$nam" != "" ] && cmdLoop kubectl -n tanzu-system-packages delete serviceaccount $nam

      # --- DELETE SERVICE ACCOUNT IF IT EXIST ----
      cmdLoop kubectl get clusterroles -n tanzu-system-packages -o json > /tmp/output.json
      nam=$(jq -r --arg key "${PACKAGE_NAME}-${PACKAGE_NAMESPACE}-role" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
      [ "$nam" != "" ] && cmdLoop kubectl -n tanzu-system-packages delete clusterrole $nam

      # --- DELETE SERVICE ACCOUNT IF IT EXIST ----
      cmdLoop kubectl get clusterrolebinding -n tanzu-system-packages -o json > /tmp/output.json
      nam=$(jq -r --arg key "${PACKAGE_NAME}-${PACKAGE_NAMESPACE}-cluster-rolebinding" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
      [ "$nam" != "" ] && cmdLoop kubectl -n tanzu-system-packages delete clusterrolebinding $nam

echo "tanzu package install $PACKAGE_NAME --package-name $PACKAGE_ID --namespace $PACKAGE_NAMESPACE --version $package_ver --create-namespace --poll-timeout 10m --wait=true"
      tanzu package install $PACKAGE_NAME --package-name $PACKAGE_ID --namespace $PACKAGE_NAMESPACE --version $package_ver \
            --create-namespace --poll-timeout 10m --wait=true > /tmp/error.log 2>&1
      waitTanzuPackageReconsile $PACKAGE_NAME $PACKAGE_NAMESPACE; ret=$?
      if [ $ret -ne 0 ]; then
        # --- DELETE PACKAGE AND START OVER ---
        deleteTanzuPackage $PACKAGE_NAME $PACKAGE_NAMESPACE > /dev/null 2>&1
      else
        break
      fi

#vvvvvvvvv

      let cnt=cnt+1
      sleep 30
    done

    if [ $ret -ne 0 ]; then
      logMessages /tmp/error.log
      echo "ERROR: failed to install Tanzu Package $PACKAGE_NAME"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ tanzu package install $PACKAGE_NAME --package-name $PACKAGE_ID --namespace $PACKAGE_NAMESPACE --version $package_ver --create-namespace"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => tanzu package install $PACKAGE_NAME --package-name $PACKAGE_ID --namespace $PACKAGE_NAMESPACE --version $package_ver --create-namespace"
      fi

      exit 1
    else
      rm -f /tmp/error.log
    fi
  fi

  cmdLoop tanzu package installed list -A -o json > /tmp/output.json 2>/dev/null
  stt=$(jq -r --arg key "$PACKAGE_NAME" '.[] | select(.name == $key).name' /tmp/output.json)
  if [ "$stt" == "$PACKAGE_NAME" ]; then
    while [ $cnt -lt 5 ]; do
      cmdLoop tanzu package installed get $PACKAGE_NAME -n $PACKAGE_NAMESPACE -o json > /tmp/pkg.json
      pkg_stt=$(jq -r '.[]."status"' /tmp/pkg.json)
      pkg_nam=$(jq -r '.[]."package-name"' /tmp/pkg.json)
      pkg_ver=$(jq -r '.[]."package-version"' /tmp/pkg.json)

      [ "$pkg_stt" == "Reconcile succeeded" -o "$pkg_stt" == "Reconciling" ] && break

      let cnt=cnt+1
      sleep 60
    done

    messageTitle "Verify $PACKAGE_DES"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME)"                 "$pkg_nam"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) Namespace"       "$NAMESPACE"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) Version"         "$pkg_ver"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) Status"          "$pkg_stt"

    if [ "$pkg_stt" != "Reconcile succeeded" -a "$pkg_stt" != "Reconciling" ]; then
      echo "ERROR: failed to reconsile Tanzu Package ($PACKAGE_NAME) status $pkg_stt"
      messageLine
      tanzu package installed get $PACKAGE_NAME -n $PACKAGE_NAMESPACE
      messageLine
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ tanzu package install $PACKAGE_NAME --package-name $PACKAGE_ID --namespace $PACKAGE_NAMESPACE --version $package_ver --create-namespace"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => tanzu package install $PACKAGE_NAME --package-name $PACKAGE_ID --namespace $PACKAGE_NAMESPACE --version $package_ver --create-namespace"
      fi

      exit 1
    fi
  else
    echo "ERROR: The Tanzu Package: $PACKAGE_NAME has not been installed"
    exit 1
  fi

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_ENABLED          "true"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_NAMESPACE        "$NAMESPACE"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_PACKAGE_NAME     "$pkg_nam"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_PACKAGE_VERSION  "$pkg_ver"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGERPACKAGE__STATUS   "$pkg_stt"
}

# ------------------------------------------------------------------------------------------
# Function Name ......: TanzuPackage_Harbor
# Function Purpose ...: Install the Harbor OCI Registry
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Tanzi Package Display Name
#          ($2) ......: Namespace Name
#          ($3) ......: Tanzu Package Name
#          ($4) ......: Package Description
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 on Success, 1 on failure
# ------------------------------------------------------------------------------------------
# https://tanzucommunityedition.io/docs/latest/package-readme-harbor-2.3.3/
# ------------------------------------------------------------------------------------------
TanzuPackage_Harbor () {
  if [ "$TDH_SERVICE_REGISTRY_HARBOR" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_REGISTRY_HARBOR        "false"
    return
  fi

  PACKAGE_NAME=$1
  NAMESPACE=registry-harbor
  PACKAGE_NAMESPACE=tanzu-system-packages
  NAMESPACE=tanzu-system-registry
  PACKAGE_ID=$2
  PACKAGE_DES="Harbor OCI Registry"

  HARBOR_TLS_SECRET=tanzu-demo-hub-tls
  NOTARY_TLS_SECRET=tanzu-demo-hub-tls
  HARBOR_HOSTNAME="harbor.apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"
  NOTARY_HOSTNAME="notary.apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"

  #tanzu package installed delete harbor-pkg --namespace tanzu-system-packages

  cmdLoop tanzu package installed list -A -o json > /tmp/output.json 2>/dev/null
  pkg=$(jq -r --arg key "$PACKAGE_NAME" '.[] | select(.name == $key).name' /tmp/output.json)
  if [ "$pkg" != "$PACKAGE_NAME" ]; then
    messagePrint " ▪ Create Namespace"  "$NAMESPACE"
    deleteNamespace $NAMESPACE > /dev/null 2>&1
    createNamespace $NAMESPACE > /dev/null 2>&1

    # --- DOCKER PULL SECRET ---
    dockerPullSecret $NAMESPACE

    # --- COPY SECRET TO NAMESPACE ---
    copySecretObject default $NAMESPACE tanzu-demo-hub-tls
    #kubectl get secret tanzu-demo-hub-tls --namespace=default  -oyaml | grep -v '^\s*namespace:\s' | \
    #kubectl apply --namespace=$NAMESPACE -f - > /dev/null 2>&1

    cmdLoop kubectl get secrets tanzu-demo-hub-tls -n $NAMESPACE -o json > /tmp/output.json
    tls_crt=$(jq -r '.data."tls.crt"' /tmp/output.json) 
    tls_crt=$(jq -r '.data."tls.key"' /tmp/output.json) 
    ca_crt=$(cat $HOME/.tanzu-demo-hub/certificates/tdh-cert.pem | base64 -w 10000)

    #DELETEME
    #tls_crt=$(kubectl get secrets tanzu-demo-hub-tls -n $NAMESPACE -o json | jq -r '.data."tls.crt"') 
    #tls_key=$(kubectl get secrets tanzu-demo-hub-tls -n $NAMESPACE -o json | jq -r '.data."tls.key"') 

    cmdLoop tanzu package available list $PACKAGE_ID -A -o json > /tmp/output.json
    package_ver=$(jq -r '.[]."version"' /tmp/output.json | tail -1)
    package_rel=$(jq -r '.[]."released-at"' /tmp/output.json | tail -1)

    #DELETEME
    #package_ver=$(tanzu package available list $PACKAGE_ID -A -o json | jq -r '.[]."version"' | tail -1)
    #package_rel=$(tanzu package available list $PACKAGE_ID -A -o json | jq -r '.[]."released-at"' | tail -1)

    HARBOR_VALUES=/tmp/harbor_values.yaml
    #echo "namespace: $NAMESPACE"                                                                                     >  $HARBOR_VALUES
    echo "harborAdminPassword: $TDH_HARBOR_ADMIN_PASSWORD"                                                                      >  $HARBOR_VALUES
    echo "secretKey: MmetwEOsKpC7VWGx"                                                                                          >> $HARBOR_VALUES
    echo "hostname: harbor.apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"                                                    >> $HARBOR_VALUES
    #echo "tlsCertificateSecretName: $HARBOR_TLS_SECRET"                                                                        >> $HARBOR_VALUES
    echo "logLevel: debug"                                                                                                      >> $HARBOR_VALUES
    #echo "tlsCertificate:"                                                                                                     >> $HARBOR_VALUES
    #echo "  tls.crt: $tls_crt"                                                                                                 >> $HARBOR_VALUES
    #echo "  tls.key: $tls_key"                                                                                                 >> $HARBOR_VALUES
    #echo "  ca.crt: $ca_crt"                                                                                                   >> $HARBOR_VALUES
    echo "tlsCertificate:"                                                                                                      >> $HARBOR_VALUES
    echo "  tls.crt: |"                                                                                                         >> $HARBOR_VALUES
    kubectl get secrets tanzu-demo-hub-tls -n $NAMESPACE -o json | jq -r '.data."tls.crt"' | base64 -d | sed 's/^/    /g'       >> $HARBOR_VALUES
    echo "  tls.key: |"                                                                                                         >> $HARBOR_VALUES
    kubectl get secrets tanzu-demo-hub-tls -n $NAMESPACE -o json | jq -r '.data."tls.key"' | base64 -d | sed 's/^/    /g'       >> $HARBOR_VALUES
    echo "  ca.crt: |"                                                                                                          >> $HARBOR_VALUES
    cat $HOME/.tanzu-demo-hub/certificates/tdh-cert.pem | sed 's/^/    /g' >> $HARBOR_VALUES
    echo ""                                                                                                                     >> $HARBOR_VALUES
    

    #echo "global:"                                                                                                              >> $HARBOR_VALUES
    #echo "  imageRegistry: docker.io"                                                                                           >> $HARBOR_VALUES
    #echo "  imagePullSecrets:"                                                                                                  >> $HARBOR_VALUES
    #echo "    - tdh-docker-repo"                                                                                                >> $HARBOR_VALUES

    if [ "${TDH_DEPLOYMENT_CLOUD}" == "vSphere" -a "${TDH_TKGMC_TKG_TYPE}" == "tkgs" ]; then
      echo "  storageClass: $VSPHERE_TKGS_STORAGE_CLASS"                                                                        >> $HARBOR_VALUES
    fi

    echo "database:"                                                                                                            >> $HARBOR_VALUES
    echo "  password: 9uorOHGRc8MjWDVg"                                                                                         >> $HARBOR_VALUES
    echo "core:"                                                                                                                >> $HARBOR_VALUES
    echo "  secret: r7eqtW42TFgm5CT1"                                                                                           >> $HARBOR_VALUES
    echo "  xsrfKey: CbU5zjhjdaQ1uol7z7AfZjr38by6phIL"                                                                          >> $HARBOR_VALUES
    echo "jobservice:"                                                                                                          >> $HARBOR_VALUES
    echo "  secret: 4jYz75cztLwQK4Uw"                                                                                           >> $HARBOR_VALUES
    echo ""                                                                                                                     >> $HARBOR_VALUES
    echo "registry:"                                                                                                            >> $HARBOR_VALUES
    echo "  secret: 7SihDjyRJsczsOaJ"                                                                                           >> $HARBOR_VALUES
    echo ""                                                                                                                     >> $HARBOR_VALUES
    echo ""                                                                                                                     >> $HARBOR_VALUES
    echo "persistence:"                                                                                                         >> $HARBOR_VALUES
    #echo "  enabled: true"                                                                                                      >> $HARBOR_VALUES
    #echo "  resourcePolicy: 'keep'"                                                                                             >> $HARBOR_VALUES
    echo "  persistentVolumeClaim:"                                                                                             >> $HARBOR_VALUES
    echo "    registry:"                                                                                                        >> $HARBOR_VALUES
    echo "      accessMode: ReadWriteOnce"                                                                                      >> $HARBOR_VALUES
    echo "      size: 50Gi"                                                                                                     >> $HARBOR_VALUES
    echo "    jobservice:"                                                                                                        >> $HARBOR_VALUES
    echo "      accessMode: ReadWriteOnce"                                                                                      >> $HARBOR_VALUES
    echo "      size: 5Gi"                                                                                                      >> $HARBOR_VALUES
    echo "    database:"                                                                                                        >> $HARBOR_VALUES
    echo "      accessMode: ReadWriteOnce"                                                                                      >> $HARBOR_VALUES
    echo "      size: 50Gi"                                                                                                     >> $HARBOR_VALUES
    echo "    redis:"                                                                                                           >> $HARBOR_VALUES
    echo "      accessMode: ReadWriteOnce"                                                                                      >> $HARBOR_VALUES
    echo "      size: 50Gi"                                                                                                     >> $HARBOR_VALUES
    echo "    trivy:"                                                                                                           >> $HARBOR_VALUES
    echo "      accessMode: ReadWriteOnce"                                                                                      >> $HARBOR_VALUES
    echo "      size: 10Gi"                                                                                                     >> $HARBOR_VALUES

    messageTitle "Installing $PACKAGE_DES"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME)"   "$PACKAGE_ID ($package_ver/$package_rel)"
    cnt=0; ret=1
#bbbbb
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      # --- DELETE SERVICE ACCOUNT IF IT EXIST ----
      cmdLoop kubectl get serviceaccount -n tanzu-system-packages -o json > /tmp/output.json
      nam=$(jq -r --arg key "${PACKAGE_NAME}-${PACKAGE_NAMESPACE}-sa" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
      [ "$nam" != "" ] && cmdLoop kubectl -n tanzu-system-packages delete serviceaccount $nam

      # --- DELETE SERVICE ACCOUNT IF IT EXIST ----
      cmdLoop kubectl get clusterroles -n tanzu-system-packages -o json > /tmp/output.json
      nam=$(jq -r --arg key "${PACKAGE_NAME}-${PACKAGE_NAMESPACE}-role" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
      [ "$nam" != "" ] && cmdLoop kubectl -n tanzu-system-packages delete clusterrole $nam

      # --- DELETE SERVICE ACCOUNT IF IT EXIST ----
      cmdLoop kubectl get clusterrolebinding -n tanzu-system-packages -o json > /tmp/output.json
      nam=$(jq -r --arg key "${PACKAGE_NAME}-${PACKAGE_NAMESPACE}-cluster-rolebinding" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
      [ "$nam" != "" ] && cmdLoop kubectl -n tanzu-system-packages delete clusterrolebinding $nam

      tanzu package install $PACKAGE_NAME --package-name $PACKAGE_ID --namespace $PACKAGE_NAMESPACE \
             --version $package_ver --create-namespace --poll-timeout 20m --poll-interval 10s \
             --values-file $HARBOR_VALUES --wait=true --verbose 9 > /tmp/error.log 2>&1; ret=$?
      waitTanzuPackageReconsile $PACKAGE_NAME $PACKAGE_NAMESPACE; ret=$?
      if [ $ret -ne 0 ]; then
        # --- DELETE PACKAGE AND START OVER ---
        deleteTanzuPackage $PACKAGE_NAME $PACKAGE_NAMESPACE > /dev/null 2>&1
      else
        break
      fi

      let cnt=cnt+1
      sleep 30
    done

    # Ignore any install error because installation hits a timeout, but mostly succeeding, sdubois 14.01.2022
    ret=0
    if [ $ret -ne 0 ]; then
      logMessages /tmp/error.log
      echo "ERROR: failed to install Tanzu Package $PACKAGE_NAME)"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ tanzu package install $PACKAGE_NAME --create-namespace \\"
        echo "                             --package-name $PACKAGE_ID \\"
        echo "                             --namespace $PACKAGE_NAMESPACE --version $package_ver \\"
        echo "                             --values-file $HARBOR_VALUES --poll-timeout 10m --wait=true"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => tanzu package install $PACKAGE_NAME --create-namespace \\"
        echo "                             --package-name $PACKAGE_ID \\"
        echo "                             --namespace $PACKAGE_NAMESPACE --version $package_ver \\"
        echo "                             --values-file $HARBOR_VALUES --poll-timeout 10m --wait=true"
      fi

      exit 1
    else
      rm -f /tmp/error.log
    fi
  fi


  cmdLoop tanzu package installed list -A -o json > /tmp/output.json 2>/dev/null
  stt=$(jq -r --arg key "$PACKAGE_NAME" '.[] | select(.name == $key).name' /tmp/output.json)
  if [ "$stt" == "$PACKAGE_NAME" ]; then
    cmdLoop tanzu package installed get $PACKAGE_NAME -n $PACKAGE_NAMESPACE -o json > /tmp/pkg.json
    pkg_stt=$(jq -r '.[]."status"' /tmp/pkg.json)
    pkg_nam=$(jq -r '.[]."package-name"' /tmp/pkg.json)
    pkg_ver=$(jq -r '.[]."package-version"' /tmp/pkg.json)

    messageTitle "Verify $PACKAGE_DES"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME)"                 "$pkg_nam"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) Namespace"       "$NAMESPACE"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) Version"         "$pkg_ver"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) Status"          "$pkg_stt"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) User:"           "admin"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) Password:"       "$TDH_HARBOR_ADMIN_PASSWORD"
  else
    echo "ERROR: The Tanzu Package: $PACKAGE_NAME has not been installed"
    exit 1
  fi

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_ENABLED          "true"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_ADMIN_PASSWORD   "$TDH_HARBOR_ADMIN_PASSWORD"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR       "$HARBOR_HOSTNAME"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_NOTARY       "$NOTARY_HOSTNAME"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_PACKAGE__NAME    "$pkg_nam"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_PACKAGE_VERSION  "$pkg_ver"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_PACKAGE_STATUS   "$pkg_stt"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_PACKAGE_UPDATE   "$dat"
}

# ------------------------------------------------------------------------------------------
# Function Name ......: TanzuPackage_Contour
# Function Purpose ...: Install Contour Ingress Controller
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Tanzi Package Display Name
#          ($2) ......: Namespace Name
#          ($3) ......: Tanzu Package Name
#          ($4) ......: Package Description      
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 on Success, 1 on failure
# ------------------------------------------------------------------------------------------
# https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.4/vmware-tanzu-kubernetes-grid-14/GUID-packages-ingress-contour.html
# tanzu package available get contour.tanzu.vmware.com/1.17.2+vmware.1-tkg.2 --values-schema
# ------------------------------------------------------------------------------------------
TanzuPackage_Contour() {
  PACKAGE_NAME=$1
  NAMESPACE=ingress-contour
  PACKAGE_NAMESPACE=tanzu-system-packages
  NAMESPACE=tanzu-system-ingress
  PACKAGE_ID=$2
  PACKAGE_DES="Contour ingress controller"

  cmdLoop tanzu package installed list -A -o json > /tmp/output.json 2>/dev/null
  pkg=$(jq -r --arg key "$PACKAGE_NAME" '.[] | select(.name == $key).name' /tmp/output.json)
  if [ "$pkg" != "$PACKAGE_NAME" ]; then
    cmdLoop tanzu package available list $PACKAGE_ID -A -o json > /tmp/output.json
    if [ -s /tmp/output.json ]; then
      package_ver=$(jq -r '.[]."version"' /tmp/output.json | tail -1)
      package_rel=$(jq -r '.[]."released-at"' /tmp/output.json | tail -1)
    else
      package_ver=""; package_rel=""
    fi

    if [ "${TDH_DEPLOYMENT_CLOUD}" == "docker" -o "${TDH_TKGMC_INFRASTRUCTURE}" == "docker" ]; then
      VALUE_FILE=/tmp/contour-data-values.yaml; rm -f $VALUE_FILE
      echo "namespace: tanzu-system-ingress"                                            >  $VALUE_FILE
      echo "contour:"                                                                   >> $VALUE_FILE
      echo " configFileContents: {}"                                                    >> $VALUE_FILE
      echo " useProxyProtocol: false"                                                   >> $VALUE_FILE
      echo " replicas: 2"                                                               >> $VALUE_FILE
      echo " pspNames: \"vmware-system-restricted\""                                    >> $VALUE_FILE
      echo " logLevel: info"                                                            >> $VALUE_FILE
      echo "envoy:"                                                                     >> $VALUE_FILE
      echo " service:"                                                                  >> $VALUE_FILE
      echo "   type: LoadBalancer"                                                      >> $VALUE_FILE
      echo "   annotations: {}"                                                         >> $VALUE_FILE
      echo "   nodePorts:"                                                              >> $VALUE_FILE
      echo "     http: null"                                                            >> $VALUE_FILE
      echo "     https: null"                                                           >> $VALUE_FILE
      echo "   externalTrafficPolicy: Cluster"                                          >> $VALUE_FILE
      echo "   disableWait: false"                                                      >> $VALUE_FILE
      echo " hostPorts:"                                                                >> $VALUE_FILE
      echo "   enable: true"                                                            >> $VALUE_FILE
      echo "   http: 80"                                                                >> $VALUE_FILE
      echo "   https: 443"                                                              >> $VALUE_FILE
      echo " hostNetwork: false"                                                        >> $VALUE_FILE
      echo " terminationGracePeriodSeconds: 300"                                        >> $VALUE_FILE
      echo " logLevel: info"                                                            >> $VALUE_FILE
      echo " pspNames: null"                                                            >> $VALUE_FILE
      echo "certificates:"                                                              >> $VALUE_FILE
      echo " duration: 8760h"                                                           >> $VALUE_FILE
      echo " renewBefore: 360h"                                                         >> $VALUE_FILE
    fi

    if [ "${TDH_DEPLOYMENT_CLOUD}" == "Azure" -o "${TDH_TKGMC_INFRASTRUCTURE}" == "Azure" ]; then
      VALUE_FILE=/tmp/contour-data-values.yaml; rm -f $VALUE_FILE
      echo "namespace: tanzu-system-ingress"                                            >> $VALUE_FILE
      echo "contour:"                                                                   >> $VALUE_FILE
      echo " configFileContents: {}"                                                    >> $VALUE_FILE
      echo " useProxyProtocol: false"                                                   >> $VALUE_FILE
      echo " replicas: 2"                                                               >> $VALUE_FILE
      echo " pspNames: \"vmware-system-restricted\""                                    >> $VALUE_FILE
      echo " logLevel: info"                                                            >> $VALUE_FILE
      echo "envoy:"                                                                     >> $VALUE_FILE
      echo " service:"                                                                  >> $VALUE_FILE
      echo "   type: LoadBalancer"                                                      >> $VALUE_FILE
      echo "   annotations: {}"                                                         >> $VALUE_FILE
      echo "   nodePorts:"                                                              >> $VALUE_FILE
      echo "     http: null"                                                            >> $VALUE_FILE
      echo "     https: null"                                                           >> $VALUE_FILE
      echo "   externalTrafficPolicy: Cluster"                                          >> $VALUE_FILE
      echo "   disableWait: false"                                                      >> $VALUE_FILE
      echo " hostPorts:"                                                                >> $VALUE_FILE
      echo "   enable: true"                                                            >> $VALUE_FILE
      echo "   http: 80"                                                                >> $VALUE_FILE
      echo "   https: 443"                                                              >> $VALUE_FILE
      echo " hostNetwork: false"                                                        >> $VALUE_FILE
      echo " terminationGracePeriodSeconds: 300"                                        >> $VALUE_FILE
      echo " logLevel: info"                                                            >> $VALUE_FILE
      echo " pspNames: null"                                                            >> $VALUE_FILE
      echo "certificates:"                                                              >> $VALUE_FILE
      echo " duration: 8760h"                                                           >> $VALUE_FILE
      echo " renewBefore: 360h"                                                         >> $VALUE_FILE
    fi

    if [ "${TDH_DEPLOYMENT_CLOUD}" == "AWS" -o "${TDH_TKGMC_INFRASTRUCTURE}" == "AWS" ]; then
      VALUE_FILE=/tmp/contour-data-values.yaml; rm -f $VALUE_FILE
      echo "---"                                                                        >  $VALUE_FILE
      echo "namespace: tanzu-system-ingress"                                            >> $VALUE_FILE
      echo "contour:"                                                                   >> $VALUE_FILE
      echo " configFileContents: {}"                                                    >> $VALUE_FILE
      echo " useProxyProtocol: false"                                                   >> $VALUE_FILE
      echo " replicas: 2"                                                               >> $VALUE_FILE
      echo " pspNames: \"vmware-system-restricted\""                                    >> $VALUE_FILE
      echo " logLevel: info"                                                            >> $VALUE_FILE
      echo "envoy:"                                                                     >> $VALUE_FILE
      echo " service:"                                                                  >> $VALUE_FILE
      echo "   type: LoadBalancer"                                                      >> $VALUE_FILE
      echo "   annotations: {}"                                                         >> $VALUE_FILE
      echo "   nodePorts:"                                                              >> $VALUE_FILE
      echo "     http: null"                                                            >> $VALUE_FILE
      echo "     https: null"                                                           >> $VALUE_FILE
      echo "   externalTrafficPolicy: Cluster"                                          >> $VALUE_FILE
      echo "   aws:"                                                                    >> $VALUE_FILE
      echo "     LBType: classic"                                                       >> $VALUE_FILE
      echo "   disableWait: false"                                                      >> $VALUE_FILE
      echo " hostPorts:"                                                                >> $VALUE_FILE
      echo "   enable: true"                                                            >> $VALUE_FILE
      echo "   http: 80"                                                                >> $VALUE_FILE
      echo "   https: 443"                                                              >> $VALUE_FILE
      echo " hostNetwork: false"                                                        >> $VALUE_FILE
      echo " terminationGracePeriodSeconds: 300"                                        >> $VALUE_FILE
      echo " logLevel: info"                                                            >> $VALUE_FILE
      echo " pspNames: null"                                                            >> $VALUE_FILE
      echo "certificates:"                                                              >> $VALUE_FILE
      echo " duration: 8760h"                                                           >> $VALUE_FILE
      echo " renewBefore: 360h"                                                         >> $VALUE_FILE
    fi

    messageTitle "Installing $PACKAGE_DES"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME)"   "$PACKAGE_ID ($package_ver/$package_rel)"
    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      # --- DELETE SERVICE ACCOUNT IF IT EXIST ----
      cmdLoop kubectl get serviceaccount -n tanzu-system-packages -o json > /tmp/output.json
      nam=$(jq -r --arg key "${PACKAGE_NAME}-${PACKAGE_NAMESPACE}-sa" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
      [ "$nam" != "" ] && cmdLoop kubectl -n tanzu-system-packages delete serviceaccount $nam

      # --- DELETE SERVICE ACCOUNT IF IT EXIST ----
      cmdLoop kubectl get clusterroles -n tanzu-system-packages -o json > /tmp/output.json
      nam=$(jq -r --arg key "${PACKAGE_NAME}-${PACKAGE_NAMESPACE}-role" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
      [ "$nam" != "" ] && cmdLoop kubectl -n tanzu-system-packages delete clusterrole $nam

      # --- DELETE SERVICE ACCOUNT IF IT EXIST ----
      cmdLoop kubectl get clusterrolebinding -n tanzu-system-packages -o json > /tmp/output.json
      nam=$(jq -r --arg key "${PACKAGE_NAME}-${PACKAGE_NAMESPACE}-cluster-rolebinding" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
      [ "$nam" != "" ] && cmdLoop kubectl -n tanzu-system-packages delete clusterrolebinding $nam

      #tanzu package install $PACKAGE_NAME --package-name $PACKAGE_ID --namespace $PACKAGE_NAMESPACE \
      #     --version $package_ver --create-namespace --poll-timeout 10m \
      #     --values-file /tmp/contour-data-values.yaml --wait=true > /tmp/contour.log 2>&1; ret=$?

echo "tanzu package install $PACKAGE_NAME --package-name $PACKAGE_ID --namespace $PACKAGE_NAMESPACE --version $package_ver --create-namespace --poll-timeout 10m --values-file /tmp/contour-data-values.yaml --wait=true"
      tanzu package install $PACKAGE_NAME --package-name $PACKAGE_ID --namespace $PACKAGE_NAMESPACE \
           --version $package_ver --create-namespace --poll-timeout 30m \
           --values-file /tmp/contour-data-values.yaml --wait=true 

      waitTanzuPackageReconsile $PACKAGE_NAME $PACKAGE_NAMESPACE; ret=$?
      if [ $ret -ne 0 ]; then
        # --- DELETE PACKAGE AND START OVER ---
        deleteTanzuPackage $PACKAGE_NAME $PACKAGE_NAMESPACE > /dev/null 2>&1
      else
        break
      fi

      let cnt=cnt+1
      sleep 30
    done

    if [ $ret -ne 0 ]; then
      logMessages /tmp/contour.log
      echo "ERROR: failed to install Tanzu Package $PACKAGE_NAME)"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ tanzu package install $PACKAGE_NAME --create-namespace \\"
        echo "                             --package-name $PACKAGE_ID \\"
        echo "                             --namespace $PACKAGE_NAMESPACE --version $package_ver \\"
        echo "                             --values-file /tmp/contour-data-values.yaml"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => tanzu package install $PACKAGE_NAME --create-namespace \\"
        echo "                             --package-name $PACKAGE_ID \\"
        echo "                             --namespace $PACKAGE_NAMESPACE --version $package_ver \\"
        echo "                             --values-file /tmp/contour-data-values.yaml"
      fi

      exit 1
    else
      rm -f /tmp/contour.log
    fi
  fi

  cmdLoop tanzu package installed list -A -o json > /tmp/output.json 2>/dev/null
  stt=$(jq -r --arg key "$PACKAGE_NAME" '.[] | select(.name == $key).name' /tmp/output.json)
  if [ "$stt" == "$PACKAGE_NAME" ]; then
    cmdLoop tanzu package installed get $PACKAGE_NAME -n $PACKAGE_NAMESPACE -o json > /tmp/pkg.json
    pkg_stt=$(jq -r '.[]."status"' /tmp/pkg.json)
    pkg_nam=$(jq -r '.[]."package-name"' /tmp/pkg.json)
    pkg_ver=$(jq -r '.[]."package-version"' /tmp/pkg.json)

    messageTitle "Verify $PACKAGE_DES"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME)"                 "$pkg_nam"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) Namespace"       "$NAMESPACE"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) Version"         "$pkg_ver"
    messagePrint " ▪ Tanzu Package ($PACKAGE_NAME) Status"          "$pkg_stt"
  else
    echo "ERROR: The Tanzu Package: $PACKAGE_NAME has not been installed"
    exit 1
  fi

  # --- VERIFY DNS DOMAIN ---
  verifyHostedZone ${TDH_ENVNAME}.$AWS_HOSTED_DNS_DOMAIN

  [ "$TDH_TOOLS_CONTAINER_TYPE" == "tkg" ] && CONTOUR_NS=tanzu-system-ingress || CONTOUR_NS=projectcontour
  CONTOUR_NS=tanzu-system-ingress

  ipa=""; cnt=0
  while [ "$ipa" == "" -a $cnt -lt 15 ]; do
    ipa=$(kubectl describe svc envoy --namespace $CONTOUR_NS | egrep "^LoadBalancer Ingress:" | awk '{ print $NF }')
    [ "$ipa" != "" ] && break
    let cnt=cnt+1
    sleep 30
  done

  if [ "$ipa" == "" ]; then
    echo "ERROR: Failed to gather IP Adress of the ingress-contour-envoy service in namespace ingress-contour"
    echo "       Please check manually:"
    messageLine
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ kubectl get svc --namespace $NAMESPACE"
      echo "          tdh-tools:/$ exit"
    else
      echo "       => kubectl get svc --namespace $NAMESPACE"
    fi

    exit 1
  fi

  # --- SET DNS RECORD ---
  if [ "$TDH_DEPLOYMENT_CLOUD" == "minikube" -o "$TDH_DEPLOYMENT_CLOUD" == "vSphere" -o "$TDH_DEPLOYMENT_CLOUD" == "Azure" -o "$TDH_DEPLOYMENT_CLOUD" == "docker" -o \
       "$TDH_TKGMC_INFRASTRUCTURE" == "minikube" -o "$TDH_TKGMC_INFRASTRUCTURE" == "vSphere" -o "$TDH_TKGMC_INFRASTRUCTURE" == "Azure" -o "$TDH_TKGMC_INFRASTRUCTURE" == "docker" ]; then
    setDNSrecord "$ipa" "${TDH_ENVNAME}" "${AWS_HOSTED_DNS_DOMAIN}" "*.apps-contour" "A"
  else
    setDNSalias "$ipa" "${TDH_ENVNAME}" "${AWS_HOSTED_DNS_DOMAIN}" "*.apps-contour" "A"
  fi

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_ENABLED          "true"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_LB_IP            "$ipa"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_LB_DOMAIN        "apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_NAMESPACE        "$NAMESPACE"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_PACKAGE_NAME     "$crt"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_PACKAGE_VERSION  "$ver"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_PACKAGE_STATUS   "$stt"
}

createTDHdirs() {
  DIRLIST="$*"

  for n in $list; do
    [ ! -d $n ] && mkdir -p $n
  done
}

chownDirectories() {
  HOMEDIR=$HOME
  USR_NAME=$(id -un)
  GRP_NAME=$(id -gn)
  MODE=$1
  [ "$MODE" == "" ] && MODE=TKG
  [ "$MODE" == "tkg" ] && TDH_TOOLS_DIR=.tdh-tools || TDH_TOOLS_DIR=.tdh-tools-tce

  list=".azure .mvn .cache .config .govmomi .gradle .kube .kube-tkg .local .local .s3cfg .tanzu-demo-hub .tanzu .terraform .vmware-cna-saas"

  if [ "$HOME" == "" ]; then 
    echo "ERROR: Home directory can not be determinded, \$HOME variable is not set"
    exit 1
  fi

  for n in $list; do
    [ ! -d $HOMEDIR/$TDH_TOOLS_DIR/$n ] && mkdir -p $HOMEDIR/$TDH_TOOLS_DIR/$n
  done
}

checkTanzuCLI() {
  MODE=$1
  if [ ! -d $HOME/.tanzu ]; then 
    if [ $MODE == "tce" ]; then 
      # --- INSTALL TANZU UTILITIES ---
      cd /tanzu/tce-linux-amd64 && ./install.sh > /dev/null 2>&1
      #cd /tanzu/tce-linux-amd64 && tanzu plugin clean                                                               <
      #cd /tanzu/tce-linux-amd64 && tanzu plugin install --local cli all  
    else
      cd /tanzu && tanzu plugin clean
      cd /tanzu && tanzu plugin install --local cli all
    fi
  fi
}

duration() {
  TIME=$1

  if [ $TIME -ge 60 ]; then 
    let tmin=TIME/60
    let tsec=TIME%60
    tmin="${tmin}m"
  else
    tmin=""
    tsec=$TIME
  fi

  echo "${tmin}${tsec}s"
}

hourInSec() {
  date "+%H" | sed 's/^0//g'
}

minInSec() {
  date "+%M" | sed 's/^0//g'
}

secInSec() {
  date "+%S" | sed 's/^0//g'
}

dockerRunPreparation() {
  TDH_TOOLS=$1
  TDH_TOOLS_PATH=".${TDH_TOOLS}"

  # --- CLEAN OLD CONTAINERS ---
  cid=$(docker ps -a | grep $TDH_TOOLS:latest | awk '{ print $1 }')
  [ "$cid" != "" ] && for n in $cid; do docker rm $n -f > /dev/null 2>&1; done

  # --- DUMP ENVIRONMENT VARIABLES ---
  env | grep TDH > /tmp/tdh.env

  export CORE_OPTIONS="-it --init --rm --hostname tdh-tools --name $TDH_TOOLS --network=host"
  export USER_OPTIONS="$CORE_OPTIONS -u $(id -u):$(id -g) --env-file /tmp/tdh.env -e \"KUBECONFIG=$HOME/.kube/config\""
  export ROOT_OPTIONS="$CORE_OPTIONS --env-file /tmp/tdh.env -e \"KUBECONFIG=$HOME/.kube/config\""

  export CORE_MOUNTS=(
           "-v /var/run/docker.sock:/var/run/docker.sock"                           ## REQIORED FOR DOCKER
           "-v $HOME/.tanzu-demo-hub:$HOME/.tanzu-demo-hub:rw"
           "-v $HOME/.tanzu-demo-hub.cfg:$HOME/.tanzu-demo-hub.cfg:ro"
           "-v $HOME/$TDH_TOOLS_PATH/.cache:$HOME/.cache:rw"
           "-v $HOME/$TDH_TOOLS_PATH/.config:$HOME/.config:rw"                      ## CONFIG FOR HELM AND TANZU
           "-v $HOME/$TDH_TOOLS_PATH/.kube:$HOME/.kube:rw"
           "-v $HOME/$TDH_TOOLS_PATH/.kube-tkg:$HOME/.kube-tkg:rw"
           "-v $HOME/$TDH_TOOLS_PATH/.local:$HOME/.local:rw"
           "-v $HOME/$TDH_TOOLS_PATH/.tanzu:$HOME/.tanzu:rw"
           "-v $HOME/$TDH_TOOLS_PATH/.terraform:$HOME/.terraform:rw"
           "-v $HOME/$TDH_TOOLS_PATH/.terraform.d:$HOME/.terraform.d:rw"
           "-v $HOME/$TDH_TOOLS_PATH/.s3cfg:$HOME/.s3cfg:rw"
           "-v $HOME/$TDH_TOOLS_PATH/.govmomi:$HOME/.govmomi:rw"
           "-v $HOME/$TDH_TOOLS_PATH/.gradle:$HOME/.gradle:rw"
           "-v $HOME/$TDH_TOOLS_PATH/.docker:$HOME/.docker:rw"                      ## DOCKER LOGIN CREDENTIALS
           "-v $HOME/$TDH_TOOLS_PATH/.mvn:$HOME/.mvn:rw"
           "-v $HOME/$TDH_TOOLS_PATH/.aws:$HOME/.aws:rw"                            ## PERSISTANT AWS CREDENTIALS
           "-v $HOME/$TDH_TOOLS_PATH/.vmware-cna-saas:$HOME/.vmware-cna-saas:rw"    ## TANZU MISSION CONTROL (TMC) LOGIN CREDENTIALS
           "-v $HOME/$TDH_TOOLS_PATH/tmp:/tmp:rw"                                   ## KEEP TEMP PERSISTENT
           "-v $TDHPATH/:$TDHPATH:ro"                                               ## TANZU-DEMO-HUB DIRECTORY
         )

  # --- MAKE SURE DIRECTORIES ARE CREATED ---
  for n in $(echo ${CORE_MOUNTS[*]} | sed 's/\-. //g'); do
    [ $n == "/var/run/docker.sock" ] && continue
    localdir=$(echo $n | awk -F: '{ print $1 }')
    [ -d $localdir -a -d $localdir ] && mkdir -p $localdir
  done
} 

# ------------------------------------------------------------------------------------------
# Function Name ......: runTDHtoolsDemos
# Function Purpose ...: Install Contour Ingress Controller
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: TDH Tools Flag (TCE/TCE)
#          ($2) ......: Header Text to be shown on start
#          ($3) ......: Commandline PATH and Command to be executed by docker
#          ($4) ......: Commandline Arguments
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 on Success, 1 on failure
# ------------------------------------------------------------------------------------------
runTDHtoolsDemos() {
  TDH_CLUSTER_TYPE=$1
  [ "$TDH_CLUSTER_TYPE" == "" ] && TDH_CLUSTER_TYPE=tdh

  checkCLIcommands BASIC

  # --- DEMOS RUNNIGN ON TKGs SUPERVISOR CLUSTER (WCP) ---
  if [ "$TDH_CLUSTER_TYPE" == "wcp1" ]; then 
    if [ "$TDH_TOOLS_TYPE" == "" -o "$TDH_TOOLS_VERS" == "" -o "$TDH_CONTEXT" == "" ]; then
      CLUSTER_LIST=/tmp/cluster_list.csv; rm -f $CLUSTER_LIST; touch $CLUSTER_LIST
      for typ in tkg tce; do
        for ver in $(ls -1 $TDHHOME/files/tdh-tools/tdh-tools-${typ}-*.cfg | sed -e 's/^.*tools-tkg-//g' -e 's/^.*tools-tce-//g' -e 's/\.cfg//g' | sort -V); do
          if [ -s $HOME/.tdh-tools-${typ}-${ver}/.kube/config ]; then
            cmdLoop kubectl config view --kubeconfig=$HOME/.tdh-tools-${typ}-${ver}/.kube/config -o json | jq -r '.contexts[]' > /tmp/output.json 2>/dev/null
            for nam in $(jq -r 'select(.name | contains("wcp.")).name' /tmp/output.json 2>/dev/null); do
              echo "${typ},${ver},${nam},${clu}"  >> $CLUSTER_LIST
            done
          fi
        done
      done

      cnt=$(wc -l $CLUSTER_LIST | awk '{ print $1 }')
      if [ $cnt -eq 0 ]; then
        echo "ERROR: No Tanzu Demo Hub (TDH) Environment found, make sure there is an"
        echo "       environment deployed"
        exit
      fi

      if [ $cnt -gt 1 ]; then
        echo "WARNING: Multiple clusters found with Tanzu Demo Hub (TDH) environments, please chose the desired one"
        echo "         and restart the demo."
        awk -F',' '{ printf("         => ./%s --type %s --version %s --context %s\n",a,$1,$2,$3) }' a="$CMD_EXEC" $CLUSTER_LIST
        echo ""
        exit
      else
        TDH_TOOLS_TYPE=$(awk -F, '{ print $1 }' $CLUSTER_LIST) 
        TDH_TOOLS_VERS=$(awk -F, '{ print $2 }' $CLUSTER_LIST) 
        TDH_CONTEXT=$(awk -F, '{ print $3 }' $CLUSTER_LIST) 
        CMD_ARGS="$CMD_ARGS --type $TDH_TOOLS_TYPE --version $TDH_TOOLS_VERS --context $TDH_CONTEXT"
      fi
    fi
  fi

  # --- GET THE RIGHT TDH TOOLS CONTAINER ---
  for n in $(docker images --filter "reference=tdh*:CHKSUM*" | egrep "^tdh-tools" | sort -Vr | \
             awk '{ printf("%s:%s\n", $1,$2)}' | sed 's/CHKSUM-//g'); do
    img=$(echo $n | awk -F: '{ print $1 }' | sed 's/tdh-tools-//g') 
    tag=$(echo $n | awk -F: '{ print $2 }') 

    if [ -f $TDHHOME/files/tdh-tools/Dockerfile-$img ]; then 
      DOCKER_CHKSUM=$(sum $TDHHOME/files/tdh-tools/Dockerfile-$img | awk '{ print $1 }')
      if [ "$DOCKER_CHKSUM" == "$tag" ]; then
        TDH_TOOLS_TYPE=$(echo "$img" | awk -F'-' '{ print $1 }') 
        TDH_TOOLS=tdh-tools-${img}
        break
      fi
    fi
  done

  if [ "$TDH_TOOLS_TYPE" == "" ]; then 
    echo "ERROR: No available tdh-tools container available. Please generate one first."
    echo "       => cd $TDHHOME"
    echo "       => tools/tdh-tools-<tce|tkg>-<version>.sh"
    exit
  fi

  TOOLS_TLAG=$TDH_TOOLS_TYPE
  POSTSCRIPT="tdh-postinstall-user.sh"
  TDH_TOOLS_PATH=".${TDH_TOOLS}"

  if [ ! -f /tdh_tools_docker_container  ]; then
    cnt=$(docker ps | grep -c "tdh-tools-${TDH_TOOLS_TYPE}-${TDH_TOOLS_VERS}:latest") 
    if [ $cnt -gt 0 ]; then 
      echo "ERROR: Docker Container tdh-tools-${TDH_TOOLS_TYPE}-${TDH_TOOLS_VERS}:latest is already running"
      echo "       please wait until the other task has been completed"
      echo "       => docker ps"
      messageLine
      docker ps
      messageLine
      exit 1
    fi

    if [ $NATIVE -eq 0 ]; then
      cid=$(docker ps -a | grep $TDH_TOOLS:latest | awk '{ print $1 }')
      [ "$cid" != "" ] && for n in $cid; do docker rm $n -f > /dev/null 2>&1; done

      # --- DUMP ENVIRONMENT VARIABLES ---
      env | grep TDH > /tmp/tdh.env

      export CORE_OPTIONS="-it --init --rm --hostname tdh-tools --name $TDH_TOOLS --network=host"
      export USER_OPTIONS="$CORE_OPTIONS -u $(id -u):$(id -g) --env-file /tmp/tdh.env -e \"KUBECONFIG=$HOME/.kube/config\""
      export ROOT_OPTIONS="$CORE_OPTIONS --env-file /tmp/tdh.env -e \"KUBECONFIG=$HOME/.kube/config\""

      export CORE_MOUNTS=(
               "-v /var/run/docker.sock:/var/run/docker.sock"                           ## REQIORED FOR DOCKER
               "-v $HOME/.tanzu-demo-hub:$HOME/.tanzu-demo-hub:rw"
               "-v $HOME/.tanzu-demo-hub.cfg:$HOME/.tanzu-demo-hub.cfg:ro"
               "-v $HOME/$TDH_TOOLS_PATH/.cache:$HOME/.cache:rw"                        ## CACHED TANZU CONFIG
               "-v $HOME/$TDH_TOOLS_PATH/.config:$HOME/.config:rw"                      ## CONFIG FOR HELM AND TANZU
               "-v $HOME/$TDH_TOOLS_PATH/.kube:$HOME/.kube:rw"                          ## KUBERNETES CLUSTER CONTEXTS
               "-v $HOME/$TDH_TOOLS_PATH/.kube-tkg:$HOME/.kube-tkg:rw"                  ## TANZU MANAGEMENT CLUSTER CONTEXTS
               "-v $HOME/$TDH_TOOLS_PATH/.local:$HOME/.local:rw"                        ## TANZU CLI CONFIG
               "-v $HOME/$TDH_TOOLS_PATH/.tanzu:$HOME/.tanzu:rw"                        ## TANZU CONFIG (DEPRECATED)
               "-v $HOME/$TDH_TOOLS_PATH/.terraform:$HOME/.terraform:rw"                ## TERRAFORM CONFIG
               "-v $HOME/$TDH_TOOLS_PATH/.terraform.d:$HOME/.terraform.d:rw"            ## TERRAFORM CONFIG
               "-v $HOME/$TDH_TOOLS_PATH/.s3cfg:$HOME/.s3cfg:rw"                        ## AWS S3 CONFIG
               "-v $HOME/$TDH_TOOLS_PATH/.govmomi:$HOME/.govmomi:rw"                    ## GOVC CONFIG
               "-v $HOME/$TDH_TOOLS_PATH/.gradle:$HOME/.gradle:rw"                      ## GRADLE CONFIG
               "-v $HOME/$TDH_TOOLS_PATH/.docker:$HOME/.docker:rw"                      ## DOCKER LOGIN CREDENTIALS
               "-v $HOME/$TDH_TOOLS_PATH/.mvn:$HOME/.mvn:rw"                            ## MAVEN 
               "-v $HOME/$TDH_TOOLS_PATH/.mc:$HOME/.mc:rw"                              ## MINIO CLIENT
               "-v $HOME/$TDH_TOOLS_PATH/.aws:$HOME/.aws:rw"                            ## PERSISTANT AWS CREDENTIALS
               "-v $HOME/$TDH_TOOLS_PATH/.azure:$HOME/.azure:rw"                        ## PERSISTANT MICROSOFT AZURE CREDENTIALS
               "-v $HOME/$TDH_TOOLS_PATH/.cluster-api:$HOME/.cluster-api:rw"            ## PERSISTANT MICROSOFT AZURE CREDENTIALS
               "-v $HOME/$TDH_TOOLS_PATH/.vmware-cna-saas:$HOME/.vmware-cna-saas:rw"    ## TANZU MISSION CONTROL (TMC) LOGIN CREDENTIALS
               "-v $HOME/$TDH_TOOLS_PATH/tmp:/tmp:rw"                                   ## KEEP TEMP PERSISTENT
               "-v $TDHHOME/:$TDHHOME:ro"                                               ## TANZU-DEMO-HUB DIRECTORY
             )

      # --- MAKE SURE DIRECTORIES ARE CREATED ---
      for n in $(echo ${CORE_MOUNTS[*]} | sed 's/\-. //g'); do
        [ $n == "/var/run/docker.sock" ] && continue
        localdir=$(echo $n | awk -F: '{ print $1 }')
        [ ! -e $localdir ] && mkdir -p $localdir && chmod 777 $localdir
      done

      docker run $ROOT_OPTIONS ${CORE_MOUNTS[*]} $TDH_TOOLS:latest chmod 666 /var/run/docker.sock > /dev/null 2>&1
      docker run $USER_OPTIONS ${CORE_MOUNTS[*]} $TDH_TOOLS:latest /usr/local/bin/$POSTSCRIPT > /dev/null 2>&1
      [ $ROOT_SHELL -eq 1 ] && SEXEC_OPTIONS=$ROOT_OPTIONS || SEXEC_OPTIONS=$USER_OPTIONS
      docker run $SEXEC_OPTIONS ${CORE_MOUNTS[*]} $TDH_TOOLS:latest $TDHDEMO/$CMD_EXEC $CMD_ARGS; ret=$?

      # --- FINISH CURRENT SESSION AS WE RUN AS CONTAINER ---
      exit $ret
    fi
  else
    # trap ctrl-c and call ctrl_c()
    #trap ctrl_c INT

    cd $TDHDEMO

    CLUSTER_LIST=""
    CONTEXT_LIST=""

    # --- GATHER RIGHT KUBECONFIG ---
    for n in $(ls -1 $HOME/.tanzu-demo-hub/config/tkgmc-vsphere*.kubeconfig 2>/dev/null); do
      nam=$(echo $n | awk -F'/' '{ print $NF }' | sed 's/\.kubeconfig//g')
      vsphereSupervisorClusterLogin $nam

      kubectl --kubeconfig=$n --request-timeout 1s get ns >/dev/null 2>&1; ret=$?
      if [ $ret -eq 0 ]; then
        nam=$(echo $n | awk -F'/' '{ print $NF }' | sed 's/\.kubeconfig//g') 
        CLUSTER_LIST="$CLUSTER_LIST $nam" 
        CONTEXT_LIST="$CONTEXT_LIST $nam"
      fi
    done

    if [ "$TDH_CLUSTER_TYPE" == "wcp" ]; then 
      #vmware
      ##curl -m 1 https://pez-portal.int-apps.pcfone.io > /dev/null 2>&1; echo $?
      #if [ $ret -ne 0 ]; then
      #  echo "ERROR: you are not in VMware VPN"
      #  echo "       environment deployed"
      #  exit
      #fi

      cnt=$(echo "$CLUSTER_LIST" | awk '{ print NF }')
      if [ $cnt -eq 0 ]; then
        echo "ERROR: No Tanzu for vSphere Supervisor Cluster  found, make sure there is an"
        echo "       environment deployed"
        exit
      fi

      if [ $cnt -gt 1 ]; then
      echo "WARNING: Multiple clusters found with Tanzu Demo Hub (TDH) environments, please chose the desired one"
        echo "         and restart the demo."
        echo ""
        exit
      else
        clu=$(echo $CLUSTER_LIST | awk '{ print $1 }')
        export KUBECONFIG=$HOME/.tanzu-demo-hub/config/${clu}.kubeconfig
      fi
    fi

    for n in $(ls -1 $HOME/.tanzu-demo-hub/config/tdh*.kubeconfig 2>/dev/null); do
      nam=$(echo $n | awk -F'/' '{ print $NF }' | sed 's/\.kubeconfig//g') 
  
      cmdLoop kubectl --kubeconfig=$n --request-timeout 1s get cm -n default -o json > /tmp/output.json 2>/dev/null
      if [ -s /tmp/output.json ]; then
        cfm=$(jq -r '.items[].metadata | select(.name == "tanzu-demo-hub").name' /tmp/output.json 2>/dev/null)
        if [ "$cfm" == "tanzu-demo-hub" ]; then
          CLUSTER_LIST="$CLUSTER_LIST $nam" 
          CONTEXT_LIST="$CONTEXT_LIST $nam"
        fi
      else
        CONFIG="$HOME/.tdh-tools-${typ}-${ver}/.kube/config"
        echo "INFO: Cluster ($nam) in $CONFIG is not reachable, consider to delete it: "
        echo "      => rm $n"
      fi
    done
  
    if [ "$TDH_CLUSTER_TYPE" == "tdh" ]; then 
      cnt=$(echo "$CLUSTER_LIST" | awk '{ print NF }') 
      if [ $cnt -eq 0 ]; then
        echo "ERROR: No Tanzu Demo Hub (TDH) Environment found, make sure there is an"
        echo "       environment deployed"
        exit
      fi
  
      if [ $cnt -gt 1 ]; then
      echo "WARNING: Multiple clusters found with Tanzu Demo Hub (TDH) environments, please chose the desired one"
        echo "         and restart the demo."
       # awk -F',' '{ printf("         => ./%s --type %s --version %s --context %s\n",a,$1,$2,$3) }' a="$CMD_EXEC" $CLUSTER_LIST
        echo ""
        exit
      else
        clu=$(echo $CLUSTER_LIST | awk '{ print $1 }') 
        export KUBECONFIG=$HOME/.tanzu-demo-hub/config/${clu}.kubeconfig
      fi
    fi
  
    echo
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: runTDHtools
# Function Purpose ...: Install Contour Ingress Controller
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: TDH Tools Flag (TCE/TCE)
#          ($2) ......: Header Text to be shown on start
#          ($3) ......: Commandline PATH and Command to be executed by docker
#          ($4) ......: Commandline Arguments
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 on Success, 1 on failure
# ------------------------------------------------------------------------------------------
runTDHtools() {
  TOOLS_TLAG="$1"
  TOOLS_VERSION="$2"
  HEADER_TEXT="$3"
  DOCKER_EXEC="$4"
  DOCKER_ARGS="$5"

  if [ "$TOOLS_TLAG" == "tce" -o "$TOOLS_TLAG" == "TCE" ]; then 
    TDH_TOOLS=tdh-tools-tce-$TOOLS_VERSION
    POSTSCRIPT="tdh-postinstall-user.sh"
  fi

  if [ "$TOOLS_TLAG" == "tkg" -o "$TOOLS_TLAG" == "TKG" ]; then 
    TDH_TOOLS=tdh-tools-tkg-$TOOLS_VERSION
    POSTSCRIPT="tdh-postinstall-user.sh"
  fi

  if [ "$TOOLS_TLAG" == "tap" -o "$TOOLS_TLAG" == "TAP" ]; then 
    TDH_TOOLS=tdh-tools-tap-$TOOLS_VERSION
    POSTSCRIPT="tdh-postinstall-user-tap.sh"
  fi

  TDH_TOOLS_PATH=".${TDH_TOOLS}"

  if [ ! -f /tdh_tools_docker_container  ]; then
    tdhHeader "$HEADER_TEXT"

    cnt=$(docker ps | grep -c "tdh-tools-${TOOLS_TLAG}-${TOOLS_VERSION}:latest")
    if [ $cnt -gt 0 ]; then 
      echo ""
      echo "ERROR: Docker Container tdh-tools-${TOOLS_TLAG}-${TOOLS_VERSION}:latest is already running"
      echo "       please wait until the other task has been completed"
      echo "       => docker ps"
      messageLine
      docker ps
      messageLine
      exit 1
    fi

    # --- VERIFY TOOLS AND ACCESS ---
    verify_docker
    checkCLIcommands        BASIC
    checkKubernetesServices registry_docker

    if [ $NATIVE -eq 0 ]; then
      tdh_tools_build $TOOLS_TLAG $TOOLS_VERSION
      messagePrint " ▪ Running TDH Tools Docker Container" "$TDH_TOOLS:latest $TDHPATH/$CMD_EXEC $CMD_ARGS"

      cid=$(docker ps -a | grep $TDH_TOOLS:latest | awk '{ print $1 }')
      [ "$cid" != "" ] && for n in $cid; do docker rm $n -f > /dev/null 2>&1; done

      # --- DUMP ENVIRONMENT VARIABLES ---
      env | grep TDH > /tmp/tdh.env

      export CORE_OPTIONS="-it --init --rm --hostname tdh-tools --name $TDH_TOOLS --network=host"
      export USER_OPTIONS="$CORE_OPTIONS -u $(id -u):$(id -g) --env-file /tmp/tdh.env -e \"KUBECONFIG=$HOME/.kube/config\""
      export ROOT_OPTIONS="$CORE_OPTIONS --env-file /tmp/tdh.env -e \"KUBECONFIG=$HOME/.kube/config\""

      export CORE_MOUNTS=(
               "-v /var/run/docker.sock:/var/run/docker.sock"                           ## REQIORED FOR DOCKER
               "-v $HOME/.tanzu-demo-hub:$HOME/.tanzu-demo-hub:rw"
               "-v $HOME/.tanzu-demo-hub.cfg:$HOME/.tanzu-demo-hub.cfg:ro"
               "-v $HOME/$TDH_TOOLS_PATH/.cache:$HOME/.cache:rw"
               "-v $HOME/$TDH_TOOLS_PATH/.config:$HOME/.config:rw"                      ## CONFIG FOR HELM AND TANZU
               "-v $HOME/$TDH_TOOLS_PATH/.kube:$HOME/.kube:rw"                          ## KUBERNETES CLUSTER CONTEXTS
               "-v $HOME/$TDH_TOOLS_PATH/.kube-tkg:$HOME/.kube-tkg:rw"                  ## TANZU MANAGEMENT CLUSTER CONTEXTS
               "-v $HOME/$TDH_TOOLS_PATH/.local:$HOME/.local:rw"
               "-v $HOME/$TDH_TOOLS_PATH/.tanzu:$HOME/.tanzu:rw"
               "-v $HOME/$TDH_TOOLS_PATH/.ssh:$HOME/.ssh:rw"                            ## SSH to AWS/AZURE AMDIN SERVER
               "-v $HOME/$TDH_TOOLS_PATH/.terraform:$HOME/.terraform:rw"
               "-v $HOME/$TDH_TOOLS_PATH/.terraform.d:$HOME/.terraform.d:rw"
               "-v $HOME/$TDH_TOOLS_PATH/.s3cfg:$HOME/.s3cfg:rw"
               "-v $HOME/$TDH_TOOLS_PATH/.govmomi:$HOME/.govmomi:rw"
               "-v $HOME/$TDH_TOOLS_PATH/.gradle:$HOME/.gradle:rw"
               "-v $HOME/$TDH_TOOLS_PATH/.docker:$HOME/.docker:rw"                      ## DOCKER LOGIN CREDENTIALS
               "-v $HOME/$TDH_TOOLS_PATH/.mvn:$HOME/.mvn:rw"
               "-v $HOME/$TDH_TOOLS_PATH/.aws:$HOME/.aws:rw"                            ## PERSISTANT AWS CREDENTIALS
               "-v $HOME/$TDH_TOOLS_PATH/.azure:$HOME/.azure:rw"                        ## PERSISTANT MICROSOFT AZURE CREDENTIALS
               "-v $HOME/$TDH_TOOLS_PATH/.vmware-cna-saas:$HOME/.vmware-cna-saas:rw"    ## TANZU MISSION CONTROL (TMC) LOGIN CREDENTIALS
               "-v $HOME/$TDH_TOOLS_PATH/tmp:/tmp:rw"                                   ## KEEP TEMP PERSISTENT
               "-v $TDHPATH/:$TDHPATH:ro"                                               ## TANZU-DEMO-HUB DIRECTORY
             )

      # --- MAKE SURE DIRECTORIES ARE CREATED ---
      for n in $(echo ${CORE_MOUNTS[*]} | sed 's/\-. //g'); do
        [ $n == "/var/run/docker.sock" ] && continue
        localdir=$(echo $n | awk -F: '{ print $1 }')
        [ ! -e $localdir ] && mkdir -p $localdir && chmod 777 $localdir 
      done

      docker run $ROOT_OPTIONS ${CORE_MOUNTS[*]} $TDH_TOOLS:latest chmod 666 /var/run/docker.sock > /dev/null 2>&1
      docker run $USER_OPTIONS ${CORE_MOUNTS[*]} $TDH_TOOLS:latest /usr/local/bin/$POSTSCRIPT > /dev/null 2>&1
      [ $ROOT_SHELL -eq 1 ] && SEXEC_OPTIONS=$ROOT_OPTIONS || SEXEC_OPTIONS=$USER_OPTIONS
      docker run $SEXEC_OPTIONS ${CORE_MOUNTS[*]} $TDH_TOOLS:latest $DOCKER_EXEC $DOCKER_ARGS; ret=$?
  
      # --- FINISH CURRENT SESSION AS WE RUN AS CONTAINER ---
      exit $ret
    fi
  else
    # trap ctrl-c and call ctrl_c()
    #trap ctrl_c INT

    cd $TDHDEMO
  fi
}

tdhHeader() {
  TEXT=$1
  echo ""
  echo "Tanzu Demo Hub - $TEXT"
  echo "by Sacha Dubois, VMware Inc,"
  messageLine
}

logMessages() {
  LOGFILE=$1
  messageLine 1>&2
  cat $LOGFILE 1>&2
  messageLine 1>&2
}

# ------------------------------------------------------------------------------------------
# Function Name ......: nodeType
# Function Purpose ...: Convert Node Cathegory (S,M,L) into Cloud Instance Type
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Node Cathegory (S=Small, M=Medium, L=Large Nodes
# Dependancies: ......: deployment/tkg-tanzu-demo-hub.cfg TDH_TKGWC_CONTROPLANE_NODETYPE
#                     : deployment/tmc-aws-awshosted.cfg  nodes[0] variable
# ------------------------------------------------------------------------------------------
# Return Value .......: AWS, Azure, vSphere Node Type (ie. m5.xlarge or Standard_D2s_v3 
# ------------------------------------------------------------------------------------------
nodeType() {
  NODETYPE=$1
  ntyoe=0

  [ "$NODETYPE" == "S" -o "$NODETYPE" == "s" ] && ntyoe=0
  [ "$NODETYPE" == "M" -o "$NODETYPE" == "m" ] && ntyoe=1
  [ "$NODETYPE" == "L" -o "$NODETYPE" == "l" ] && ntyoe=2

  echo "${nodes[$ntyoe]}"
}

#https://vcsa-01.haas-464.pez.vmware.com/ui/app/devcenter/api-explorer

xxx() {
    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      helm install kubeapps --namespace $NAMESPACE bitnami/kubeapps -f $HELM_VALUES --version $HELM_KUBEAPPS_VERSION --wait --wait-for-jobs --timeout 10m > /tmp/error.log 2>&1; ret=$?

      [ $ret -eq 0 ] && break
      let cnt=cnt+1
      sleep 30
    done

    if [ $ret -ne 0 ]; then
      logMessages /tmp/error.log
      echo "ERROR: Failed to deploy Kubeapps Helm Chart"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ helm install kubeapps --namespace $NAMESPACE bitnami/kubeapps -f $HELM_VALUES"
        echo "          tdh-tools:/$ exit"
      else
        echo "       => helm install kubeapps --namespace $NAMESPACE bitnami/kubeapps -f $HELM_VALUES"
      fi

      exit 1
    fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: tmcGetKubernetesImage
# Function Purpose ...: Get the Kubernetes Version
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Kubernetes search string 1.20 or 1.20.2
# ------------------------------------------------------------------------------------------
# Return Value .......: Kubernetes Image ie. 1.18.10-1-amazon2 
# ------------------------------------------------------------------------------------------
tmcGetKubernetesImage() {
  VERSION=$1

  echo $TMC_K8S_VERSIONS | awk 'BEGIN{a=""}{ for(i=1; i<=NF; i++) { printf("%s\n",$i) } }' | \
  egrep "^$VERSION" | head -1
}

# ------------------------------------------------------------------------------------------
# Function Name ......: dockerPullImages
# Function Purpose ...: Get the Kubernetes Version
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Kubernetes search string 1.20 or 1.20.2
# ------------------------------------------------------------------------------------------
# Return Value .......: Kubernetes Image ie. 1.18.10-1-amazon2
# ------------------------------------------------------------------------------------------
dockerPullImages() {
  list="$1"

  for n in $list; do
    messagePrint " ▪ Pull Docker Image"    "$n"

    cnt=0; ret=1
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do
      docker pull $n > /dev/null 2>&1; ret=$?
      [ $ret -eq 0 ] && break
      sleep 60
      let cnt=cnt+1
    done
  done
}

# ------------------------------------------------------------------------------------------
# Function Name ......: deleteFailedPods
# Function Purpose ...: Waiting for system Services to have completed starting
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Namespace
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
deleteFailedPods() {
  NAMESPACE=$1

  for n in $(cmdLoop kubectl -n $NAMESPACE get pods | egrep "Evicted|Error|CrashLoopBackOff" | awk '{ print $1 }'); do
    cmdLoop kubectl -n $NAMESPACE delete pod $n > /dev/null 2>&1
  done
}

# ------------------------------------------------------------------------------------------
# Function Name ......: runningPods
# Function Purpose ...: Waiting for system Services to have completed starting
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Namespace
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
runningPods() {
  NAMESPACE=$1

  cmdLoop kubectl get pods -A -o json > /tmp/output.yaml
  tot=$(jq -r --arg key "$NAMESPACE" '.items[] | select(.metadata.namespace == $key).status.phase' /tmp/output.yaml | \
        egrep -v "Evicted|Failed" | wc -l | awk '{ print $1 }')
  cnt=$(jq -r --arg key "$NAMESPACE" '.items[] | select(.metadata.namespace == $key).status.phase' /tmp/output.yaml | \
        egrep -vc "Running|Completed|Succeeded|Evicted|Failed")

  let bsy=tot-cnt

  messageCountInit " ▪ Waiting for pods to start in namespace $NAMESPACE" $tot $bsy

  cnt=1; 
  while [ $cnt -ne 0 ]; do
    cmdLoop kubectl get pods -A -o json > /tmp/output.yaml
    cnt=$(jq -r --arg key "$NAMESPACE" '.items[] | select(.metadata.namespace == $key).status.phase' /tmp/output.yaml | \
        egrep -vc "Running|Completed|Succeeded|Evicted")

    let bsy=tot-cnt
    messageCount $bsy

    [ $cnt -eq 0 ] && break

    sleep 60
  done

  messageCountFine
}

# ------------------------------------------------------------------------------------------
# Function Name ......: waitForKappServices
# Function Purpose ...: Waiting for Kapp Services to have completed starting
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Namespace
#          ($2) ......: Kapp Service
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
waitForKappServices_old() {
  SVC=$2
  NAMESERVICE=$1

  messagePrint " ▪ Waiting for kapp services completed starting"               "$SVC"
  stt="false"; ret=0
  while [ "$stt" != "true" -a $ret -lt 15 ]; do
    cmdLoop kapp list -n $NAMESERVICE --json > /tmp/output.json
    stt=$(jq -r --arg key $SVC '.Tables[].Rows[] | select(.name == $key).last_change_successful' /tmp/output.json  2>/dev/null)
    [ "$stt" == "true" ] && break
    sleep 60
    let ret=ret+1
  done

  if [ "$stt" != "true"  ]; then
    messageLine
    echo "ERROR: Kapp service ($SVC) in namespace $NAMESERVICE failed to start"
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ kapp list -n $NAMESERVICE"
      echo "          tdh-tools:/$ kapp inspect -n $NAMESERVICE -a $SVC"
      echo "          tdh-tools:/$ kapp logs -n $NAMESERVICE -a $SVC"
      echo "          tdh-tools:/$ exit"
    else
      echo "       => kapp list -n $NAMESERVICE"
      echo "       => kapp inspect -n $NAMESERVICE -a $SVC"
      echo "       => kapp logs -n $NAMESERVICE -a $SVC"
    fi
    messageLine
    exit 1
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: waitForKappServices
# Function Purpose ...: Waiting for Kapp Services to have completed starting
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Namespace
#          ($2) ......: Kapp Service
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
waitForKappServices() {
echo idiot
  SVC=$2
  NAMESERVICE=$1

  messagePrint " ▪ Waiting for kapp services completed starting"               "$SVC"

  cnt=1; ret=0
  while [ $cnt -ne 0 -a $ret -lt 15 ]; do
    cmdLoop kapp inspect -a $SVC -n $NAMESERVICE --json > /tmp/output.json
    cnt=$(jq -r '.Tables[].Rows[] | select(.reconcile_state != "ok").name' /tmp/output.json 2>/dev/null | wc -l | awk '{ print $1 }')
echo "CNT:$cnt"
    [ $cnt -eq 0 ] && break
    sleep 60
    let ret=ret+1
  done

echo "CNT:$cnt"
  if [ $cnt -ne 0 ]; then
    messageLine
    echo "ERROR: Kapp service ($SVC) in namespace $NAMESERVICE failed to start"
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh"
      echo "          tdh-tools:/$ kapp list -n $NAMESERVICE"
      echo "          tdh-tools:/$ kapp inspect -n $NAMESERVICE -a $SVC"
      echo "          tdh-tools:/$ kapp logs -n $NAMESERVICE -a $SVC"
      echo "          tdh-tools:/$ exit"
    else
      echo "       => kapp list -n $NAMESERVICE"
      echo "       => kapp inspect -n $NAMESERVICE -a $SVC"
      echo "       => kapp logs -n $NAMESERVICE -a $SVC"
    fi
    messageLine
    exit 1
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: createServiceAccount
# Function Purpose ...: Create a new Service Account (delete if it already exists)
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Namespace
#          ($2) ......: Service Account name
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
createServiceAccount() {
  TMP_NAMESPACE=$1
  SERVICE_ACCOUNT=$2

  cmdLoop kubectl --namespace $TMP_NAMESPACE get serviceaccounts -o json > /tmp/output.json
  if [ -s /tmp/output.json ]; then 
    sa=$(jq -r --arg key "$SERVICE_ACCOUNT" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
    if [ "$sa" != "" ]; then 
      cmdLoop kubectl delete --namespace $TMP_NAMESPACE serviceaccount $SERVICE_ACCOUNT > /dev/null 2>&1
    fi
  fi

  cmdLoop kubectl create --namespace $TMP_NAMESPACE serviceaccount $SERVICE_ACCOUNT > /dev/null 2>&1

  # --- WAIT UNTIL SERVICE ACCOUNT HAS BEEN CREATED ---
  uid=""; cnt=0
  while [ "$uid" != "" -a $cnt -lt 5 ]; do 
    cmdLoop kubectl get serviceaccount -n $TMP_NAMESPACE -o json > /tmp/output.json
    crb=$(jq -r --arg key "$SERVICE_ACCOUNT" '.items[] | select(.metadata.name == $key).metadata.uid' /tmp/output.json) 
    [ "$uid" != "" ] && break

    cnt=cnt+1
    sleep 30
  done
}

# ------------------------------------------------------------------------------------------
# Function Name ......: createClusterRoleBinding
# Function Purpose ...: Create a new Cluster Role Binding  (delete if it already exists)
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Namespace
#          ($2) ......: Service Account name
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
createClusterRoleBinding() {
  CLUSTER_TOLE_BINDING=$1
  CLUSTER_ROLE=$2
  SERVICE_ACCOUNT=$3

  cmdLoop kubectl get clusterrolebinding -o json > /tmp/output.json
  if [ -s /tmp/output.json ]; then
    crb=$(jq -r --arg key "$CLUSTER_TOLE_BINDING" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json) 
    if [ "$crb" != "" ]; then
      cmdLoop kubectl delete clusterrolebinding $CLUSTER_TOLE_BINDING > /dev/null 2>&1
    fi
  fi

  cmdLoop kubectl create clusterrolebinding $CLUSTER_TOLE_BINDING --clusterrole=$CLUSTER_ROLE --serviceaccount=$SERVICE_ACCOUNT
}

# ------------------------------------------------------------------------------------------
# Function Name ......: getServiceAccountSecret
# Function Purpose ...: Get ServiceAccount Secret
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Service Account name
# ------------------------------------------------------------------------------------------
# Return Value .......: Service Account Secret
# ------------------------------------------------------------------------------------------
getServiceAccountSecret() {
  SERVICE_ACCOUNT=$1

  cmdLoop kubectl get serviceaccount $SERVICE_ACCOUNT -o json > /tmp/output.json
  if [ -s /tmp/output.json ]; then
    sec=$(jq -r --arg key "$SERVICE_ACCOUNT" 'select(.metadata.name == $key).secrets[].name' /tmp/output.json)
  fi

  echo $sec
}

# ------------------------------------------------------------------------------------------
# Function Name ......: createNamespace
# Function Purpose ...: Create a Kubernetes Namespace
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Namespace
# ------------------------------------------------------------------------------------------
# Return Value .......: Service Account Secret
# ------------------------------------------------------------------------------------------
createNamespace() {
  NAMESPACE=$1

  cmdLoop kubectl get ns -o json > /tmp/output.json
  if [ -s /tmp/output.json ]; then
    nam=$(jq -r --arg key "$NAMESPACE" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json) 

    if [ "$nam" == "" ]; then 
      cmdLoop kubectl create ns $NAMESPACE
    fi
  fi

  echo $sec
}

# ------------------------------------------------------------------------------------------
# Function Name ......: deleteNamespace
# Function Purpose ...: Delete a Kubernetes Namespace
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Namespace
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - Success, 1 - Failed
# ------------------------------------------------------------------------------------------
deleteNamespace() {
  NAMESPACE=$1

  nam=$NAMESPACE
  while [ "$nam" == "$NAMESPACE" ]; do
    cmdLoop kubectl get ns -o json > /tmp/output.json
    if [ -s /tmp/output.json ]; then
      nam=$(jq -r --arg key "$NAMESPACE" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
  
      if [ "$nam" != "" ]; then
        _stt=$(jq -r --arg key "$NAMESPACE" '.items[] | select(.metadata.name == $key).status.phase' /tmp/output.json)
        if [ "$_stt" == "Terminating" ]; then 
          messagePrint " ▪ Deleting Namespace (force)"     "$NAMESPACE"
          cmdLoop kubectl get ns $NAMESPACE -o json | \
                  jq -r '. | select(.spec.finalizers[] == "kubernetes").spec.finalizers |= []' > /tmp/namespace.json
          cmdLoop kubectl replace --raw "/api/v1/namespaces/$NAMESPACE/finalize" -f /tmp/namespace.json > /dev/null 2>&1
        else
          messagePrint " ▪ Deleting Namespace"     "$NAMESPACE"
          cmdLoop kubectl delete ns $NAMESPACE
        fi
      fi
    fi

    sleep 10
  done
}

# ------------------------------------------------------------------------------------------
# Function Name ......: deleteHelmChart
# Function Purpose ...: Delete a Helm Chart
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Namespace
#          ($2) ......: Helm Chart
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - Success, 1 - Failed
# ------------------------------------------------------------------------------------------
deleteHelmChart() {
  NAMESPACE=$1
  HELMCHART=$2

  cmdLoop helm list -A -o json > /tmp/output.json
  if [ -s /tmp/output.json ]; then
    nam=$(jq -r --arg ns "$NAMESPACE" --arg chart "$HELMCHART" '.[] | select(.namespace == $ns and .name == $chart).name' /tmp/output.json)

    if [ "$nam" != "" ]; then
      cmdLoop helm uninstall -n $NAMESPACE $HELMCHART
    fi
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: deleteSecret
# Function Purpose ...: Delete a Kubernetes Secret
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Namespace
#          ($2) ......: Secret
# ------------------------------------------------------------------------------------------
# Return Value .......: Service Account Secret
# ------------------------------------------------------------------------------------------
deleteSecret() {
  NAMESPACE=$1
  SECRET=$2

  cmdLoop kubectl -n $NAMESPACE get secrets -o json > /tmp/output.json
  if [ -s /tmp/output.json ]; then
    nam=$(jq -r --arg key "$SECRET" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json) 
    if [ "$nam" != "" ]; then
      cmdLoop kubectl -n $NAMESPACE delete secret $SECRET
    fi
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: deleteCertificate
# Function Purpose ...: Delete a Kubernetes Secret
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Namespace
#          ($2) ......: Certificate
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCESS, 1 - FAILURE
# ------------------------------------------------------------------------------------------
deleteCertificate() {
  NAMESPACE=$1
  CERTIFICATE=$2

  cmdLoop kubectl -n $NAMESPACE get certificate -o json > /tmp/output.json
  if [ -s /tmp/output.json ]; then
    nam=$(jq -r --arg key "$CERTIFICATE" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
    if [ "$nam" != "" ]; then
      cmdLoop kubectl -n $NAMESPACE delete certificate $CERTIFICATE
    fi
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: helmRepoAdd
# Function Purpose ...: Add an Helm Repository
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Repo NAme
#          ($2) ......: Repo URL
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCESS, 1 - FAILURE
# ------------------------------------------------------------------------------------------
helmRepoAdd() {
  REPO_NAME=$1
  REPO_URL=$2

  helm repo list > /dev/null 2>&1; ret=$?
  if [ $ret -eq 0 ]; then 
    cmdLoop helm repo list -o json > /tmp/output.json
    if [ -s /tmp/output.json ]; then
      nam=$(jq -r --arg key "$REPO_NAME" '.[] | select(.name == $key).name' /tmp/output.json)
      if [ "$nam" == "" ]; then
        messagePrint " ▪ Add Helm Repo ($REPO_NAME)" "$REPO_URL"
        cmdLoop helm repo add $REPO_NAME $REPO_URL > /dev/null 2>&1
        cmdLoop helm repo update > /dev/null 2>&1
      fi
    fi
  else
    # --- NO REPO CONFIGURED YET ---
    messagePrint " ▪ Add Helm Repo ($REPO_NAME)" "$REPO_URL"
    cmdLoop helm repo add $REPO_NAME $REPO_URL > /dev/null 2>&1
    cmdLoop helm repo update > /dev/null 2>&1
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: copySecretObject
# Function Purpose ...: Copy a secret Object to another namespace
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Source Namespace Name
#          ($2) ......: Target Namespace Name
#          ($3) ......: Secret Name
#          ($4) ......: New Secret Name
# ------------------------------------------------------------------------------------------
# Return Value .......: Service Account Secret
# ------------------------------------------------------------------------------------------
copySecretObject() {
  SOURCE_NAMESPACE=$1
  TARGET_NAMESPACE=$2
  SECRET_NAME=$3
  NEW_SECRET_NAME=$4

  if [ "$NEW_SECRET_NAME" == "" ]; then
    # --- VERIFY IF SECRET ALREADY EXISTS --
    cmdLoop kubectl get secret -n $TARGET_NAMESPACE -o json > /tmp/output.yaml
    if [ -s /tmp/output.yaml ]; then 
      nam=$(jq -r --arg key $SECRET_NAME '.items[].metadata | select(.name == $key).name' /tmp/output.yaml) 
      if [ "$nam" == "$SECRET_NAME" ]; then 
        cmdLoop kubectl -n $TARGET_NAMESPACE delete secret $SECRET_NAME > /dev/null 2>&1
      fi
    fi

    # --- COPY SECRET ---
    cmdLoop kubectl get secret $SECRET_NAME --namespace=default -oyaml | \
            grep -v '^\s*namespace:\s' > /tmp/tmpfile.yaml

    if [ -s /tmp/tmpfile.yaml ]; then 
      cmdLoop kubectl apply --namespace=$TARGET_NAMESPACE -f /tmp/tmpfile.yaml > /dev/null 2>&1
    fi
  else
    # --- VERIFY IF SECRET ALREADY EXISTS --
    cmdLoop kubectl get secret -n $TARGET_NAMESPACE -o json > /tmp/output.yaml
    if [ -s /tmp/output.yaml ]; then
      nam=$(jq -r --arg key $NEW_SECRET_NAME '.items[].metadata | select(.name == $key).name' /tmp/output.yaml)
      if [ "$nam" == "$NEW_SECRET_NAME" ]; then
        cmdLoop kubectl -n $TARGET_NAMESPACE delete secret $NEW_SECRET_NAME > /dev/null 2>&1
      fi
    fi

    # --- COPY SECRET ---
    cmdLoop kubectl get secret $SECRET_NAME --namespace=default -oyaml | \
            grep -v '^\s*namespace:\s' | sed "s/name: $SECRET_NAME/name: $NEW_SECRET_NAME/g" > /tmp/tmpfile.yaml

    if [ -s /tmp/tmpfile.yaml ]; then
      cmdLoop kubectl apply --namespace=$TARGET_NAMESPACE -f /tmp/tmpfile.yaml > /dev/null 2>&1
    fi
  fi
}

waitTanzuPackageReconsile() {
  TANZU_PACKAGE=$1
  TANZU_NAMESPACE=$2

  cmdLoop tanzu package installed list -A -o json > /tmp/output.json 2>/dev/null
  pkg=$(jq -r --arg key "$TANZU_PACKAGE" '.[] | select(.name == $key).name' /tmp/output.json)
  if [ "$pkg" == "$pkg" ]; then
    cnt=0; stt="Reconciling"
    while [ "$stt" != "Reconcile succeeded" -a $cnt -lt 5 ]; do

      tanzu package installed list -A -o json > /tmp/output.json 2>/dev/null
      stt=$(jq -r --arg key "$TANZU_PACKAGE" '.[] | select(.name == $key).status' /tmp/output.json)

      if [ "$stt" != "Reconcile succeeded" -a "$stt" != "Reconciling" ]; then 
        return 1
      else
        [ "$stt" == "Reconcile succeeded" ] && return 0
      fi

      let cnt=cnt+1
      sleep 30
    done
  fi 
}


# ------------------------------------------------------------------------------------------
# Function Name ......: deleteTanzuPackage
# Function Purpose ...: Delete Tanzu Package
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Command to execure
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
deleteTanzuPackage() {
  TANZU_PACKAGE=$1
  TANZU_NAMESPACE=$2

  cmdLoop tanzu package installed list -A -o json > /tmp/output.json 2>/dev/null
  pkg=$(jq -r --arg key "$TANZU_PACKAGE" '.[] | select(.name == $key).name' /tmp/output.json)
  if [ "$pkg" == "$TANZU_PACKAGE" ]; then
    cnt=0; ret=1 
    while [ $ret -ne 0 -a $cnt -lt 5 ]; do

      tanzu package installed delete $TANZU_PACKAGE --namespace $TANZU_NAMESPACE -y > /dev/null 2>&1; ret=$?

      # --- VERIFY IF PACKAGE IS DELETED ---
      cmdLoop tanzu package installed list -A -o json > /tmp/output.json 2>/dev/null
      pkg=$(jq -r --arg key "$TANZU_PACKAGE" '.[] | select(.name == $key).name' /tmp/output.json)
echo "pkg:$pkg"
      if [ "$pkg" != "$TANZU_PACKAGE" ]; then
        # --- DELETE SERVICE ACCOUNT IF IT EXIST ----
        cmdLoop kubectl get serviceaccount -n tanzu-system-packages -o json > /tmp/output.json
        nam=$(jq -r --arg key "${TANZU_PACKAGE}-${TANZU_NAMESPACE}-sa" '.items[] | select(.metadata.name == $key).metadata.name' /tmp/output.json)
echo xxx3
 cmdLoop tanzu package installed list -A 2>/dev/null
echo xxx4

echo "kubectl -n tanzu-system-packages delete serviceaccount $nam"
        [ "$nam" != "" ] && cmdLoop kubectl -n tanzu-system-packages delete serviceaccount $nam
echo xxx5
 cmdLoop tanzu package installed list -A 2>/dev/null
echo xxx6
      else
        break
      fi

      let cnt=cnt+1
      sleep 30
    done

    if [ $ret -ne 0 ]; then
      logMessages /tmp/error.log
      echo "ERROR: Failed to run command, please try manually"
      if [ "$NATIVE" == "0" ]; then
        echo "       => tools/${TDH_TOOLS}.sh"
        echo "          tdh-tools:/$ $CMD"
        echo "          tdh-tools:/$ exit"
      else
        echo "       =>  $CMD"
      fi

      exit 1
    fi
  fi
}


# ------------------------------------------------------------------------------------------
# Function Name ......: cmdLoop
# Function Purpose ...: Execute Command in a Loop until its working
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Command to execure
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
cmdLoop() {
  CMD=$*

  _cnt=0; _ret=1
  while [ $_ret -ne 0 -a $_cnt -lt 5 ]; do
    $CMD > /tmp/loopCmdIgnore.out 2>/tmp/cmdloop_error.log; _ret=$?
    [ $_ret -eq 0 ] && break
    let _cnt=_cnt+1
    sleep 30
  done

  if [ $_ret -ne 0 ]; then
    logMessages /tmp/cmdloop_error.log 1>&2
    echo "ERROR: Failed to run command, please try manually" 1>&2
    if [ "$NATIVE" == "0" ]; then
      echo "       => tools/${TDH_TOOLS}.sh" 1>&2
      echo "          tdh-tools:/$ $CMD" 1>&2
      echo "          tdh-tools:/$ exit" 1>&2
    else
      echo "       =>  $CMD" 1>&2
    fi

    return 1
  fi

  cat /tmp/loopCmdIgnore.out
  return 0
}

# ------------------------------------------------------------------------------------------
# Function Name ......: listTMCDeployments
# Function Purpose ...: List TMC Deploymnets
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: n/a
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
listTMCDeployments() {
  echo
  printf "%-31s %-7s %-35s %s\n" "TMC-DEPLOYMENT" "CLOUD" "MANAGEMENT-CLUSTER" "STATUS"
  messageLine

  for deployment in $(ls -1 ${TDHPATH}/deployments/tmc-*.cfg ${HOME}/.tanzu-demo-hub/config/tmc* 2>/dev/null ); do
    TDH_TKGMC_INFRASTRUCTURE=$(egrep "^TDH_TKGMC_INFRASTRUCTURE=" $deployment | awk -F'=' '{ print $2 }' | sed 's/"//g')
    TDH_TKGMC_ENVNAME=$(egrep "^TDH_TKGMC_ENVNAME=" $deployment | awk -F'=' '{ print $2 }' | sed 's/"//g')
    TDH_DEPLOYMENT_CLOUD=$(egrep "^TDH_DEPLOYMENT_CLOUD=" $deployment | awk -F'=' '{ print $2 }' | sed 's/"//g')
    TDH_MANAGEMENT_CLUSTER=$(egrep "^TDH_MANAGEMENT_CLUSTER=" $deployment | awk -F'=' '{ print $2 }')

    cmdLoop tmc managementcluster list -o json > /tmp/output.json
    nam=$(jq -r --arg key "$TDH_MANAGEMENT_CLUSTER" '.managementClusters[] | select(.fullName.name == $key).fullName.name' /tmp/output.json)
    if [ "$nam" == "$TDH_MANAGEMENT_CLUSTER" ]; then
      st1=$(jq -r --arg key "$TDH_MANAGEMENT_CLUSTER" '.managementClusters[] | select(.fullName.name == $key).status.phase' /tmp/output.json)
      st2=$(jq -r --arg key "$TDH_MANAGEMENT_CLUSTER" '.managementClusters[] | select(.fullName.name == $key).status.health' /tmp/output.json)

      #DELETEME
      #st1=$(tmc managementcluster get $TDH_MANAGEMENT_CLUSTER -o json | jq -r '.status.phase')
      #st2=$(tmc managementcluster get $TDH_MANAGEMENT_CLUSTER -o json | jq -r '.status.health')
      [ "$st1" == "null" -o "$st1" == "" ] && stt="NOT READY / $st2" || stt="$st1 / $st2"
    else
      stt="unavailable"
    fi

    dep=$(basename $deployment)

    printf "%-31s %-7s %-35s %s\n" $dep $TDH_DEPLOYMENT_CLOUD $TDH_MANAGEMENT_CLUSTER "$stt"
  done

  messageLine
}

# ------------------------------------------------------------------------------------------
# Function Name ......: listTKGmc
# Function Purpose ...: List TMC Deploymnets
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: n/a
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
listTKGmc() {
  echo
  printf "%-40s %-7s %-32s %8s  %s\n" "TKG-DEPLOYMENT" "CLOUD" "MANAGEMENT-CLUSTER" "VERSION" "STATUS"
  messageLine

  for deployment in $(ls -1 $HOME/.tanzu-demo-hub/config/tkgmc*.cfg $HOME/.tanzu-demo-hub/config/tcemc*.cfg 2>/dev/null ); do
    TDH_TKGMC_INFRASTRUCTURE=$(egrep "^TDH_TKGMC_INFRASTRUCTURE=" $deployment | awk -F'=' '{ print $2 }' | sed 's/"//g')
    TDH_TKGMC_ENVNAME=$(egrep "^TDH_TKGMC_ENVNAME=" $deployment | awk -F'=' '{ print $2 }' | sed 's/"//g')
    TDH_TKGMC_NAME=$(egrep "^TDH_TKGMC_NAME=" $deployment | awk -F'=' '{ print $2 }' | sed 's/"//g')
    TDH_TKGMC_TOOLS_CONTAINER=$(egrep "^TDH_TKGMC_TOOLS_CONTAINER=" $deployment | awk -F'=' '{ print $2 }' | sed 's/"//g' | awk -F'-' '{ print $NF }')
    TDH_TKGMC_KUBECONFIG=$(egrep "^TDH_TKGMC_KUBECONFIG=" $deployment | awk -F'=' '{ print $2 }' | sed 's/"//g')

    kubectl get nodes --kubeconfig $HOME/.tanzu-demo-hub/config/$TDH_TKGMC_KUBECONFIG > /dev/null 2>&1; ret=$? 
    if [ $ret -ne 0 ]; then
      stt="configured / unavailable"
    else
      stt="configured / ready"
    fi

    dep=$(basename $deployment)

    printf "%-40s %-7s %-32s %8s  %s\n" $dep $TDH_TKGMC_INFRASTRUCTURE $TDH_TKGMC_NAME "$TDH_TKGMC_TOOLS_CONTAINER" "$stt"

  done

  messageLine
}


# ------------------------------------------------------------------------------------------
# Function Name ......: listClusters
# Function Purpose ...: List Clusters
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: n/a
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
listClusters() {
  #cat /tmp/2 | jq -r '.spec | select(.provisionedcluster.accountName == "smidgley-aws").provisionedcluster.accountName'
  cnt=$(tmc cluster list --group sadubois --output json | jq -r '."totalCount"')
  cnt=$(tmc cluster list --output json | jq -r '."totalCount"')
  if [ $cnt -gt 0 ]; then
    TMPFILE=/tmp/tdh_listCluster.tmp; rm -f $TMPFILE

    echo "NAME                 KUBERNETES           PROVIDER   CREDENTIALS          REGION          STATE"
    echo "-----------------------------------------------------------------------------------------------------------"

    tmc cluster list --group $TMC_CLUSTER_GROUP --output json > $TMPFILE
    tmc cluster list --output json > $TMPFILE
    for cln in $(jq -r '.clusters[] | select(.status.type == "PROVISIONED").fullName.name' $TMPFILE | head -5); do
      ver=$(jq -r --arg cluster "$cln" '.clusters[] | select(.fullName.name == $cluster)' $TMPFILE | \
            jq -r '.spec.provisionedcluster.version')
      acc=$(jq -r --arg cluster "$cln" '.clusters[] | select(.fullName.name == $cluster)' $TMPFILE | \
            jq -r '.spec.provisionedcluster.accountName')
      cpv=$(jq -r --arg cluster "$cln" '.clusters[] | select(.fullName.name == $cluster)' $TMPFILE | \
            jq -r '.status.agent.metadata.cloudProvider')
      reg=$(jq -r --arg cluster "$cln" '.clusters[] | select(.fullName.name == $cluster)' $TMPFILE | \
            jq -r '.status.agent.metadata.region')
      stt=$(jq -r --arg cluster "$cln" '.clusters[] | select(.fullName.name == $cluster)' $TMPFILE | \
            jq -r '.status.status.state.state')

      printf "%-20s %-20s %-10s %-20s %-15s %-10s\n" $cln $ver $cpv $acc $reg $stt
    done

    # --- CLEANUP ---
    rm -f $TMPFILE
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: listClusterConfig
# Function Purpose ...: List Clusters Config
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: n/a
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
listClusterConfig() {
  echo
  printf "%-40s %-7s %-15s %-32s %-5s %s\n" "TKG-CONFIGURATION" "PLAN" "DESCRIPTION"
  messageLine

  for deployment in $(ls -1 ${TDHPATH}/deployments/${TDH_TOOLS_CONTAINER_TYPE}-*.cfg | grep -v "${TDH_TOOLS_CONTAINER_TYPE}mc-"); do
    TDH_DEPLOYMENT_DESCRIPTION=$(egrep "^TDH_DEPLOYMENT_DESCRIPTION=" $deployment | awk -F'=' '{ print $2 }' | sed 's/"//g')
    TDH_DEPLOYMENT_CLUSTER_PLAN=$(egrep "^TDH_DEPLOYMENT_CLUSTER_PLAN=" $deployment | awk -F'=' '{ print $2 }')

    dep=$(basename $deployment)

    printf "%-40s %-7s %-15s %-32s %-5s %s\n" $dep "$TDH_DEPLOYMENT_CLUSTER_PLAN" "$TDH_DEPLOYMENT_DESCRIPTION"
  done

  messageLine
}

# ------------------------------------------------------------------------------------------
# Function Name ......: listTKGmcDeployments
# Function Purpose ...: List TKC Management Cluster Deployments
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: n/a
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
listTKGmcDeployments() {
  echo ""
  printf "%-30s %-7s %-7s %-30s %-5s %s\n" "CONFIURATION" "CLOUD" "DOMAIN" "MGMT-CLUSTER" "PLAN" "CONFIGURATION"
  echo "-----------------------------------------------------------------------------------------------------------"

  for deployment in $(ls -1 ${TDHPATH}/deployments/tkgmc*.cfg); do
    . $deployment

    dep=$(basename $deployment)

    printf "%-30s %-7s %-7s %-30s %-5s %s\n" $dep $TDH_TKGMC_INFRASTRUCTURE ${TDH_TKGMC_ENVNAME} "${TDH_TKGMC_NAME}-<TDH_USER>" \
           $TDH_TKGMC_PLAN "$TDH_TKGMC_CONFIG"
  done

  echo "-----------------------------------------------------------------------------------------------------------"
}

# ------------------------------------------------------------------------------------------
# Function Name ......: listTCEmcDeployments  
# Function Purpose ...: List TCE Management Cluster Deployments
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: n/a
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
listTCEmcDeployments() {
  echo ""
  printf "%-30s %-7s %-7s %-30s %-5s %s\n" "CONFIURATION" "CLOUD" "DOMAIN" "MGMT-CLUSTER" "PLAN" "CONFIGURATION"
  echo "-----------------------------------------------------------------------------------------------------------"

  for deployment in $(ls -1 ${TDHPATH}/deployments/tcemc*.cfg); do
    . $deployment

    dep=$(basename $deployment)

    printf "%-30s %-7s %-7s %-30s %-5s %s\n" $dep $TDH_TKGMC_INFRASTRUCTURE ${TDH_TKGMC_ENVNAME} "${TDH_TKGMC_NAME}-<TDH_USER>" \
           $TDH_TKGMC_PLAN "$TDH_TKGMC_CONFIG"
  done

  echo "-----------------------------------------------------------------------------------------------------------"
}

# ------------------------------------------------------------------------------------------
# Function Name ......: dockerPullSecret
# Function Purpose ...: List TCE Management Cluster Deployments
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Namespace
#          ($2) ......: Service Account
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
dockerPullSecret() {
  TMP_NAMESPACE=$1
  TMP_SERVICEACCOUNT=$2

  [ "$TMP_SERVICEACCOUNT" == "" ] && TMP_SERVICEACCOUNT=default

  cmdLoop kubectl -n $TMP_NAMESPACE get secret -o json > /tmp/output.json
  nam=$(jq -r --arg key "tdh-docker-repo" '.items[].metadata | select(.name == $key).name' /tmp/output.json) 
  if [ "$nam" != "tdh-docker-repo" ]; then 
    messagePrint " ▪ Create Docker secret"  "tdh-docker-repo"
    cmdLoop kubectl -n $TMP_NAMESPACE create secret docker-registry tdh-docker-repo \
            --docker-server $TDH_REGISTRY_DOCKER_NAME \
            --docker-username $TDH_REGISTRY_DOCKER_USER \
            --docker-password $TDH_REGISTRY_DOCKER_PASS > /dev/null 2>&1
  fi
  
  cmdLoop kubectl -n $TMP_NAMESPACE get sa -o json > /tmp/output.json
  san=$(jq -r --arg key "$TMP_SERVICEACCOUNT" '.items[].metadata | select(.name == $key).name' /tmp/output.json)
  if [ "$san" == "$TMP_SERVICEACCOUNT" ]; then
    kubectl get serviceaccount $TMP_SERVICEACCOUNT -n $TMP_NAMESPACE -o yaml > /tmp/serviceaccount.yaml
    echo "imagePullSecrets:"              >> /tmp/serviceaccount.yaml
    echo "- name: tdh-docker-repo"        >> /tmp/serviceaccount.yaml
  
    kubectl apply -f /tmp/serviceaccount.yaml > /dev/null 2>&1
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: detachClusterFromTMConFailure
# Function Purpose ...: Detaching a TKG Cluster from TMC on bad status
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Cluster Name
#          ($2) ......: Provisioner
#          ($3) ......: Management Cluster
# ------------------------------------------------------------------------------------------
# Return Value .......: 0 - SUCCEED, 1 - FAILED
# ------------------------------------------------------------------------------------------
# Example ............: detachClusterFromTMConFailure tdh-azure-sadubois attached attached
#                     : detachClusterFromTMConFailure tdh-vsphere-sadubois attached attached
# ------------------------------------------------------------------------------------------
detachClusterFromTMConFailure() {
  clusterName=$1
  provisioner=$2
  mgmtCluster=$3
  flag="$4"

  #########################################################################################################################
  ############################################ FORCE DELETE OF CLUSTER  ###################################################
  #########################################################################################################################
  cmdLoop tmc cluster list --name $clusterName -p $provisioner -m $mgmtCluster -o json > /tmp/output.json
  nam=$(jq -r --arg clu "$clusterName" --arg pvn "$provisioner" --arg mcl "$mgmtCluster" \
        '.clusters[] | select(.fullName.name == $clu and .fullName.provisionerName == $pvn and .fullName.managementClusterName == $mcl).fullName.name' 2>/dev/null /tmp/output.json) 
  if [ "$nam" == "$clusterName" ]; then
    # --- CLUSTER EXISTS - CHECK STATUS ---
    stt=$(jq -r --arg clu "$clusterName" --arg pvn "$provisioner" --arg mcl "$mgmtCluster" \
          '.clusters[] | select(.fullName.name == $clu and .fullName.provisionerName == $pvn and .fullName.managementClusterName == $mcl).status.phase' 2>/dev/null /tmp/output.json) 
    hlt=$(jq -r --arg clu "$clusterName" --arg pvn "$provisioner" --arg mcl "$mgmtCluster" \
          '.clusters[] | select(.fullName.name == $clu and .fullName.provisionerName == $pvn and .fullName.managementClusterName == $mcl).status.health' 2>/dev/null /tmp/output.json) 

    [ "$flag" == "force" ] && stt="FAILED"
    if [ ! "$stt" == "READY" -o ! "$hlt" == "HEALTHY" ]; then
      tmc cluster delete $clusterName -p $provisioner -m $mgmtCluster > /dev/null 2>&1

      # --- WAIT FOR THE CLUSTER TO BE DELETED ---
      nam="$clusterName"; ccc=0
      while [ "$nam" == "$clusterName" -a $ccc -lt 5 ]; do
        cmdLoop tmc cluster list --name $clusterName -p $provisioner -m $mgmtCluster -o json > /tmp/output.json
        nam=$(jq -r --arg clu "$clusterName" --arg pvn "$provisioner" --arg mcl "$mgmtCluster" \
              '.clusters[] | select(.fullName.name == $clu and .fullName.provisionerName == $pvn and .fullName.managementClusterName == $mcl).fullName.name' 2>/dev/null /tmp/output.json) 
        [ "$nam" != "$clusterName" ] && break
        let ccc=ccc+1
        sleep 60
      done
    fi
  fi

  #########################################################################################################################
  ############################################ FORCE DELETE OF CLUSTER  ###################################################
  #########################################################################################################################
  cmdLoop tmc cluster list --name $clusterName -p $provisioner -m $mgmtCluster -o json > /tmp/output.json
  nam=$(jq -r --arg clu "$clusterName" --arg pvn "$provisioner" --arg mcl "$mgmtCluster" \
        '.clusters[] | select(.fullName.name == $clu and .fullName.provisionerName == $pvn and .fullName.managementClusterName == $mcl).fullName.name' 2>/dev/null /tmp/output.json) 
  if [ "$nam" == "$clusterName" ]; then
    nam=$(jq -r --arg clu "$clusterName" --arg pvn "$provisioner" --arg mcl "$mgmtCluster" \
          '.clusters[] | select(.fullName.name == $clu and .fullName.provisionerName == $pvn and .fullName.managementClusterName == $mcl).fullName.name' /tmp/output.json)
    stt=$(jq -r --arg clu "$clusterName" --arg pvn "$provisioner" --arg mcl "$mgmtCluster" \
          '.clusters[] | select(.fullName.name == $clu and .fullName.provisionerName == $pvn and .fullName.managementClusterName == $mcl).status.phase' 2>/dev/null /tmp/output.json)
    hlt=$(jq -r --arg clu "$clusterName" --arg pvn "$provisioner" --arg mcl "$mgmtCluster" \
          '.clusters[] | select(.fullName.name == $clu and .fullName.provisionerName == $pvn and .fullName.managementClusterName == $mcl).status.health' 2>/dev/null /tmp/output.json)
    if [ ! "$stt" == "READY" -o ! "$hlt" == "HEALTHY" ]; then
      tmc cluster delete $clusterName -p $provisioner -m $mgmtCluster -f > /dev/null 2>&1

      # --- WAIT FOR THE CLUSTER TO BE DELETED ---
      nam="$clusterName"; ccc=0
      while [ "$nam" == "$clusterName" -a $ccc -lt 5 ]; do
        cmdLoop tmc cluster list --name $clusterName -p $provisioner -m $mgmtCluster -o json > /tmp/output.json
        nam=$(jq -r --arg clu "$clusterName" --arg pvn "$provisioner" --arg mcl "$mgmtCluster" \
              '.clusters[] | select(.fullName.name == $clu and .fullName.provisionerName == $pvn and .fullName.managementClusterName == $mcl).fullName.name ' /tmp/output.json)
        [ "$nam" != "$clusterName" ] && break
        let ccc=ccc+1
        sleep 30
      done
    fi

    #########################################################################################################################
    ######################################### CLEANUP NAMESPACE IF STILL THERE ##############################################
    #########################################################################################################################
    #deleteNamespace vmware-system-tmc 
  fi
}

# ------------------------------------------------------------------------------------------
# Function Name ......: attachClusterToTMC
# Function Purpose ...: Attaching a TKG Cluster to TMC 
# ------------------------------------------------------------------------------------------
# Argument ($1) ......: Cluster Name
#          ($2) ......: Provisioner
#          ($3) ......: Management Cluster
#          ($4) ......: Kubeconfig
# ------------------------------------------------------------------------------------------
# Return Value .......: Status (HEALTY, DETACHING, FAILED) 
# ------------------------------------------------------------------------------------------
# Example: attachClusterToTMC tdh-azure-sadubois attached attached $HOME/.tanzu-demo-hub/config/tdh-azure-sadubois.kubeconfig 
# ------------------------------------------------------------------------------------------
attachClusterToTMC() {
  clusterName=$1
  provisioner=$2
  mgmtCluster=$3
  kubeconfig=$4

  #########################################################################################################################
  ######################################### VERIFY CLUSTER AVAIABLILITY ###################################################
  #########################################################################################################################
  kubectl --kubeconfig=$kubeconfig --request-timeout 1s get ns > /dev/null 2>&1; ret=$?
  [ $ret -ne 0 ] && echo "UNREACHABLE" && return 1

  #########################################################################################################################
  ########################################## DELETE CLUSTER IN FAILED STATE ###############################################
  #########################################################################################################################
  detachClusterFromTMConFailure $clusterName $provisioner $mgmtCluster
  
  #########################################################################################################################
  ################################################ ATTACH CLUSTER TO TMC ##################################################
  #########################################################################################################################
  cmdLoop tmc cluster list --name $clusterName -p $provisioner -m $mgmtCluster -o json > /tmp/output.json
  nam=$(jq -r --arg clu "$clusterName" --arg pvn "$provisioner" --arg mcl "$mgmtCluster" \
        '.clusters[] | select(.fullName.name == $clu and .fullName.provisionerName == $pvn and .fullName.managementClusterName == $mcl).fullName.name' /tmp/output.json)
  stt=$(jq -r --arg clu "$clusterName" --arg pvn "$provisioner" --arg mcl "$mgmtCluster" \
        '.clusters[] | select(.fullName.name == $clu and .fullName.provisionerName == $pvn and .fullName.managementClusterName == $mcl).status.phase' /tmp/output.json)
  hlt=$(jq -r --arg clu "$clusterName" --arg pvn "$provisioner" --arg mcl "$mgmtCluster" \
        '.clusters[] | select(.fullName.name == $clu and .fullName.provisionerName == $pvn and .fullName.managementClusterName == $mcl).status.health' /tmp/output.json)
  [ "$nam" == "$clusterName" ] && echo "$stt/$hlt" && return 0

  if [ "$nam" != "$clusterName" ]; then 
    (cd /tmp && tmc cluster attach --name $clusterName --kubeconfig=$kubeconfig -p $provisioner \
          -m $mgmtCluster -g tanzu-demo-hub --force > /tmp/error.log 2>&1); ret=$?

    deleteFailedPods  vmware-system-tmc > /dev/null 2>&1
    runningPods       vmware-system-tmc > /dev/null 2>&1

    # --- WAIT FOR THE CLUSTER TO BE STARTED ---
    stt="NOTREADY"; hlt="UNKNOWN"; ccc=0
    while [ "$stt/$hlt" != "READY/HEALTHY" -a $ccc -lt 10 ]; do
      cmdLoop tmc cluster list --name $clusterName -p $provisioner -m $mgmtCluster -o json > /tmp/output.json
      stt=$(jq -r --arg clu "$clusterName" --arg pvn "$provisioner" --arg mcl "$mgmtCluster" \
            '.clusters[] | select(.fullName.name == $clu and .fullName.provisionerName == $pvn and .fullName.managementClusterName == $mcl).status.phase' /tmp/output.json) 
      hlt=$(jq -r --arg clu "$clusterName" --arg pvn "$provisioner" --arg mcl "$mgmtCluster" \
          '.clusters[] | select(.fullName.name == $clu and .fullName.provisionerName == $pvn and .fullName.managementClusterName == $mcl).status.health' /tmp/output.json) 
      [ "$stt/$hlt" == "READY/HEALTHY" ] && break
      let ccc=ccc+1
      sleep 60
    done
  fi

  #########################################################################################################################
  ################################################## VERIFY STATUS ########################################################
  #########################################################################################################################
  cmdLoop tmc cluster list --name $clusterName -p $provisioner -m $mgmtCluster -o json > /tmp/output.json
  stt=$(jq -r --arg clu "$clusterName" --arg pvn "$provisioner" --arg mcl "$mgmtCluster" \
        '.clusters[] | select(.fullName.name == $clu and .fullName.provisionerName == $pvn and .fullName.managementClusterName == $mcl).status.phase' /tmp/output.json)
  hlt=$(jq -r --arg clu "$clusterName" --arg pvn "$provisioner" --arg mcl "$mgmtCluster" \
        '.clusters[] | select(.fullName.name == $clu and .fullName.provisionerName == $pvn and .fullName.managementClusterName == $mcl).status.health' /tmp/output.json)

  echo "$stt/$hlt"; return 0
}



#kubectl get tanzukubernetescluster -n tanzu-demo-hub tdh-vsphere-sadubois
#kubectl delete tanzukubernetescluster -n tanzu-demo-hub tdh-vsphere-sadubois

#kubectl get ClusterRole psp:privileged -n tanzu-system-packages  
#kubectl apply -f files/psp/vmware-system-privileged.yaml

#tanzu package install cert-manager-pkg --package-name cert-manager.tanzu.vmware.com --namespace tanzu-system-packages --version 1.5.3+vmware.2-tkg.1 --create-namespace 
#kapp list -A
#kapp inspect -a cert-manager-pkg-ctrl -n tanzu-system-packages

